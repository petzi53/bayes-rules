{
  "hash": "4a69a833ed8aa8a60133965d017fe619",
  "result": {
    "engine": "knitr",
    "markdown": "# Balance and Sequentiality in Bayesian Analyses {#sec-chap-004}\n\n\n\n\n\n\n::::: {#obj-chap-004}\n:::: {.my-objectives}\n::: {.my-objectives-header}\nObjectives\n:::\n\n::: {.my-objectives-container}\n\n- **Explore the balanced influence of the prior and data on the posterior**. See how the choice of prior model, the features of the data, and the delicate balance between them can impact the posterior model. \n- **Perform sequential Bayesian analysis**. Explore one of the coolest features of Bayesian analysis: how a posterior model evolves as it’s updated with new data.\n\n:::\n::::\n:::::\n\n::: {.callout-caution #cau-004-incomplete-notes}\n###### Incomplete notes for this chapter\n\nAfter starting my notes I became aware that most of the material presented in this chapter is already familiar to me. So I will skip writing notes after some preliminary remarks until @sec-004-dont-be-stubborn.\n\n\n:::\n\n\nThe chapter starts with the example of the <a class='glossary' title='The Bechdel test (pronounce /ˈbɛkdəl/) also known as the Bechdel–Wallace test is a measure of the representation of women in film and other fiction. The test asks (a) whether a work features at least two women (b) who have a conversation © about something other than a man. Some versions of the test also require that those two women have names.'>Bechdel test</a> [@bechdel-2008]. \n\nLet $\\pi$, a random value between 0 and 1, denote the unknown proportion of recent movies that pass the Bechdel test. Three friends – the feminist, the clueless, and the optimist have some prior ideas about $\\pi$.\n\n- **The feminist** believes the majority of the films lack strong women characters.\n- **The clueless** doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon.\n- **The optimist** assumes almost all movies pass the test.\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-004-bechdel-test}\n: Who believes what?\n::::::\n:::\n::::{.my-assessment-container}\n\n![Three prior models for the proportion of films that pass the Bechdel test](img/004-quiz-bechdel-test.png){#fig-004-quiz-bechdel-test \nfig-alt=\"The priors from left to right: (a) Beta(1,1), (b) Beta(5,11) (c) Beta(14, 1)\" fig-align=\"center\" \nwidth=\"100%\"}\n\n\n- \"Feminist: <select class='webex-select'><option value='blank'></option><option value=''>a</option><option value='answer'>b</option><option value=''>c</option></select>\"\n- \"Cueless: <select class='webex-select'><option value='blank'></option><option value='answer'>a</option><option value=''>b</option><option value=''>c</option></select>\"\n- \"Optimist: <select class='webex-select'><option value='blank'></option><option value=''>a</option><option value=''>b</option><option value='answer'>c</option></select>\"\n::::\n:::::\n\nThe three analysts agree to review a sample of $n$ recent movies and record $Y$ , the number that pass the Bechdel test. Recognizing $Y$ as the number of “successes” in a fixed number of independent trials, they specify the dependence of $Y$ on $\\pi$ using a Binomial model. Thus, each analyst has a unique <a class='glossary' title='The Beta-Binomial Bayesian Model is a fundamental framework in Bayesian statistics used to estimate a proportion π (e.g., success probability) when data are binary outcomes (success/failure) across n independent trials. The Beta-Binomial Bayesian Model combines a Beta prior with a Binomial likelihood, resulting in a Beta posterior due to conjugacy.  This makes it ideal for estimating unknown proportions (e.g., success rates) from binary data.'>Beta-Binomial model</a> of $\\pi$ with differing prior <a class='glossary' title='In the Bayesian framework, a hyperparameter is a parameter of a prior distribution.  It defines the shape, location, or scale of the prior belief about a model parameter before observing any data. Unlike model parameters, which are estimated from the data (e.g., the mean or variance of a distribution), hyperparameters are set before the analysis begins and are not directly learned from the data. (Brave-AI)'>hyperparameters</a> $\\alpha$ and $\\beta$:  \n\n$$\n\\begin{align*}\nY \\mid \\pi &\\sim Bin(n, \\pi) \\\\\n\\pi &\\sim Beta(\\alpha, \\beta)\n\\end{align*}\n$$ {#eq-cross-ref-ID}\n\n\nBy our work in @sec-chap-003, it follows that each analyst has a *unique* posterior model of $\\pi$ which depends upon their *unique* prior (through $\\alpha$ and $\\beta$) and the *common* observed data (through $y$ and $n$)\n\n$$\n\\pi \\mid (Y = y) \\sim Beta(\\alpha + y, \\beta + n − y)\n$$ {#eq-004-beta-binomial-formula}\n\n\n## Different priors, different posteriors\n\n@fig-004-quiz-bechdel-test summarizes the prior models of $\\pi$, the proportion of recent movies that pass the Bechdel test, tuned by the clueless, the feminist, and the optimist. Not only do the differing **prior means** reflect disagreement about whether $\\pi$ is closer to 0 or 1, the differing levels of **prior variability** reflect the fact that the analysts have different degrees of certainty in their prior information. Loosely speaking, the more **certain** the prior information, the *smaller* the prior variability. The more **vague** the prior information, the *greater* the prior variability. \n\nThe priors of the optimist and the clueless represent these two extremes. \n\n- The optimist is the most certain in their prior understanding of $\\pi$. We refer to such believes as <a class='glossary' title='An informative prior reflects specific information about the unknown variable with high certainty, i.e., low variability.'>informative priors</a>.\n- The clueless is the least certain about $\\pi$. This type of “shoulder shrug” prior model has an official name: it’s a <a class='glossary' title='A vague or diffuse prior reflects little specific information about the unknown variable. A flat prior, which assigns equal prior plausibility to all possible values of the variable, is a special case.'>vague prior</a>, also called a <a class='glossary' title='A prior distribution which is specified in an attempt to be non commital about a parameter, for example, a uniform distribution. (The Cambridge Dictionary of Statistics, CDS, p.303)'>non-informative prior</a>.\n\nThe next natural question to ask is: how will their different priors influence the posterior conclusions of the feminist, the clueless, and the optimist? \n\no answer this question, we need some data. Our analysts decide to review a random sample of $n = 20$ recent movies using data collected for the FiveThirtyEight article on the Bechdel test [^004-balance-sequentiality-1]. The {**bayesrules**} package includes a partial version of this dataset, named bechdel. A complete version is provided by the {**fivethirtyeight**} R package [@fivethirtyeight]. Along with the title and year of each movie in this dataset, the binary variable records whether the film passed or failed the Bechdel test:\n\n[^004-balance-sequentiality-1]: Unfortunately the award winning FiveThirtyEight website that focused on opinion poll analysis, politics, economics, and sports blogging in the United States was shut down by ABC News on March 5, 2025 and its staff were laid off. The books URL shows the unformated text of the old article, but all data are still available via <https://github.com/fivethirtyeight/data> and <https://data.fivethirtyeight.com/>. It turned out that besides the [data of the referenced article](https://github.com/fivethirtyeight/data/tree/master/bechdel) there is a newer test data file, called the [The Next Bechdel Test](https://github.com/fivethirtyeight/data/tree/master/next-bechdel). This newer test was not designed to replace the original, but to challenge the industry to move beyond minimal standards.  It emphasizes that passing a checklist is not enough—systemic change in hiring, storytelling, and crew diversity is essential.\n\n\n::: {.callout-note #nte-004-skipped-several-sections}\n###### I will skip to @sec-004-dont-be-stubborn \n\nAt this point, I realized that the coming information of the next few sections is not new for me. So I will skip writing notes until @sec-004-dont-be-stubborn.\n\n:::\n\n## Different data, different posteriors (skipped)\n\n## Striking a balance between the prior & data (skipped)\n\n## Sequential analysis: Evolving with data (skipped)\n\n## Proving data order invariance (skipped)\n\n## Don’t be stubborn {#sec-004-dont-be-stubborn}\n\nThe previous sections of chapter 4 has highlighted three of the most compelling aspects of the Bayesian philosophy: \n\n- The balance between prior and data \n- <a class='glossary' title='A Bayesian model begins with one set of plausibilities assigned to each of these possibilities. These are the prior plausibilities. Then it updates them in light of the data, to produce the posterior plausibilities. This updating process is a kind of learning.'>Bayesian updating</a> in a sequential analysis\n- Order and the splitting of the data are irrelevant\n\nOne of the only ways to *lose* these Bayesian benefits is by starting with an *extremely* stubborn prior model. A model so stubborn that it assigns a prior probability of *zero* to certain parameter values. \n\nConsider an example within the Milgram study setting where $\\pi$ is the proportion of people that will obey authority even if it means bringing harm to others (cf. @sec-003-milgram-experiment). Suppose that a certain researcher has a stubborn belief in the good of humanity, insisting that $\\pi$ is equally likely to be anywhere between 0 and 0.25, and surely doesn’t exceed 0.25. They express this prior understanding through a <a class='glossary' title='The standard uniform distribution is a continuous probability distribution where all values in the interval [0, 1] are equally likely.  It is a special case of the continuous uniform distribution with parameters a = 0 and b = 1, often denoted as U(0, 1). (Brave-AI)'>Uniform model</a> on 0 to 0.25,\n\n$$\\pi \\sim \\text{Unif}(0, 0.25)$$\n\nwith pdf f ($\\pi$) exhibited in Figure 4.7 and specified by  \n\n$$f (\\pi) = 4 for \\pi \\in [0, 0.25]$$\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-004-stubborn-researcher}\n: Which plot accurately depicts the researcher’s corresponding posterior?\n::::::\n:::\n::::{.my-assessment-container}\n\nSuppose this researcher was told that the first $Y = 8$ of $n = 10$ participants delivered the shock. This 80% figure runs counter to the stubborn researcher’s belief. Check your intuition about how the researcher will update their posterior in light of this data.\n\n![The stubborn researcher’s prior and likelihood, with three potential corresponding posterior models.](img/004-stubborn-researcher.png){#fig-004-stubborn-researcher\nfig-alt=\"alt-text\" fig-align=\"center\" \nwidth=\"100%\"}\n\n\"Which posterior: <select class='webex-select'><option value='blank'></option><option value=''>a</option><option value=''>b</option><option value='answer'>c</option></select>\"\n\n::::\n:::::\n\nAs odd as it might seem, the posterior model in plot (c) corresponds to the stubborn researcher’s updated understanding of $\\pi$ in light of the observed data.\n\n**A posterior model is defined on the same values for which the prior model is defined.** \n\nThat is, the support of the posterior model is inherited from the support of the prior model. Since the psychologist’s prior model assigns zero probability to any value of $\\pi$ past 0.25, their posterior model must *also* assign zero probability to any value in that range. \n\nMathematically, the posterior pdf $f (\\pi \\mid y = 8) = 0 \\text{ for any } \\pi \\notin [0, 0.25] \\text{ and, for any } \\pi \\in [0, 0.25]$,\n\n$$\n\\begin{align*}\nf (\\pi \\mid y = 8) &\\propto f (\\pi)L(\\pi \\mid y = 8)  \\\\\n&= 4 \\cdot \\binom{10}{8} \\pi^8(1 − π)^2  \\\\\n&∝ \\pi^8(1 − \\pi)^2\n\\end{align*}\n$$ {#eq-004-explain-stubborn-calculation}\n\n::: {.callout-note #nte-004-explain-formula}\n\n###### Explain @eq-004-explain-stubborn-calculation\n\nI had difficulty to understand above formula. So I asked <a class='glossary' title='Claude Haiku is Anthropic’s smallest, fastest, and most cost-effective model in the Claude family, designed for high-speed, low-latency tasks while maintaining strong performance on coding, computer use, and agent workflows.'>Claude Haiku</a> \n\n::: {#big-text style=\"font-size: 150%\"}\n**Explain @eq-004-explain-stubborn-calculation** \n:::\n\nThis formula shows **Bayesian inference** for estimating a probability parameter. Here's what each part means:\n\n::: {#big-text style=\"font-size: 125%\"}\n**Line 1: Bayes' Theorem**\n:::\n\n$$f(\\pi \\mid y = 8) \\propto f(\\pi)L(\\pi \\mid y = 8)$$\n\n- **$f(\\pi \\mid y = 8)$** = posterior distribution (what we want to find) — the probability of $\\pi$ given we observed 8 successes\n- **$f(\\pi)$** = prior distribution — our belief about $\\pi$ *before* seeing data\n- **$L(\\pi \\mid y = 8)$** = likelihood — how likely the data (8 successes) is under different values of $\\pi$\n- **$\\propto$** = \"proportional to\" (ignoring constants that don't affect the shape)\n\n\n::: {#big-text style=\"font-size: 125%\"}\n**Line 2: Expanding the components**\n:::\n\n$$4 \\cdot \\binom{10}{8} \\pi^8(1 − π)^2$$\n\n- **$4$** = the prior (appears to be a uniform or flat prior scaled by 4)\n- **$\\binom{10}{8}$** = binomial coefficient — the number of ways to get 8 successes in 10 trials\n- **$\\pi^8(1 − \\pi)^2$** = likelihood from a binomial distribution with 8 successes and 2 failures\n\n::: {#big-text style=\"font-size: 125%\"}\n**Line 3: Simplifying**\n:::\n\n$$\\pi^8(1 − \\pi)^2$$\n\nThe constant factors ($4$ and $\\binom{10}{8}$) are dropped because they don't affect the *shape* of the distribution — only the scaling. This is the **kernel** of a **Beta distribution** with parameters $\\alpha = 9$ and $\\beta = 3$.\n\n**In short:** This shows how to combine prior beliefs with observed data (8 successes out of 10 trials) to get a posterior distribution for the unknown probability $\\pi$.\n\n***\n\n::: {#big-text style=\"font-size: 150%\"}\n**Why has the kernel of the Beta distribution the parameters $\\alpha = 9$ and $\\beta = 3$?** \n:::\n\n::: {#big-text style=\"font-size: 125%\"}\n**Beta Distribution Kernel**\n:::\n\n\nThe Beta distribution has the general form:\n$$\\text{Beta}(\\alpha, \\beta) \\propto \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}$$\n\nNotice the exponents are $\\alpha - 1$ and $\\beta - 1$.\n\n::: {#big-text style=\"font-size: 125%\"}\n**Matching Your Posterior**\n:::\n\n\nYour posterior kernel is:\n$$\\pi^8(1 − \\pi)^2$$\n\nTo match this with the Beta form, we need:\n- $\\alpha - 1 = 8$ → **$\\alpha = 9$**\n- $\\beta - 1 = 2$ → **$\\beta = 3$**\n\n::: {#big-text style=\"font-size: 125%\"}\n**Why This Makes Sense (Bayesian Conjugacy)**\n:::\n\n\nThis is a beautiful result of **conjugate priors**:\n\n- You started with a **Beta prior** on $\\pi$\n- You observed **8 successes and 2 failures** from a binomial distribution\n- Your posterior is *also* a **Beta distribution**\n\nThe update rule is simple: if your prior is $\\text{Beta}(\\alpha_0, \\beta_0)$ and you observe $s$ successes and $f$ failures, your posterior is:\n$$\\text{Beta}(\\alpha_0 + s, \\beta_0 + f)$$\n\nIn your case, the prior was $\\text{Beta}(1, 1)$ (uniform), so:\n$$\\text{Beta}(1 + 8, 1 + 2) = \\text{Beta}(9, 3)$$\n\nThis conjugacy property makes Beta-Binomial models very convenient for Bayesian analysis!\n:::\n\n\n\nThe implications of this math are huge. No matter how much counterevidence the stubborn researcher collects, their posterior will *never* budge beyond the 0.25 cap, not even if they collect data on a billion subjects. Luckily, we have some good news for you: **this Bayesian bummer is completely preventable**.\n\n::: {.callout-tip #tip-dont-be-stubborn}\n###### Avoid a regrettable prior model  \n\nLet $\\pi$ be *always* some parameter of interest. No matter how much prior information you think you have about $\\pi$ or how informative you want to make your prior, be sure to assign non-0 plausibility to every *possible* value of $\\pi$, even if this plausibility is *near* 0. For example, if $\\pi$ is a *proportion* which can technically range from 0 to 1, then your prior model should also be defined across this continuum.\n:::\n\n\n## A note on subjectivity {#sec-004-subjectivity}\n\nIn @sec-chap-001, we alluded to a common critique about Bayesian statistics – it’s too *subjective.* Specifically, some worry that “subjectively” tuning a prior model allows a Bayesian analyst to come to any conclusion that they want to. We can more rigorously push back against this critique in light of what we’ve learned in in this chapter.\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-004-subjectivity}\n: About subjectivity\n::::::\n:::\n::::{.my-assessment-container}\nFor each statement below, indicate whether the statement is true or false. Provide your reasoning. The reasoning is provided in the footnote[^004-balance-sequentiality-2].\n\n1. All prior choices are informative. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n2. There may be good reasons for having an informative prior. <select class='webex-select'><option value='blank'></option><option value='answer'>TRUE</option><option value=''>FALSE</option></select>\n3. Any prior choice can be overcome by enough data. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n4. The frequentist paradigm is totally objective. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n\n\n[^004-balance-sequentiality-2]: **1. False**. Vague priors are typically uninformative. **2. True**. We might have ample previous data or expertise from which to build our prior. **3. False**. If you assign zero prior probability to a potential parameter value, no amount of data can change that! **4. False**. Subjectivity always creeps in to both frequentist.\n::::\n:::::\n\nBayesian analysis can indeed build a prior based on “subjective” experience. Very seldom is this a bad thing, and quite often it’s a great thing! In the best-case scenarios, a subjective prior can reflect a wealth of past experiences that *should* be incorporated into our analysis – it would be unfortunate not to. Even if a subjective prior runs counter to actual observed evidence, its influence over the posterior fades away as this evidence piles up. We’ve seen one worst-case scenario exception. And it was *preventable.* If a subjective prior is stubborn enough to assign zero probability on a possible parameter value, no amount of counterevidence will be enough to budge it.  \n\nFinally, though we encourage you to be critical in your application of Bayesian methods, please don’t worry about them being any more subjective than frequentist methods. *No* human is capable of removing all subjectivity from an analysis. The life experiences and knowledge we carry with us inform everything from what research questions we ask to what data we collect. It’s important to consider the potential implications of this subjectivity in *both* Bayesian and frequentist analyses.\n\n\n## Chapter summary {#sec-004-summary}\n\nThis chapter explored the balance that a posterior model strikes between a prior model and the data. In general, we saw the following trends:\n\n::: {#big-text style=\"font-size: 120%\"}\n**Prior influence**\n:::\n\nThe less vague and more informative the prior, i.e., the greater our prior certainty, the more influence the prior has over the posterior.\n\n::: {#big-text style=\"font-size: 120%\"}\n**Data influence**\n:::\n\nThe more data we have, the more influence the data has over the posterior. Thus, if they have ample data, two researchers with different priors will have similar posteriors.\n\n::: {#big-text style=\"font-size: 120%\"}\n**Sequential Bayesian analysis**\n:::\n\nWe saw that in a <a class='glossary' title='In a sequential Bayesian analysis (aka Bayesian Learning or Bayesian Updating), a posterior model is updated incrementally as more data come in. With each new piece of data, the previous posterior model reflecting our understanding prior to observing this data becomes the new prior model.'>sequential Bayesian analysis</a>, we incrementally update our posterior model as more and more data come in. \n\n::: {#big-text style=\"font-size: 120%\"}\n**Data order invariance**\n:::\n\nThe final destination of the posterior is not impacted by the order in which we observe this data (i.e., the posterior is data order invariant) or whether we observe the data in one big dump or incrementally.\n\n## Exercises (skipped)\n\n## Glossary Entries {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Bayesian Updating </td>\n   <td style=\"text-align:left;\"> A Bayesian model begins with one set of plausibilities assigned to each of these possibilities. These are the prior plausibilities. Then it updates them in light of the data, to produce the posterior plausibilities. This updating process is a kind of learning. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bechdel Test </td>\n   <td style=\"text-align:left;\"> The Bechdel test (pronounce /ˈbɛkdəl/) also known as the Bechdel–Wallace test is a measure of the representation of women in film and other fiction. The test asks (a) whether a work features at least two women (b) who have a conversation (c) about something other than a man. Some versions of the test also require that those two women have names. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Beta-Binomial-Model </td>\n   <td style=\"text-align:left;\"> The Beta-Binomial Bayesian Model is a fundamental framework in Bayesian statistics used to estimate a proportion π (e.g., success probability) when data are binary outcomes (success/failure) across n independent trials. The Beta-Binomial Bayesian Model combines a Beta prior with a Binomial likelihood, resulting in a Beta posterior due to conjugacy.  This makes it ideal for estimating unknown proportions (e.g., success rates) from binary data. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Claude Haiku </td>\n   <td style=\"text-align:left;\"> Claude Haiku is Anthropic’s smallest, fastest, and most cost-effective model in the Claude family, designed for high-speed, low-latency tasks while maintaining strong performance on coding, computer use, and agent workflows. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hyperparameter </td>\n   <td style=\"text-align:left;\"> In the Bayesian framework, a hyperparameter is a parameter of a prior distribution.  It defines the shape, location, or scale of the prior belief about a model parameter before observing any data. Unlike model parameters, which are estimated from the data (e.g., the mean or variance of a distribution), hyperparameters are set before the analysis begins and are not directly learned from the data. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Informative_Prior </td>\n   <td style=\"text-align:left;\"> An informative prior reflects specific information about the unknown variable with high certainty, i.e., low variability. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Non-informative Prior </td>\n   <td style=\"text-align:left;\"> A prior distribution which is specified in an attempt to be non commital about a parameter, for example, a uniform distribution. (The Cambridge Dictionary of Statistics, CDS, p.303) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Sequential Bayesian Analysis </td>\n   <td style=\"text-align:left;\"> In a sequential Bayesian analysis (aka Bayesian Learning or Bayesian Updating), a posterior model is updated incrementally as more data come in. With each new piece of data, the previous posterior model reflecting our understanding prior to observing this data becomes the new prior model. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Uniform Distribution </td>\n   <td style=\"text-align:left;\"> The standard uniform distribution is a continuous probability distribution where all values in the interval [0, 1] are equally likely.  It is a special case of the continuous uniform distribution with parameters a = 0 and b = 1, often denoted as U(0, 1). (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Vague Prior </td>\n   <td style=\"text-align:left;\"> A vague or diffuse prior reflects little specific information about the unknown variable. A flat prior, which assigns equal prior plausibility to all possible values of the variable, is a special case. </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Session Info {.unnumbered}\n\n::: my-r-code\n::: my-r-code-header\nSession Info\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.5.2 (2025-10-31)\n#>  os       macOS Tahoe 26.2\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2026-02-08\n#>  pandoc   3.9 @ /opt/homebrew/bin/ (via rmarkdown)\n#>  quarto   1.8.27 @ /usr/local/bin/quarto\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  ! package      * version date (UTC) lib source\n#>  P cli            3.6.5   2025-04-23 [?] CRAN (R 4.5.0)\n#>  P commonmark     2.0.0   2025-07-07 [?] RSPM\n#>  P digest         0.6.39  2025-11-19 [?] CRAN (R 4.5.2)\n#>  P evaluate       1.0.5   2025-08-27 [?] CRAN (R 4.5.1)\n#>  P farver         2.1.2   2024-05-13 [?] RSPM\n#>  P fastmap        1.2.0   2024-05-15 [?] CRAN (R 4.5.0)\n#>  P glossary     * 1.0.0   2023-05-30 [?] RSPM\n#>  P glue           1.8.0   2024-09-30 [?] CRAN (R 4.5.0)\n#>  P htmltools      0.5.9   2025-12-04 [?] CRAN (R 4.5.2)\n#>  P htmlwidgets    1.6.4   2023-12-06 [?] RSPM\n#>  P jsonlite       2.0.0   2025-03-27 [?] CRAN (R 4.5.0)\n#>  P kableExtra     1.4.0   2024-01-24 [?] RSPM\n#>  P knitr          1.51    2025-12-20 [?] CRAN (R 4.5.2)\n#>  P lifecycle      1.0.5   2026-01-08 [?] RSPM\n#>  P litedown       0.9     2025-12-18 [?] RSPM\n#>    magrittr       2.0.4   2025-09-12 [1] RSPM\n#>  P markdown       2.0     2025-03-23 [?] RSPM\n#>  P otel           0.2.0   2025-08-29 [?] RSPM\n#>  P R6             2.6.1   2025-02-15 [?] CRAN (R 4.5.0)\n#>  P RColorBrewer   1.1-3   2022-04-03 [?] RSPM\n#>    renv           1.1.7   2026-01-27 [1] RSPM\n#>  P rlang          1.1.7   2025-12-20 [?] Github (tidyverse/rlang@7a519a2)\n#>  P rmarkdown      2.30    2025-09-28 [?] CRAN (R 4.5.0)\n#>    rstudioapi     0.18.0  2026-01-16 [1] RSPM\n#>  P scales         1.4.0   2025-04-24 [?] RSPM\n#>  P sessioninfo    1.2.3   2025-02-05 [?] RSPM\n#>  P stringi        1.8.7   2025-03-27 [?] RSPM\n#>  P stringr        1.6.0   2025-11-04 [?] RSPM\n#>  P svglite        2.2.2   2025-10-21 [?] RSPM\n#>  P systemfonts    1.3.1   2025-10-01 [?] RSPM\n#>  P textshaping    1.0.4   2025-10-10 [?] RSPM\n#>    vctrs          0.7.1   2026-01-23 [1] RSPM\n#>  P viridisLite    0.4.2   2023-05-02 [?] RSPM\n#>  P webexercises * 1.1.0   2023-05-15 [?] RSPM\n#>    xfun           0.56    2026-01-18 [1] RSPM\n#>    xml2           1.5.2   2026-01-17 [1] RSPM\n#>  P yaml           2.3.12  2025-12-10 [?] CRAN (R 4.5.2)\n#> \n#>  [1] /Users/petzi/Documents/Meine-Repos/bayes-rules/renv/library/macos/R-4.5/aarch64-apple-darwin20\n#>  [2] /Users/petzi/Library/Caches/org.R-project.R/R/renv/sandbox/macos/R-4.5/aarch64-apple-darwin20/4cd76b74\n#> \n#>  * ── Packages attached to the search path.\n#>  P ── Loaded and on-disk path mismatch.\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}