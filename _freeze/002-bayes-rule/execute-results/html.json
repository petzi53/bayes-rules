{
  "hash": "4dab8637131f377ded7f2915e37bcdb1",
  "result": {
    "engine": "knitr",
    "markdown": "# Bayes’ Rule {#sec-chap-002}\n\n\n\n:::::: {#obj-chap-002}\n::::: my-objectives\n::: my-objectives-header\nObjectives\n:::\n\n::: my-objectives-container\n-   **Explore foundational probability tools** such as marginal, conditional, and joint probability models and the Binomial model.\n-   **Conduct your first formal Bayesian analysis!** Construct your first prior and data models and, from these, construct your first posterior models via Bayes’ Rule.\n-   **Practice your Bayesian grammar**. Practice the formal notation and terminology central to Bayesian grammar.\n-   **Simulate Bayesian models**. Conduct your first simulation, using the R statistical software. Simulation is integral to building intuition for and supporting Bayesian analyses.\n:::\n:::::\n::::::\n\n## Introduction {.unnumbered}\n\nIf you read this chapter carefully you will note that there there are many repetitions. Read the reason and consequences behind this educational strategy in @sec-000-repetitions and why I have created \"Note\" callouts.\n\n### Exploring the `fake_news` dataset {.unnumbered}\n\nWe start this chapter with the examination of a sample of 150 articles which were posted on Facebook and fact checked by five [BuzzFeed](https://www.buzzfeed.com/) journalists [@shu-2017]. Information about each article is stored in the `fake_news` dataset in the {**bayesrules**} package.\n\n::: {#tip-002-skim-data .callout-tip}\n##### Explore and Summarize data\n\nTo learn more about this dataset, type `?fake_news` in your console or go to [A collection of 150 news articles](https://bayes-rules.github.io/bayesrules/docs/reference/fake_news.html).\n\nIn addition to read the help page, I recommend to use the `skim()` function from {**skimr**} package to get a summary of the dataset.\n:::\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-load-and-skim-fake-data}\n: Load relevant packages and skim the `fake_news` dataset\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-002-load-fake-data .cell tbl-cap='Skim the `fake_news` dataset'}\n\n```{#lst-002-load-fake-data .r .cell-code  lst-cap=\"Load relevant packages and the `fake-news` dataset\"}\n# Import article data\ndata(fake_news, package = \"bayesrules\")\n\n# Skim data\nskimr::skim(fake_news)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |          |\n|:------------------------|:---------|\n|Name                     |fake_news |\n|Number of rows           |150       |\n|Number of columns        |30        |\n|_______________________  |          |\n|Column type frequency:   |          |\n|character                |4         |\n|factor                   |1         |\n|logical                  |1         |\n|numeric                  |24        |\n|________________________ |          |\n|Group variables          |None      |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min|   max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|-----:|-----:|--------:|----------:|\n|title         |         0|          1.00|  29|   136|     0|      149|          0|\n|text          |         0|          1.00|  31| 31860|     0|      149|          0|\n|url           |         5|          0.97|  21|   169|     0|      145|          0|\n|authors       |        27|          0.82|   6|   135|     0|       84|          0|\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts       |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------|\n|type          |         0|             1|FALSE   |        2|rea: 90, fak: 60 |\n\n\n**Variable type: logical**\n\n|skim_variable  | n_missing| complete_rate| mean|count             |\n|:--------------|---------:|-------------:|----:|:-----------------|\n|title_has_excl |         0|             1| 0.12|FAL: 132, TRU: 18 |\n\n\n**Variable type: numeric**\n\n|skim_variable           | n_missing| complete_rate|    mean|      sd|    p0|     p25|     p50|     p75|     p100|hist  |\n|:-----------------------|---------:|-------------:|-------:|-------:|-----:|-------:|-------:|-------:|--------:|:-----|\n|title_words             |         0|             1|   11.18|    3.54|  5.00|    9.00|   11.00|   14.00|    22.00|▆▇▆▂▁ |\n|text_words              |         0|             1|  533.35|  596.44|  5.00|  253.00|  389.00|  534.00|  5392.00|▇▁▁▁▁ |\n|title_char              |         0|             1|   67.95|   20.54| 29.00|   54.00|   65.50|   82.75|   136.00|▅▇▆▂▁ |\n|text_char               |         0|             1| 3282.56| 3604.07| 31.00| 1527.50| 2385.00| 3339.25| 31860.00|▇▁▁▁▁ |\n|title_caps              |         0|             1|    0.67|    1.07|  0.00|    0.00|    0.00|    1.00|     7.00|▇▂▁▁▁ |\n|text_caps               |         0|             1|    4.77|    6.75|  0.00|    1.00|    3.00|    5.75|    56.00|▇▁▁▁▁ |\n|title_caps_percent      |         0|             1|    5.37|    8.34|  0.00|    0.00|    0.00|   10.00|    43.75|▇▂▁▁▁ |\n|text_caps_percent       |         0|             1|    1.10|    1.32|  0.00|    0.29|    0.68|    1.52|    10.00|▇▁▁▁▁ |\n|title_excl              |         0|             1|    0.15|    0.42|  0.00|    0.00|    0.00|    0.00|     2.00|▇▁▁▁▁ |\n|text_excl               |         0|             1|    0.36|    0.88|  0.00|    0.00|    0.00|    0.00|     8.00|▇▁▁▁▁ |\n|title_excl_percent      |         0|             1|    0.20|    0.61|  0.00|    0.00|    0.00|    0.00|     3.77|▇▁▁▁▁ |\n|text_excl_percent       |         0|             1|    0.02|    0.07|  0.00|    0.00|    0.00|    0.00|     0.61|▇▁▁▁▁ |\n|anger                   |         0|             1|    1.61|    0.92|  0.00|    0.95|    1.52|    2.04|     4.66|▅▇▆▂▁ |\n|anticipation            |         0|             1|    1.62|    0.71|  0.00|    1.23|    1.56|    2.03|     4.10|▂▇▇▂▁ |\n|disgust                 |         0|             1|    0.97|    0.58|  0.00|    0.54|    0.88|    1.39|     2.54|▅▇▆▃▁ |\n|fear                    |         0|             1|    1.94|    1.21|  0.00|    0.95|    1.66|    2.70|     5.13|▇▇▇▃▂ |\n|joy                     |         0|             1|    1.07|    0.61|  0.00|    0.61|    1.06|    1.52|     3.12|▆▇▆▂▁ |\n|sadness                 |         0|             1|    1.36|    0.80|  0.00|    0.82|    1.29|    1.85|     4.66|▆▇▃▁▁ |\n|surprise                |         0|             1|    0.94|    0.51|  0.00|    0.61|    0.95|    1.16|     2.33|▃▇▇▂▁ |\n|trust                   |         0|             1|    3.08|    1.75|  0.54|    2.19|    2.89|    3.63|    20.00|▇▁▁▁▁ |\n|negative                |         0|             1|    3.13|    1.36|  0.00|    2.21|    3.09|    4.04|     8.47|▃▇▆▁▁ |\n|positive                |         0|             1|    4.06|    1.23|  0.00|    3.35|    4.03|    4.55|     9.22|▁▆▇▂▁ |\n|text_syllables          |         0|             1|  912.55| 1006.86| 10.00|  409.50|  648.00|  945.25|  8875.00|▇▁▁▁▁ |\n|text_syllables_per_word |         0|             1|    1.72|    0.11|  1.48|    1.63|    1.71|    1.77|     2.12|▂▇▅▂▁ |\n\n\n:::\n:::\n\n:::\n::::::\n\nThe help file on `fake_news` describes the format: \"A data frame with 150 rows and 6 variables\". But as you can see from @tbl-002-load-fake-data it has 30 columns. I do not know where the dataset comes from, because the cited reference originated from an [article on BuzzFeed](https://www.buzzfeed.com/craigsilverman/partisan-fb-pages-analysis) uses a [different dataset](https://github.com/BuzzFeedNews/2016-10-facebook-fact-check/blob/master/data/facebook-fact-check.csv) with 12 columns.\n\nFrom the many different variables we are going to use only three: `type`, `title_has_excl` and `title_excl`. To get a feeling of the concrete data values look at a random sample of these three variables:\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-glance-random-data}\n: Glance at the values of a subset of the `fake_news` dataset\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-002-glance-random-data .cell tbl-cap='First, last and eight random rows of the reduced `fake_news` dataset'}\n\n```{#lst-002-glance-random-data .r .cell-code  lst-cap=\"First, last and eight random rows of the reduced `fake_news` dataset\"}\nfake_news |> \n  dplyr::select(type, title_has_excl, title_excl) |> \n  my_glance_data(seed = 3) |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| obs|type |title_has_excl | title_excl|\n|---:|:----|:--------------|----------:|\n|   1|fake |FALSE          |          0|\n|   5|fake |FALSE          |          0|\n|  20|fake |TRUE           |          1|\n|  36|real |FALSE          |          0|\n|  48|real |FALSE          |          0|\n|  74|real |FALSE          |          0|\n| 107|real |FALSE          |          0|\n| 136|fake |FALSE          |          0|\n| 140|fake |TRUE           |          2|\n| 150|fake |FALSE          |          0|\n\n\n:::\n:::\n\n:::\n::::::\n\nFrom this small sample we see that only two articles of type `fake` have exclamation points. One of these `fake` article sports even two exclamation signs.\n\n### Unconditional probabilites as an intuitive approximation {.unnumbered}\n\nWe can already see from the output of the `skimr::skim()` function that the `type` variable (`real` or `fake`) has the relation 90:60. Using the `tabyl()` function in the {**janitor**} package [@janitor], shows also the percentage (60%:40%).\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-relation-real-to-fake}\n: Relation of `real` to `fake` articles\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-002-relation-real-to-fake .cell tbl-cap='Relation of `real` to `fake` articles'}\n\n```{#lst-002-relation-real-to-fake .r .cell-code  lst-cap=\"Relation of `real` to `fake` articles\"}\nfake_news |>\n  janitor::tabyl(type) |>\n  janitor::adorn_totals(\"row\") |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|type  |   n| percent|\n|:-----|---:|-------:|\n|fake  |  60|     0.4|\n|real  |  90|     0.6|\n|Total | 150|     1.0|\n\n\n:::\n:::\n\n:::\n::::::\n\n::: {#nte-002-marginal-probabilities .callout-note}\n###### Unconditional, total or marginal probabilities\n\nTo understand better the following text it is important to note that @lst-002-relation-real-to-fake produces <a class='glossary' title='Unconditional probability, also known as marginal probability, refers to the likelihood of an event occurring without consideration of any other preceding or future events. It is a stand-alone probability that is not dependent on the occurrence of another event and is denoted as P(A).'>unconditional</a> or <a class='glossary' title='Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes’ Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI)'>total probabilities</a> also called as <a class='glossary' title='Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table.'>marginal probabilities</a>. The unconditional probabilities serve as a prior probability model as can be seen in @fig-002-diagram-fake-news.\n\n\\begin{align*}\nP(\\text{fake}) &= 0.4 \\\\\nP(\\text{real}) &= 0.6 \\\\\nP(\\text{Total}) &= 1.0\n\\end{align*}\n\nThere is more information about these different concepts in the following sections. But it helped my understanding to note these different names for @lst-002-relation-real-to-fake.\n:::\n\nIf we take the result of @lst-002-relation-real-to-fake, we could say: \"Since most articles are real, we should read and believe all articles\". If we follow this rule we wouldn't miss no real article, but at the cost reading many fake articles. 4 out of 10 articles would be fake news.\n\n### Conditional probabilities as an intuitive approximation {.unnumbered}\n\nWe need to create a better filter that is not only supported by the overall relation but also by some features of the articles it selves. One of these features would be an exclamation sign for the headlines. Exclamation points in headlines are often perceived as shouting or juvenile, which can make the content seem less credible.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-exclamation-usage}\n: Tabulate article type and exclamation usage = Conditional Probability\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-002-exclamation-usage .cell tbl-cap='Exclamation usage'}\n\n```{#lst-002-exclamation-usage .r .cell-code  lst-cap=\"Exclamation usage\"}\nfake_news |>\n  janitor::tabyl(type, title_has_excl) |>\n  janitor::adorn_totals(where = c(\"row\", \"col\")) |>\n  janitor::adorn_percentages() |>\n  janitor::adorn_pct_formatting(digits = 3) |>\n  janitor::adorn_ns(position = \"front\") |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|type  |FALSE         |TRUE         |Total          |\n|:-----|:-------------|:------------|:--------------|\n|fake  |44 (73.333%)  |16 (26.667%) |60 (100.000%)  |\n|real  |88 (97.778%)  |2  (2.222%)  |90 (100.000%)  |\n|Total |132 (88.000%) |18 (12.000%) |150 (100.000%) |\n\n\n:::\n:::\n\n:::\n::::::\n\n::: {#nte-002-conditional-probabilities .callout-note}\n###### Conditional probabilities\n\n@lst-002-exclamation-usage shows <a class='glossary' title='Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for “conditional on” or “given that”.'>Conditional probabilities</a> assessing how exclamation point usage depends on the article type.\n\n\\begin{align*}\nP(\\text{excl} \\mid \\text{fake}) &= 16/60 &= 26.67\\%  \\\\\nP(\\text{excl} \\mid \\text{real}) &=  2/90 &= 2.22\\% \\\\\nP(\\text{excl} \\mid \\text{total}) &=  18/150 &= 12.00\\%  \n\\end{align*}\n\n$P(\\text{excl} \\mid \\text{total})$ is the normalizing constant! See @sec-002-normalizing-constant-fake-news.\n\n------------------------------------------------------------------------\n\nHow to pronounce the above equations? I take as an example the first line:\n\n-   The probability of an exclamation sign in the headline of a fake article is 26.67%, or more formal:\n-   The probability to see an exclamation sign in the headline given the article is of type `fake` is 26.67%.\n\nImportant here is that we know the type of article. But normally we are interesting in the reverse operation. If we see an exclamations sign in the headline, e.g., we know there is an exclamation sign in the headline, what is the probability that the article type is `fake`? See for more detail and a general treatment @sec-002-conditional-probability-and-likelihood.\n:::\n\nThe usage of an exclamation point might seem like an odd choice for a real news article. The data backs up this instinct – in our article collection, 26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point.\n\nOr formulated differently: Only 18 articles has an exclamation sign in the title. But almost all of them (with only two exceptions) are used in the titles of fake news articles.\n\n### Bayesian thinking process {.unnumbered}\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-diagram-fake-news}\n: Bayesian knowledge-building diagram\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-diagram-fake-news .r .cell-code  lst-cap=\"Bayesian knowledge-building diagram whether or not the article is fake.\"}\nDiagrammeR::grViz(\"\ndigraph real_or_fake{\n\n# node statement\nnode [shape = oval]\na [label = 'Prior\\n40% of articles are fake'];\nb [label = 'Data\\n! more common among fake news'];\nc [label = 'Posterior\\nIs the article fake or not?'];\n\n# edge statement\na -> c b -> c\n}\")\n```\n\n::: {#fig-002-diagram-fake-news .cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-3d7e64f6b58789432012\" style=\"width:100%;height:260px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-3d7e64f6b58789432012\">{\"x\":{\"diagram\":\"\\ndigraph real_or_fake{\\n\\n# node statement\\nnode [shape = oval]\\na [label = \\\"Prior\\n40% of articles are fake\\\"];\\nb [label = \\\"Data\\n! more common among fake news\\\"];\\nc [label = \\\"Posterior\\nIs the article fake or not?\\\"];\\n\\n# edge statement\\na -> c b -> c\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n\nBayesian knowledge-building diagram whether or not the article is fake.\n:::\n:::\n\n:::\n::::::\n\n@fig-002-diagram-fake-news shows the Bayesian thinking process.\n\n1.  We start with a **prior understanding**: From all the articles only 40% are `fake`. From this vantage point it would be feasible to judge a random article as `real`. After all the probability that it is an article of type `real` is higher than that it would be `fake`.\n2.  But looking at exclamation signs in the title give us another more detailed perspective. The new **data changes our hypothesis**.\n3.  We have now another, a **posterior understanding**: Depending if the article has an exclamation point in the title or not, we can build a more probable hypotheses. Instead of 6:4 or 1.5 we have now a relation of 26.67:2.22 or 12. Based on the appearances of exclamations sign in the title our hypothesis is now 8 times more probable.\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-002-quiz-1-understanding-article}\n: What best describes your updated, posterior understanding about the article?\n::::::\n:::\n::::{.my-assessment-container}\n\n\n::: {.cell}\n\n:::\n\n\n<div class='webex-radiogroup' id='radio_WVHJDASFTY'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_WVHJDASFTY\" value=\"answer\"></input> <span>The chance that this article is fake jumps from 40% to roughly 90%. Though exclamation points are more common among fake articles, let’s not forget that only 40% of articles are fake.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_WVHJDASFTY\" value=\"\"></input> <span>The chance that this article is fake drops from 40% to 20%. The exclamation point in the title might simply reflect the author’s enthusiasm.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_WVHJDASFTY\" value=\"\"></input> <span>The chance that this article is fake jumps from 40% to roughly 98%. Given that so few real articles use exclamation points, this article is most certainly fake.</span></label></div>\n\n\nIf your intuition was incorrect, don’t fret. By the end of this chapter, you will have learned how to support Bayesian thinking with rigorous Bayesian calculations.\n\n\n::::\n:::::\n\n\n\n\n## Building a Bayesian Model for Events\n\nAfter an intuitive approximation in the previous section we will go now into the details.\n\n### Prior Probability Model {#sec-002-prior-probability-fake-news}\n\nAs a first step in our Bayesian analysis, we’ll formalize our prior understanding of whether the new article is fake.\n\nBefore even reading the new article, there’s a 0.4 <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>prior probability</a> that it’s fake and a 0.6 prior probability it’s *not*.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-prior-probability-model}\n: Prior probability model\n:::\n::::\n\n::: my-theorem-container\n(1) We can represent this information using mathematical notation. Letting $B$ denote the event that an article is `fake` and $B^c$ (read \"B complement\" or \"B not\" denote the event that it’s not `fake`, e.g., that it is `real.`\n(2) Additionally I have added the specific formula for the example at hand.\n\n$$\n\\begin{align*}\n&P(B) &= 0.40 \\text{ and } &P(B^c) &= 0.60 \\text{ (1)} \\\\\n&P(\\text{fake}) &= 0.40 \\text{ and } &P(\\text{real}) &= 0.60 \\text{ (2)}\n\\end{align*} \n$$ {#eq-002-prior-probability-model}\n:::\n::::::\n\nAs a collection, @eq-002-prior-probability-model $P(B)$ and $P(B^c)$ specify the simple <a class='glossary' title='In Bayesian statistics, a prior model refers to the specification of a prior distribution, which represents the initial beliefs or assumptions about the parameters of a statistical model before observing any data. This prior distribution quantifies the uncertainty about the parameters.'>prior model</a> of fake news summarized in @tbl-002-prior-probability-model.\n\nThere are three requirements of a valid probability model:\n\n(1) it accounts for all possible events (all articles must be fake or real);\n(2) it assigns (prior) probabilities to each event; and\n(3) these probabilities sum to one.\n\n| event       | $\\mathbf{B}$ | $\\mathbf{B^c}$ | Total |\n|:------------|--------------|----------------|-------|\n| probability | 0.4          | 0.6            | 1     |\n\n: Prior model of fake news {#tbl-002-prior-probability-model}\n\n::: {#nte-002-total-probabilities .callout-note}\n###### Prior Probability Model\n\n@tbl-002-prior-probability-model shows again the unconditional, marginal or total probabilities as I have already shown with @tbl-002-relation-real-to-fake. See also my @nte-002-marginal-probabilities.\n\n\\begin{align*}\nP(\\text{fake}) &= 0.4 \\\\\nP(\\text{real}) &= 0.6 \\\\\nP(\\text{Total}) &= 1.0\n\\end{align*}\n:::\n\n### Conditional Probability and Likelihood {#sec-002-conditional-probability-and-likelihood}\n\nIn the second step of our Bayesian analysis, we’ll summarize the insights from the data we collected on the new article. Specifically, we’ll formalize our observation that the exclamation point data is more compatible with fake news than with real news.\n\n#### Conditional Probability {#sec-002-conditional-probability-fake-news}\n\nConditional probabilities are fundamental to Bayesian analyses, and thus a quick pause to absorb this concept is worth it. In general, comparing the conditional vs unconditional probabilities, $P (A \\mid B) \\text{ vs } P (A)$, reveals the extent to which information about $B$ informs our understanding of $A$.\n\n-   In some cases, the certainty of an event $A$ might increase in light of new data $B$.\n-   In other cases, the certainty of an event might decrease in light of new data.\n\nRecall that *if* an article is fake, *then* there’s a roughly 26.67% chance it uses exclamation points in the title. In contrast, *if* an article is real, *then* there’s only a roughly 2.22% chance it uses exclamation points. When stated this way, it’s clear that the occurrence of exclamation points depends upon, or is *conditioned* upon, whether the article is fake.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-conditional-probability}\n: Conditional Probability\n:::\n::::\n\n::: my-theorem-container\nThis dependence is specified by the following conditional probabilities of exclamation point usage in the title ($A$) *given* an article’s fake status ($B$) or ($B^c$):\n\n$$\n\\begin{align*}\nP(A \\mid B) &= 0.2667 \\text{ and } P(A \\mid B^c) &= 0.0222 \\text{ (1)} \\\\\nP(\\text{excl} \\mid \\text{fake}) &= 0.2667 \\text{ and } P(\\text{excl} \\mid \\text{real}) &= 0.0222 \\text{ (2)} \\\\\n\\\\\nP(A^c \\mid B) &= 0.7333 \\text{ and } P(A^c \\mid B^c) &= 0.9778 \\text{ (1)} \\\\\nP(\\text{!excl} \\mid \\text{fake}) &= 0.7333 \\text{ and } P(\\text{!excl} \\mid \\text{real}) &= 0.9778 \\text{ (2)}\n\\end{align*}\n$$ {#eq-conditional-probability-1}\n:::\n::::::\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-conditional-probability}\n: Conditional Probability\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\ncond_prob <- fake_news |>\n  janitor::tabyl(type, title_has_excl) |>\n  janitor::adorn_totals(where = c(\"row\", \"col\")) |>\n  janitor::adorn_percentages() |>\n  janitor::adorn_pct_formatting(digits = 3) \n\ncond_prob |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|type  |FALSE   |TRUE    |Total    |\n|:-----|:-------|:-------|:--------|\n|fake  |73.333% |26.667% |100.000% |\n|real  |97.778% |2.222%  |100.000% |\n|Total |88.000% |12.000% |100.000% |\n\n\n:::\n:::\n\n:::\n::::::\n\nLet $A$ and $B$ be two events.\n\n-   The <a class='glossary' title='Unconditional probability, also known as marginal probability, refers to the likelihood of an event occurring without consideration of any other preceding or future events. It is a stand-alone probability that is not dependent on the occurrence of another event and is denoted as P(A).'>unconditional probability</a> of $A$, $P (A)$, measures the probability of observing $A$, without any knowledge of $B$.\n-   In contrast, the <a class='glossary' title='Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for “conditional on” or “given that”.'>conditional probability</a> of $A$ given $B$, $P (A \\mid B)$, measures the probability of observing $A$ in light of the information that $B$ occurred.\n\nIn general, comparing the conditional vs unconditional probabilities, $P (A \\mid B)$ vs $P (A)$, reveals the extent to which information about $B$ informs our understanding of $A$.\n\nThe *order* of conditioning is also important. Since they measure two different phenomena, it’s typically the case that $P (A \\mid B) ≠ P (B \\mid A)$. For instance, roughly 100% of puppies are adorable. Thus, if the next object you pass on the street is a puppy, $P (adorable \\mid puppy) = 1$. However, the reverse is not true. Not every adorable object is a puppy, thus $P (puppy \\mid adorable) < 1$.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-independent-events}\n: Independent Events\n:::\n::::\n\n::: my-theorem-container\nInformation about $B$ doesn’t always change our understanding of $A$. For example, suppose your friend has a yellow pair of shoes and a blue pair of shoes, thus four shoes in total. They choose a shoe at random and don’t show it to you.\n\n-   Without actually seeing the shoe, there’s a 0.5 probability that it goes on the right foot: $P (\\text{right foot}) = 2/4$.\n-   And even if they tell you that they happened to get one of the two yellow shoes, there’s still a 0.5 probability that it goes on the right foot: $P (\\text{right foot} \\mid \\text{yellow}) = 1/2$.\n\nThat is, information about the shoe’s color tells us nothing about which foot it fits – shoe color and foot are independent.\n\n$$\n\\begin{align*}\nP (A \\mid B) = P (A) \\text{ (1)} \\\\\nP (\\text{right foot} \\mid \\text{yellow}) &= 1/2 \\\\= P (\\text{right foot}) = 2/4 &= 1/2 \\text{ (2)}\n\\end{align*}\n$$ {#eq-002-independent-events}\n:::\n::::::\n\n#### Likelihood {#sec-002-likelihood-fake-news}\n\nLet’s reexamine our fake news example with these conditional concepts in place. The conditional probabilities we derived above, $P (A \\mid B) = 0.2667$ and $P (A \\mid B^c) = 0.0222$, indicate that a whopping 26.67% of fake articles versus a mere 2.22% of real articles use exclamation points. Since exclamation point usage is so much more **likely** among fake news than real news, this data provides some evidence that the article is fake.\n\nWith the above observation we’ve evaluated the exclamation point data by flipping the conditional probabilities $P (A \\mid B)$ and $P (A \\mid B^c)$ on their heads. **Flipping the conditional probabilities results in the likelihood function!** See @thm-002-probability-vs-likelihood.\n\nFor example, on its face, the conditional probability $P (A \\mid B)$ measures the uncertainty in event $A$ given we know event $B$ occurs. However, we find ourselves in the opposite situation. We *know* that the incoming article used exclamation points, $A$. What we *don’t* know is whether or not the article is fake, $B$ or $B^c$. Thus, in this case, we compared $P (A \\mid B)$ and $P (A \\mid B^c)$ to ascertain the relative <a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihoods</a> of observing data $A$ under different scenarios of the *uncertain* article status.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-probability-vs-likelihood}\n: Probability function versus likelihood function\n:::\n::::\n\n::: my-theorem-container\nTo help distinguish this application of conditional probability calculations from that when $A$ is uncertain and $B$ is known, we’ll utilize the following <a class='glossary' title='The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI)'>likelihood function</a> notation $L(\\cdot \\mid A)$:\n\n$$\n\\begin{align*}\nL(B \\mid A) = P (A \\mid B) \\text{ and } L(B^c \\mid A) = P (A \\mid B^c) \\text{ (1)} \\\\\nL(\\text{fake} \\mid \\text{excl}) = P(\\text{excl} \\mid \\text{fake}) \\text{ and } L(\\text{real} \\mid \\text{excl}) = P(\\text{excl} \\mid \\text{real}) \\text{ (2)}\n\\end{align*}\n$$ {#eq-002-probability-vs-likelihood}\n\n------------------------------------------------------------------------\n\n**Conditional Probability Function: Compare probabilities of an unknown event**\n\nWhen $B$ is known, the <a class='glossary' title='The conditional probability function describes the probability of an event occurring given that another event has already occurred.'>conditional probability function</a> $P (⋅∣B)$ allows us to compare the probabilities of an unknown event, $A$ or $A^c$, occurring with $B$:\n\n$$\n\\begin{align*}\nP (A \\mid B) &\\text{ vs } P (A^c \\mid B) \\text{ (1)} \\\\\nP (\\text{excl} \\mid \\text{fake}) &\\text{ vs } P (\\text{!excl} \\mid \\text{fake}) \\text{ (2)}\n\\end{align*}\n$$ {#eq-002-compare-probability-of-unknown-events}\n\n**Likelihood Function: Evaluate the relative compatibility of data with events**\n\nWhen A is known, the <a class='glossary' title='The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI)'>likelihood function</a> $L(⋅∣A) = P (A∣⋅)$ allows us to evaluate the relative compatibility of data $A$ with events $B$ or $B^c$:\n\n$$\n\\begin{align*}\nL(B \\mid A) &\\text{ vs } L(B^c \\mid A) \\text{ (1)} \\\\\nL(\\text{fake} \\mid \\text{excl}) &\\text{ vs } L(\\text{real} \\mid \\text{excl})\n\\end{align*}\n$$ {#eq-002-evaluate-relative-compatibility-of-data-with-events}\n:::\n::::::\n\n@tbl-002-prior-model-and-likelihood summarizes the information that we’ve amassed thus far, including the prior probabilities and likelihoods associated with the new article being fake or real, $B$ or $B^c$. Notice that the prior probabilities add up to 1 but the likelihoods do not. **The likelihood function is not a probability function**, but rather provides a framework to compare the relative compatibility of our exclamation point data with $B$ and $B^c$. Thus, whereas the prior evidence suggested the article is most likely real ($P (B) < P (B^c)$), the data is more consistent with the article being fake ($L(B∣A) > L(B^c∣A)$).\n\n| event       | $\\mathbf{B}$ | $\\mathbf{B^c}$ | Total  |\n|:------------|:-------------|:---------------|:-------|\n| probability | 0.4          | 0.6            | 1      |\n| likelihood  | 0.2667       | 0.0222         | 0.2889 |\n\n: Prior probabilities and likelihoods of fake news. {#tbl-002-prior-model-and-likelihood}\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-prior-probabilities-and-likelihood}\n: Prior probabilities and likelihoods of fake news\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-prior-probabilities-and-likelihood}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prior probability\nrow_prior <- fake_news |>  \n  dplyr::count(type) |>  \n  dplyr::mutate(prop = n / sum(n)) |>  \n  dplyr::select(-n) |>  \n  tidyr::pivot_wider(names_from = type, values_from = prop)\n\n# Likelihood\nrow_likelihood <- fake_news |>  \n  dplyr::count(type, title_has_excl) |>  \n  tidyr::pivot_wider(names_from = title_has_excl, values_from = n) |>  \n  dplyr::mutate(likelihood = `TRUE` / (`TRUE` + `FALSE`)) |>  \n  dplyr::select(-c(`FALSE`, `TRUE`)) |> \n  tidyr::pivot_wider(names_from = type, values_from = likelihood)\n\n# build table\ndplyr::bind_cols(Statistic = c(\"Prior probability\", \"Likelihood\"),\n          dplyr::bind_rows(row_prior, row_likelihood)) |>  \n  dplyr::mutate(Total = fake + real) |>  \n  dplyr::rename(`Fake ($\\\\mathbf{B}$)` = fake, \n         `Real ($\\\\mathbf{B^c}$)` = real) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|Statistic         | Fake ($\\mathbf{B}$)| Real ($\\mathbf{B^c}$)|  Total|\n|:-----------------|-------------------:|---------------------:|------:|\n|Prior probability |              0.4000|                0.6000| 1.0000|\n|Likelihood        |              0.2667|                0.0222| 0.2889|\n\n\n:::\n:::\n\n\nPrior probabilities and likelihoods of fake news (taken from [bayesf22 class](https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/02-chapter.html#likelihood))\n:::\n::::\n:::::::\n\n::: {#nte-002-likelihood .callout-note}\n###### Prior probabilities and likelihoods\n\nThe prior probability was already calculated in @lst-002-relation-real-to-fake. Here I have repeated the calculation with column and row reversed.\n\nThe interesting and new part here is the **<a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a>**.\n\n\\begin{align*}\nP(A \\mid B) &= 0.2667 \\\\\nP(A \\mid B^c) &= 0.0222 \\\\\n\\end{align*}\n\n26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point.\n\n**Likelihood compares how well data supports hypotheses**. Here, we calculate the likelihood of observing an exclamation point under each type:\n\n\\begin{align*}\nL(\\text{fake} \\mid \\text{excl}) &= P(\\text{excl} \\mid \\text{fake}) &= 16 / 60 &= 0.2667 \\\\\nL(\\text{real} \\mid \\text{excl}) &= P(\\text{excl} \\mid \\text{real}) &= 2 / 90 &= 0.0222 \\\\\nP(\\text{excl} \\mid \\text{fake}) &+ P(\\text{excl} \\mid \\text{real}) &= 0.2667 + 0.0222 &= 0.2889\n\\end{align*}\n\nAgain, **the likelihood function is not a probability function, it does not sum up to 1.00**. It provides a framework to compare the relative compatibility of our exclamation point data with $B$ and $B^c$.\n\nCompare the manual created @tbl-002-prior-model-and-likelihood with my @lst-002-exclamation-usage, my personal @nte-002-conditional-probabilities and the above @lst-002-prior-probabilities-and-likelihood.\n:::\n\n### Joint Probabilites and Normalizing Constants\n\n#### Joint Probabilites {#sec-002-joint-probabilities-fake-news}\n\nThe <a class='glossary' title='Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table.'>marginal probability</a> of observing exclamation points across all news articles, $P (A)$, provides an important point of comparison.\n\nWe’ll first use our prior model and likelihood function to fill in the table below. This table summarizes the possible **joint occurrences** of the fake news and exclamation point variables.\n\n|   | $\\mathbf{B} \\text{ (fake)}$ | $\\mathbf{B^c} \\text{ (real)}$ | Total |\n|:-----------------|:-----------------|:-----------------|:-----------------|\n| $A$ (excl) |  |  |  |\n| $A^C$ (!excl) |  |  |  |\n| Total | 0.4 | 0.6 | 1 |\n\nFirst, focus on the $B$ (articles of type `fake` column which splits fake articles into two groups:\n\n(1) those that are fake *and* use exclamation points, denoted $A \\cap B$; and\n(2) those that are fake *and* don’t use exclamation points, denoted $A^c \\cap B$.\n\n::: {#tip-002-defintion-pronounciation-cap-cup .callout-tip}\n###### How to define and pronounce $\\cap$ and $\\cup$ (Brave-AI)\n\n-   The formula $A \\cap B$ is pronounced as \"A intersect B\". The symbol $\\cap$ represents the intersection of two sets, indicating the elements common to both sets $A$ and $B$. It refers to the set of elements shared between two or more sets. The word \"and\" is often used as a synonym for intersection in this context, reflecting the logical condition that an element must belong to both sets simultaneously.\n-   The formula $A \\cup B$ is pronounced as \"A union B\". The symbol $\\cup$ represents the union operation in set theory, which combines all elements from sets $A$ and $B$ into a single set containing all unique elements from both. The term \"union\" is also referred to as the \"logical sum\" or simply \"sum,\" although these terms are considered old-fashioned and are not commonly used today. The symbol $\\cup$ is used to denote the union of two sets, and the operation is sometimes described as combining all elements from either set $A$ or set $B$ (or both).\n:::\n\nTo determine the probabilities of these **joint events**, note that 40% of articles are fake and 26.67% of fake articles use exclamation points, $P (B) = 0.4 \\text{ and } P (A \\mid B) = 0.2667$. It follows that across all articles, 26.67% of 40%, or 10.67%, are fake with exclamation points.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-joint-probability}\n: Joint Probability\n:::\n::::\n\n::: my-theorem-container\n**Co-occurrence of (not) exclamation point and fake article**\n\nThat is, the <a class='glossary' title='Joint probability is a statistical measure that calculates the likelihood of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as P(A∩B), which is read as “the probability of A and B”. The joint probability of two independent events A and B is computed as the product of their individual probabilities: P(A∩B)=P(A)×P(B). This formula applies when the occurrence of one event does not influence the other. For dependent events, the joint probability is calculated using conditional probability: P(A∩B)=P(A)×P(B∣A) where P(B∣A) is the probability of event B occurring given that event A has already occurred. (Brave-AI)'>joint probability</a> of observing both $A$ and $B$ is\n\n$$\n\\begin{align*}\nP (A \\cap B) = P (A \\mid B) \\cdot P (B) &= 0.2667 \\cdot 0.4 = 0.1067 \\text{ (1)} \\\\\nP (\\text{excl} \\cap \\text{fake}) = P (\\text{excl} \\mid\\text{fake}) \\cdot P (\\text{fake}) &= 0.2667 \\cdot 0.4 = 0.1067 \\text{ (2)}\n\\end{align*}\n$$ {#eq-joint-probability-fake}\n\nFurther, since 26.67% of fake articles use exclamation points, 73.33% do not. That is, the <a class='glossary' title='Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for “conditional on” or “given that”.'>conditional probability</a> that an article does not use exclamation points ($A^c$) given it’s fake ($B$) is:\n\n$$\n\\begin{align*}\nP (A^c \\mid B) = 1 − P (A \\mid B) &= 1 − 0.2667 = 0.7333 \\text{ (1)} \\\\\nP (\\text{!excl} \\mid \\text{fake}) = 1 - P (\\text{excl} \\mid \\text{fake}) &= 1 − 0.2667 = 0.7333 \\text{ (2)}\n\\end{align*}\n$$ {#eq-conditional-probabilities-1}\n\nIt follows that 73.33% of 40%, or 29.33%, of all articles are fake without exclamation points:\n\n$$\n\\begin{align*}\nP (A^c \\cap B) = P (A^c \\mid B) \\cdot P (B) &= 0.7333 \\cdot 0.4 = 0.2933 \\text{ (1)} \\\\\nP (\\text{!excl} \\cap \\text{fake}) = P (\\text{!excl} \\mid \\text{fake}) \\cdot P (\\text{fake}) &= 0.7333 \\cdot 0.4 = 0.2933 \\text{ (2)}\n\\end{align*}\n$$ {#eq-conditional-probabilities-2}\n\nIn summary, the <a class='glossary' title='Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes’ Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI)'>total probability</a> of observing a fake article is the sum of its parts:\n\n$$\n\\begin{align*}\nP (B) &= P (A \\cap B) + P (A^c \\cap B) &= 0.1067 + 0.2933 = 0.4 \\text{ (1)} \\\\\nP (\\text{fake}) &= P (\\text{excl} \\cap \\text{fake}) + P (\\text{!excl} \\cap \\text{fake}) &= 0.1067 + 0.2933 = 0.4 \\text{ (2)}\n\\end{align*}\n$$ {#eq-total-probability-fake}\n\n**Co-occurrence of (not) exclamation point and real article**\n\nWe can similarly break down real articles into those that do and those that don’t use exclamation points. Across all articles, only 1.33% (2.22% of 60%) are real and use exclamation points whereas 58.67% (97.78% of 60%) are real without exclamation points:\n\n$$\n\\begin{align*}\nP (A \\cap B^c) &= P (A \\mid B^c) \\cdot P (B^c) &= 0.0222 \\cdot 0.6 &= 0.0133 \\text{ (1a)} \\\\\nP (\\text{excl} \\cap \\text{real}) &= P (\\text{excl} \\mid \\text{real}) \\cdot P (\\text{real}) &= 0.0222 \\cdot 0.6 &= 0.0133 \\text{ (2a)} \\\\\n\\\\\nP (A^c \\cap B^c) &= P (A^c \\mid B^c) \\cdot P (B^c) &= 0.9778 \\cdot 0.6 &= 0.5867 \\text{ (1b)} \\\\\nP (\\text{!excl} \\cap \\text{real}) &= P (\\text{!excl} \\mid \\text{real}) \\cdot P (\\text{real}) &= 0.9778 \\cdot 0.6 &= 0.5867 \\text{ (2b)}\n\\end{align*}\n$$ {#eq-joint-probability-real}\n\nThus, the <a class='glossary' title='Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes’ Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI)'>total probability</a> of observing a real article is again the sum of these two parts:\n\n$$\n\\begin{align*}\nP (B^c) = P (A \\cap B^c) + P (A^c \\cap B^c) &= 0.0133 + 0.5867 = 0.6 \\text{ (1)} \\\\\nP (\\text{real}) = P (\\text{excl} \\cap \\text{real}) + P (\\text{!excl} \\cap \\text{real} &= 0.0133 + 0.5867 = 0.6) \\text{ (2)}\n\\end{align*}\n$$ {#eq-total-probability-real}\n:::\n::::::\n\n:::::::::::::::::: my-code-collection\n::::: my-code-collection-header\n::: my-code-collection-icon\n:::\n\n::: {#exm-002-joint-probabilities}\n: Joint Probabilities\n:::\n:::::\n\n:::::::::::::: my-code-collection-container\n::::::::::::: panel-tabset\n###### Version 1\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-joint-probabilities-1}\n: Joint Probabilities (my table)\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-joint-probabilities-1}\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_news |> \n  janitor::tabyl(title_has_excl, type) |>\n  janitor::adorn_totals(where = c(\"row\", \"col\")) |> \n  janitor::adorn_percentages(\"all\") |> \n  janitor::adorn_pct_formatting(digits = 3) |>\n  janitor::adorn_ns(position = \"front\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  title_has_excl         fake         real          Total\n#>           FALSE 44 (29.333%) 88 (58.667%) 132  (88.000%)\n#>            TRUE 16 (10.667%)  2  (1.333%)  18  (12.000%)\n#>           Total 60 (40.000%) 90 (60.000%) 150 (100.000%)\n```\n\n\n:::\n:::\n\n\nJoint probabilities (my own table)\n:::\n::::\n:::::::\n\n###### Version 2\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-joint-probabilities-2}\n: Joint Probability (Brave-AI & Positron Assistant)\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-joint-probabilities-2}\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_prob <- fake_news  |>  \n  janitor::tabyl(type)  |>  \n  dplyr::mutate(prop = n / sum(n))  |>  \n  dplyr::select(type, prior = prop)\n\nconditional_prob <- fake_news |>  \n  janitor::tabyl(title_has_excl, type) |>  \n  janitor::adorn_percentages(\"col\") |>  \n  dplyr::filter(title_has_excl == TRUE) |>  \n  dplyr::select(-title_has_excl) |>\n  tidyr::pivot_longer(cols = everything(), \n                      names_to = \"type\", \n                      values_to = \"conditional\") |>\n  dplyr::mutate(type = factor(type, levels = c(\"fake\", \"real\")))\n\njoint_prob <- prior_prob |> \n  dplyr::left_join(conditional_prob, by = \"type\") |> \n  dplyr::mutate(joint = prior * conditional)\n\njoint_prob  |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type | prior| conditional|  joint|\n|:----|-----:|-----------:|------:|\n|fake |   0.4|      0.2667| 0.1067|\n|real |   0.6|      0.0222| 0.0133|\n\n\n:::\n:::\n\n\nJoint probabilities (Brave-AI, middle part corrected by Positron Assistant)\n:::\n::::\n:::::::\n:::::::::::::\n::::::::::::::\n::::::::::::::::::\n\n::: {#nte-002-joint-probabilities .callout-note}\n####### Joint probabilities\n\n<a class='glossary' title='Joint probability is a statistical measure that calculates the likelihood of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as P(A∩B), which is read as “the probability of A and B”. The joint probability of two independent events A and B is computed as the product of their individual probabilities: P(A∩B)=P(A)×P(B). This formula applies when the occurrence of one event does not influence the other. For dependent events, the joint probability is calculated using conditional probability: P(A∩B)=P(A)×P(B∣A) where P(B∣A) is the probability of event B occurring given that event A has already occurred. (Brave-AI)'>Joint probabilities</a> measure the co-occurrence of two events. In the case of @exm-002-joint-probabilities it measures the co-occurrence of fake article *and* using an exclamation sign in the title.\n\nThis table is missing in the original book text. I have added it to improve my understanding.\n\n\\begin{align*}\nP(\\text{fake} \\cap \\text{excl}) = 0.40 \\cdot 0.2667 = 0.1067 \\\\\nP(\\text{real} \\cap \\text{excl}) = 0.60 \\cdot 0.0222 = 0.0133\n\\end{align*}\n:::\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-joint-and-conditional-probability}\n: Calculating joint and conditional probabilities\n:::\n::::\n\n::: my-theorem-container\nFor events $A$ and $B$, the joint probability of $A \\cap B$ is calculated by weighting the conditional probability of $A$ given $B$ by the marginal probability of $B$:\n\n$$P (A \\cap B) = P (A \\mid B) \\cdot P (B)$$ {#eq-002-joint-probability}\n\nThus, when $A$ and $B$ are *independent*,\n\n$$P (A \\cap B) = P (A) \\cdot P (B)$$\n\nDividing both sides of @eq-002-joint-probability by $P (B)$, and assuming $P (B) ≠ 0$, reveals the definition of the conditional probability of $A$ given $B$:\n\n$$P (A \\mid B) =  \\frac{P (A \\cap B) }{ P (B)}$$ {#eq-002-conditional-probability}\n\nThus, to evaluate the chance that $A$ occurs in light of information$B$, we can consider the chance that they occur together, $P (A \\cap B)$, relative to the chance that $B$ occurs at all, $P (B)$.\n:::\n::::::\n\n@tbl-002-joint-probability-model summarizes our new understanding of the joint behavior of our two article variables. The fact that the grand total of this table is one confirms that our calculations are reasonable. @tbl-002-joint-probability-model also provides the point of comparison we sought: 12% of *all* news articles use exclamation points, $P (A) = 0.12$.\n\nSo that we needn’t always build similar marginal probabilities from scratch, let’s consider the theory behind this calculation. As usual, we can start by recognizing the two ways that an article can use exclamation points: if it is fake ($A \\cap B$) and if it is not fake ($A \\cap B^c$).\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-law-of-total-probability-ltp}\n: Law of Total Probability (LTP)\n:::\n::::\n\n::: my-theorem-container\nThus, the **total probability** of observing $A$ is the combined probability of these distinct parts:\n\n$$\n\\begin{align*}\nP (A) &= P (A \\cap B) &+ P (A \\cap B^c) \\text{ (1)} \\\\\nP (\\text{excl}) &= P (\\text{excl} \\cap \\text{fake}) &+ P (\\text{excl} \\cap \\text{real}) \\text{ (2)}\n\\end{align*}\n$$ {#eq-chapXY-formula}\n\nBy @eq-002-joint-probability, we can compute the two pieces of this puzzle using the information we have about exclamation point usage among fake and real news, $P (A \\mid B)$ and $P (A \\mid B^c)$, weighted by the prior probabilities of fake and real news, $P (B)$ and $P (B^c)$:\n\n$$\n\\begin{align*}\nP (A) &= P (A \\cap B) + P (A \\cap B^c) \\\\\n&= P (A \\mid B) \\cdot P (B) + P (A \\mid B^c) \\cdot P (B^c) &\\text{ (1)} \\\\\nP (\\text{excl}) &= P (\\text{excl} \\cap \\text{fake}) + P(\\text{excl} \\cap \\text{real}) \\\\\n&= P (\\text{excl} \\mid \\text{fake}) \\cdot P (\\text{fake}) + P (\\text{excl} \\mid \\text{real}) \\cdot P (\\text{real}) &\\text{ (2)} \n\\end{align*}\n$$ {#eq-002-total-probability}\n:::\n::::::\n\n::: {#nte-002-ltp .callout-note}\n###### Law of Total Probability (LTP)\n\nFinally, plugging in, we can confirm that roughly 12% of all articles use exclamation points:\n\n$$\n\\begin{align*}\nP(\\text{excl}) &= (P(\\text{excl} \\mid \\text{fake}) \\cdot P(\\text{fake})) &+ (P(\\text{excl} \\mid {real}) \\cdot P(\\text{real})) &\\text{ (1)} \\\\\nP(\\text{excl}) &= (0.2667 \\cdot 0.4) &+ (0.0222 \\cdot 0.6) = 0.12 &\\text{ (2)}\n\\end{align*} \n$$ {#eq-002-ltp}\n:::\n\nThe formula we’ve built to calculate $P (A)$ here is a special case of the aptly named <a class='glossary' title='The law of total probability (LTP) is a fundamental rule in probability theory that relates marginal probabilities to conditional probabilities. It expresses the total probability of an event that can occur through several distinct, mutually exclusive, and collectively exhaustive scenarios. It is particularly useful when direct computation of an event’s probability is difficult, and it enables breaking down complex probability problems into simpler, manageable components. (Brave-AI)'>Law of Total Probability</a> (LTP).\n\n:::::: my-resource\n:::: my-resource-header\n::: {#lem-002-ltp}\n: Law of Total Probability (LTP)\n:::\n::::\n\n::: my-resource-container\n-   [Law of total probability \\| Wikipedia](https://www.wikiwand.com/en/articles/Law_of_total_probability)\n-   [Law of Total Probability \\| BYJU'S](https://byjus.com/maths/total-probability-theorem/)\n:::\n::::::\n\n|       | $\\mathbf{B}$ | $\\mathbf{B^c}$ | Total |\n|:------|:-------------|:---------------|:------|\n| $A$   | 0.1067       | 0.0133         | 0.12  |\n| $A^C$ | 0.2933       | 0.5867         | 0.88  |\n| Total | 0.4000       | 0.6000         | 1.00  |\n\n: A joint probability model of the fake status and exclamation point usage across all articles. {#tbl-002-joint-probability-model}\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-joint-probability-model}\n: Joint Probability Model\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-joint-probability-model}\n\n::: {.cell}\n\n```{.r .cell-code}\njoint_prob_model <- fake_news |> \n  janitor::tabyl(title_has_excl, type) |>\n  janitor::adorn_totals(where = c(\"col\", \"row\")) |> \n  janitor::adorn_percentages(\"all\") |> \n  dplyr::arrange(fake)\n\njoint_prob_model  |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|title_has_excl |   fake|   real| Total|\n|:--------------|------:|------:|-----:|\n|TRUE           | 0.1067| 0.0133|  0.12|\n|FALSE          | 0.2933| 0.5867|  0.88|\n|Total          | 0.4000| 0.6000|  1.00|\n\n\n:::\n:::\n\n\nJoint probability model of the fake status and exclamation point usage across all articles.\n:::\n::::\n:::::::\n\n#### Normalizing Constant {#sec-002-normalizing-constant-fake-news}\n\n::: {#nte-002-normalizing-constant .callout-note}\n###### Normalizing constant\n\nThe last piece we need is the marginal probability of observing exclamation points across all articles, or $P(A)$, which is the normalizing constant. For the calculation see the calculated values in @tbl-002-prior-model-and-likelihood and @lst-002-prior-probabilities-and-likelihood.\n\n$$\n\\begin{align*}\nP (B) \\cdot L (B \\mid A) &+ P (B^c) \\cdot L (B^c \\mid A) \\text{ (1)} \\\\\nP (\\text{fake}) \\cdot L (\\text{fake} \\mid \\text{excl}) &+ P (\\text{real}) \\cdot L (\\text{real} \\mid \\text{excl}) \\text{ (2)} \\\\\n0.4 \\cdot 0.2667 &+ 0.6 \\cdot 0.0222 = \\\\\n0.1067 &+ 0.0133 = 0.1199 \\approx 0.12 \\text{ (3)} \\\\\n\\\\\nP(\\text{excl}) = P(\\text{excl} \\mid \\text{fake}) \\cdot P(\\text{fake}) &+ P(\\text{excl} \\mid {real}) \\cdot P(\\text{real}) \\text{ (4)} \\\\\nP(\\text{excl}) = (0.2667 \\cdot 0.4) &+ (0.0222 \\cdot 0.6) = 0.1199 \\approx 0.12 \\text{ (5)}\n\\end{align*} \n$$ {#eq-002-normalizing-constant}\n\nWe filled in (3) and (5) the figures applying the law of total probability (LTP) as outlined in @nte-002-ltp.\n:::\n\n:::::::::::::::: my-code-collection\n::::: my-code-collection-header\n::: my-code-collection-icon\n:::\n\n::: {#exm-002-normalizing-constant}\n: Normalizing constant\n:::\n:::::\n\n:::::::::::: my-code-collection-container\n::::::::::: panel-tabset\n###### bayesf22 Notebook\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-normalizing-constant-1}\n: Normalizing constant ([bayesf22](https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/02-chapter.html#normalizing-constants))\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_news |> \n  dplyr::count(type, title_has_excl) |> \n  dplyr::mutate(prop = n / sum(n)) |> \n  dplyr::filter(title_has_excl == TRUE) |>  \n  dplyr::summarize(normalizing_constant = sum(prop))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   normalizing_constant\n#> 1                 0.12\n```\n\n\n:::\n\n```{.r .cell-code}\n##   normalizing_constant\n## 1                 0.12\n```\n:::\n\n:::\n::::::\n\n###### Brave-AI\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-normalizing-constant-2}\n: Normalizing constant (Brave-AI)\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\ntotal_prob <- joint_prob |>  \n  dplyr::summarise(total = base::sum(joint)) |>  \n  dplyr::pull(total)\n\ntotal_prob\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.12\n```\n\n\n:::\n\n```{.r .cell-code}\n# Result: 0.1067 + 0.0133 = 0.12   \n```\n:::\n\n:::\n::::::\n:::::::::::\n::::::::::::\n::::::::::::::::\n\n### Posterior probability model {#sec-002-posterior-probability-fake-news}\n\n#### Bayes rule!\n\nWe’re now in a position to answer the ultimate question: What’s the probability that the latest article is fake? Formally speaking, we aim to calculate the <a class='glossary' title='Posterior Probability, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes’ theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data.  It’s essentially learning from experience - your understanding becomes more refined as you incorporate new information.'>posterior probability</a> that the article is fake given that it uses exclamation points, $P (B \\mid A)$.\n\nTo build some intuition, let’s revisit @tbl-002-joint-probability-model and @lst-002-joint-probability-model. Since our article uses exclamation points, we can zoom in on the 12% of articles that fall into the $A$ row resp. in the `TRUE` row. Among these articles, proportionally 88.9% (0.1067 / 0.12) are fake and 11.1% (0.0133 / 0.12) are real. **This is the answer we were seeking: there’s an 88.9% posterior chance that this latest article is fake.**\n\n::: {#nte-002-bayes-rule .callout-note}\n###### We built Bayes’ Rule from scratch!\n\nStepping back from the details, we’ve accomplished something big: we built <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ Rule</a> from scratch! In short, Bayes’ Rule provides the mechanism we need to put our Bayesian thinking into practice. It defines a <a class='glossary' title='Posterior Probability, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes’ theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data.  It’s essentially learning from experience - your understanding becomes more refined as you incorporate new information.'>posterior probability model</a> for an event $B$ from two pieces: the <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>prior probability</a> of $B$ and the <a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a> of observing data $A$ if $B$ were to occur.\n:::\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-bayes-rule}\n: Bayes’ Rule for Events\n:::\n::::\n\n::: my-theorem-container\nFor events $A$ and $B$, the posterior probability of $B$ given $A$ follows by combining (@eq-002-conditional-probability) with (@eq-002-joint-probability) and recognizing that we can evaluate data $A$ through the likelihood function, $L(B \\mid A) = P (A \\mid B)$ and $L(B^c \\mid A) = P (A \\mid B^c)$:\n\n$$\n\\begin{align*}\nP (B \\mid A) = \\frac{P (A \\cap B)}{P (A)} = \\frac{P (B) \\cdot L (B \\mid A)}{P (A)}\n\\end{align*}\n$$ {#eq-002-bayes-rule-1}\n\n-   $P (B \\mid A)$ is the posterior probability,\n-   $P (B)$ is the prior,\n-   $L (B \\mid A)$ is the likelihood, and\n-   $P (A)$ is the marginal likelihood or evidence.\n\nwhere by the Law of Total Probability (@eq-002-ltp)\n\n$$P (A) = P (B) \\cdot L (B \\mid A) + P (B^c) \\cdot L (B^c \\mid A)$$ {#eq-002-bayes-rule-2}\n\nMore generally,\n\n$$\\text{posterior} = \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}}$$ {#eq-002-bayes-rule-general}\n\nTo convince ourselves that Bayes’ Rule works, let’s directly apply it to our news analysis. Into (@eq-002-bayes-rule-1), we can plug the prior information that 40% of articles are fake, the 26.67% likelihood that a fake article would use exclamation points, and the 12% marginal probability of observing exclamation points across all articles. The resulting posterior probability that the incoming article is fake is roughly 0.889, just as we calculated from @tbl-002-joint-probability-model resp. @lst-002-joint-probability-model:\n\n$$\n\\begin{align*}\nP (B \\mid A) = \\frac{P (B) \\cdot L (B \\mid A)}{P (A)} = \\frac{0.4 \\cdot 0.2667}{0.12} = \\frac{0.1067}{0.12} = 0.889\n\\end{align*}\n$$ {#eq-002-bayes-rule-with-data}\n:::\n::::::\n\n#### From Prior to Posterior\n\n@tbl-prior-posterior-model summarizes our news analysis journey, from the prior to the posterior model. We started with a prior understanding that there’s only a 40% chance that the incoming article would be fake. Yet upon observing the use of an exclamation point in the title “The president has a funny secret!”, a feature that’s more common to fake news, our posterior understanding evolved quite a bit – the chance that the article is fake jumped to 88.9%.\n\n| Event                 | $\\mathbf{B}$ | $\\mathbf{B^c}$ | Total |\n|:----------------------|-------------:|---------------:|------:|\n| prior probability     |        0.400 |          0.600 |   1.0 |\n| posterior probability |        0.889 |          0.111 |   1.0 |\n\n: The prior and posterior models of fake news. {#tbl-prior-posterior-model}\n\nThe following calculation of the posterior probability has used Positron Assistant: P(fake \\| title has exclamation) using Bayes' rule.\n\nThe posterior probability using Bayes’ rule is calculated with:\n\n$$P(\\text{fake} \\mid \\text{excl}) = P(\\text{excl} \\mid \\text{fake}) \\cdot P(\\text{fake}) / P(\\text{excl})$$\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-prior-and-posterior-probability}\n: Prior and Posterior Probability\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-prior-and-posterior-probability}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate prior probabilities P(type)\nprior_prob <- fake_news |>  \n  janitor::tabyl(type) |>  \n  dplyr::rename(prior = percent) |> \n  dplyr::select(type, prior)\n\n# Calculate conditional probabilities P(excl | type)\nconditional_prob <- fake_news |>  \n  janitor::tabyl(title_has_excl, type) |>  \n  janitor::adorn_percentages(\"col\") |>  \n  dplyr::filter(title_has_excl == TRUE) |>  \n  tidyr::pivot_longer(cols = c(fake, real), \n                      names_to = \"type\", \n                      values_to = \"conditional\")\n\n# Calculate joint probabilities P(excl AND type)\n# prior probability * conditional probability\njoint_prob <- prior_prob |> \n  dplyr::left_join(conditional_prob, by = \"type\") |> \n  dplyr::mutate(joint = prior * conditional)\n\n# Calculate marginal probability P(excl)\nmarginal_prob <- sum(joint_prob$joint)\n\n# Calculate posterior probabilities P(type | excl)\nposterior_prob <- joint_prob |>\n  dplyr::mutate(posterior = joint / marginal_prob) |>\n  dplyr::select(type, posterior)\n\n# Create the final table\nresult_table <- tibble::tibble(\n  Event = c(\"Prior Probability\", \"Posterior Probability\"),\n  fake = c(\n    prior_prob |> dplyr::filter(type == \"fake\") |> dplyr::pull(prior),\n    posterior_prob |> dplyr::filter(type == \"fake\") |> dplyr::pull(posterior)\n  ),\n  real = c(\n    prior_prob |> dplyr::filter(type == \"real\") |> dplyr::pull(prior),\n    posterior_prob |> dplyr::filter(type == \"real\") |> dplyr::pull(posterior)\n  )\n) |>\n  dplyr::mutate(Total = fake + real)\n\nresult_table |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|Event                 |   fake|   real| Total|\n|:---------------------|------:|------:|-----:|\n|Prior Probability     | 0.4000| 0.6000|     1|\n|Posterior Probability | 0.8889| 0.1111|     1|\n\n\n:::\n:::\n\n\nPrior and Posterior Probability of news articles with an exclamation sign in the title\n:::\n::::\n:::::::\n\n#### Step-by-Step\n\n@lst-002-prior-and-posterior-probability gives a nice summary of all the things we learned in this section. It separates and names the different steps to calculate the posterior probability.\n\n-   **1. Prior Probability**: The <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>Prior Probability</a>, also called the Prior, is the assumed probability distribution before we have seen the data. In the case of our example with the `fake_news` dataset it integrates already some data, namely the proportion of `fake` to `real` articles (04 : 06). The prior quantifies how likely our initial belief is: $P(B) = P(\\text{fake}) = 0.4$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_prob |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type | prior|\n|:----|-----:|\n|fake |   0.4|\n|real |   0.6|\n\n\n:::\n:::\n\n\n-   **2. Conditional probability**: The <a class='glossary' title='Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for “conditional on” or “given that”.'>Conditional Probability</a> is a measure of the likelihood of an event occurring given that another event has already occurred. The mathematical notation uses the pipe symbol for \"conditional on\" or \"given that\". It is denoted as $P(A \\mid B)$, which represents the probability of event $A$ occurring given that event $B$ has already occurred. In our example it is the probability of an exclamation sign in the title given that we are inspecting a fake article: $P(A \\mid B) = P(\\text{excl} \\mid \\text{fake}) = 0.2667$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_prob |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|title_has_excl |type | conditional|\n|:--------------|:----|-----------:|\n|TRUE           |fake |      0.2667|\n|TRUE           |real |      0.0222|\n\n\n:::\n:::\n\n\n-   **3. Joint probability:** <a class='glossary' title='Joint probability is a statistical measure that calculates the likelihood of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as P(A∩B), which is read as “the probability of A and B”. The joint probability of two independent events A and B is computed as the product of their individual probabilities: P(A∩B)=P(A)×P(B). This formula applies when the occurrence of one event does not influence the other. For dependent events, the joint probability is calculated using conditional probability: P(A∩B)=P(A)×P(B∣A) where P(B∣A) is the probability of event B occurring given that event A has already occurred. (Brave-AI)'>Joint Probability</a> is a statistical measure that calculates the <a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a> of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as $P(A \\cap B)$, which is read as \"the probability of A and B\". The joint probability of two independent events $A$ and $B$ is computed as the product of their individual probabilities: $P(A \\cap B) = P(A) \\cdot P(B)$. In our example: $P(\\text{excl} \\cap \\text{fake}) = P(\\text{excl}) \\cdot P(\\text{fake}) = 0.2667 \\cdot 0.4 = 0.1067$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njoint_prob |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type | prior|title_has_excl | conditional|  joint|\n|:----|-----:|:--------------|-----------:|------:|\n|fake |   0.4|TRUE           |      0.2667| 0.1067|\n|real |   0.6|TRUE           |      0.0222| 0.0133|\n\n\n:::\n:::\n\n\n-   **4. Marginal probability**: <a class='glossary' title='Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table.'>Marginal Probability</a> refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as $P(A)$ or $P(B)$, and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table. The summary of the joint probabilities $P(\\text{excl} \\cap \\text{fake}) + P(\\text{excl} \\cap \\text{real}) = 0.1067 + 0.0133 = 0.12$ is the <a class='glossary' title='In Bayesian statistics, the normalizing constant ensures that the posterior distribution integrates to 1, making it a valid probability distribution. It is also known as the marginal likelihood or evidence, and appears in the denominator of Bayes’ theorem. The integral for the normalizing constant is often intractable for complex models because it involves high-dimensional parameter spaces and there exists no closed-form solution for most realistic models. As a result, approximation methods like Markov Chain Monte Carlo (MCMC), bridge sampling, or variational inference are used to estimate or bypass the normalizing constant. (Brave-AI)'>normalizing constant</a>. It ensures that the posterior distribution integrates to 1, making it a valid probability distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njoint_prob |> \n  tibble::as_tibble() |> \n  janitor::adorn_totals(\"row\") |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type  | prior|title_has_excl | conditional|  joint|\n|:-----|-----:|:--------------|-----------:|------:|\n|fake  |   0.4|TRUE           |      0.2667| 0.1067|\n|real  |   0.6|TRUE           |      0.0222| 0.0133|\n|Total |   1.0|-              |      0.2889| 0.1200|\n\n\n:::\n:::\n\n\n-   **5. Posterior Probability**: The <a class='glossary' title='Posterior Probability, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes’ theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data.  It’s essentially learning from experience - your understanding becomes more refined as you incorporate new information.'>Posterior Probability</a>, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes' theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data. It's essentially learning from experience - your understanding becomes more refined as you incorporate new information.\n\nThe @eq-002-bayes-rule-1 can be expressed for our example as\n\n$$\n\\begin{align*}\nP (\\text{fake} \\mid \\text{excl}) &= \\frac{P (\\text{excl} \\cap \\text{fake})}{P (\\text{excl})} &= \\\\ \nP (\\text{fake} \\mid \\text{excl}) &= \\frac{P (\\text{fake}) \\cdot L (\\text{fake} \\mid \\text{excl})}{P (\\text{excl})} &= \\\\\nP (\\text{fake} \\mid \\text{excl}) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} &= \\\\\nP (\\text{fake} \\mid \\text{excl}) &= \\frac{0.4 \\cdot 0.2667}{0.12} = \\mathbf{0.889} \n\\end{align*}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_prob <- joint_prob |>\n  dplyr::mutate(posterior = joint / marginal_prob) |>\n  dplyr::select(type, posterior) |> \n  tibble::as_tibble() |> \n  janitor::adorn_totals(\"row\") \n  \ncomplete_prob <- joint_prob |>\n  dplyr::left_join(posterior_prob, by = dplyr::join_by(type)) |> \n  tibble::as_tibble() |> \n  janitor::adorn_totals(\"row\")\n\ncomplete_prob |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type  | prior|title_has_excl | conditional|  joint| posterior|\n|:-----|-----:|:--------------|-----------:|------:|---------:|\n|fake  |   0.4|TRUE           |      0.2667| 0.1067|    0.8889|\n|real  |   0.6|TRUE           |      0.0222| 0.0133|    0.1111|\n|Total |   1.0|-              |      0.2889| 0.1200|    1.0000|\n\n\n:::\n:::\n\n\n### Posterior Simulation {#sec-002-posterior-simulation-fake-news}\n\nIt’s important to keep in mind that the <a class='glossary' title='A probability model is a mathematical representation that describes the likelihood of various outcomes in a random experiment. It consists of a sample space, which includes all possible outcomes, and a probability assignment that assigns a probability to each outcome. The probabilities assigned in a probability model must always sum to 1, reflecting the certainty that one of the outcomes will occur. (Brave-AI)'>probability models</a> we built for our news analysis above are just that – <a class='glossary' title='A statistical model (both in frequentist and Bayesian statistics) is a mathematical representation that specifies how observed data are generated, often involving assumptions about the underlying processes and parameters. Both approaches use models to represent data-generating processes, but they differ fundamentally in how they treat uncertainty and parameters: frequentist models treat parameters as fixed and focus on the sampling distribution of statistics, while Bayesian models treat parameters as random variables and update beliefs based on data and prior knowledge. (Brave-AI)'>models</a>. They provide theoretical representations of what we observe in practice. To build intuition for the connection between the articles that might actually be posted to social media and their underlying models, let’s run a <a class='glossary' title='Simulation is a way to model random events, such that simulated outcomes closely match real-world outcomes. By observing simulated outcomes, researchers gain insight on the real world. (Stat Trek)'>simulation</a>.\n\n#### Define article `type` and prior probabilities {#sec-002-define-prior-probabilities}\n\nDefine the possible article `type`, `real` or `fake`, and their corresponding prior probabilities:\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-define-prior-probabilities}\n: Define article `type` and prior probabilities\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-define-prior-probabilities}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define possible articles\narticle <- data.frame(type = c(\"real\", \"fake\"))\n\n# Define the prior model\nprior <- c(0.6, 0.4)\n```\n:::\n\n\nDefine the possible article `type`, `real` or `fake`, and their corresponding prior probabilities\n:::\n::::\n:::::::\n\n#### Randomly sample rows from the article data frame\n\nThe book recommends the function `dplyr::sample_n()` which is superseded by `dplyr::slice_sample()`. We need three more information to get the desired simulation:\n\n-   We must specify the sample size `n`.\n-   We need to sample with `replacement` ensuring that we start with a fresh set of possibilities for each article – any article can either be fake or real.\n-   Finally we have to specify that there’s a 60% chance an article is real and a 40% chance it’s fake. This is done by the argument `weight_by = prior`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate 5 articles\ndplyr::slice_sample(article, n = 5, weight_by = prior, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   type\n#> 1 real\n#> 2 fake\n#> 3 fake\n#> 4 real\n#> 5 fake\n```\n\n\n:::\n:::\n\n\nIf you would run the code above several times you would see that the result changes each time. I want to demonstrate this behavior with a more complex code using a loop.\n\n\n::: {.cell}\n\n```{#lst-sample-results-1b .r .cell-code  lst-cap=\"Generate 5 articles in 4 samples using map and slice_sample\"}\n# Generate 5 articles in 4 samples using purrr::map and dplyr::slice_sample\nsample_results_1b <- purrr::map(1:4, ~ fake_news |> \n                        dplyr::slice_sample(n = 5) |> \n                        dplyr::pull(type)) |>\n  rlang::set_names(paste0(\"sample_\", 1:4)) |>\n  tibble::as_tibble()\n\nsample_results_1b |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|sample_1 |sample_2 |sample_3 |sample_4 |\n|:--------|:--------|:--------|:--------|\n|real     |real     |real     |fake     |\n|real     |fake     |real     |fake     |\n|fake     |fake     |fake     |real     |\n|real     |fake     |real     |real     |\n|real     |fake     |fake     |fake     |\n\n\n:::\n:::\n\n\nGenerate 5 articles in 4 samples using map and slice_sample\n\nWe ran the above code four times to see that the result changes will every run.\n\n#### Set the seed\n\nEvery time we run the above code the random number generator (RNG) “starts” at a new place: the random seed. Starting at different seeds can thus produce different samples. To secure reproducibility we have to set the seed. The simulation produces still a random sample but with our seed other people will get the same random values. The book authors apply the number 84735 for the `base::set.seed()` function. The number 84735 is a funny [reference to the name BAYES](https://bayes-rules.github.io/posts/fun/#why-84735).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the seed. Simulate 5 articles.\n\nbase::set.seed(84735)\ndplyr::slice_sample(article, n = 5, weight_by = prior, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   type\n#> 1 fake\n#> 2 fake\n#> 3 real\n#> 4 fake\n#> 5 fake\n```\n\n\n:::\n:::\n\n\nRun the above code several times to see that the values will *not* change. To demonstrate this behavior I will use the same code as in @lst-sample-results-1b but this time with the `set.seed()` function inside the loop.\n\n\n::: {.cell}\n\n```{#sample-results-2b .r .cell-code}\n# Each iteration uses the same seed, so all 5 samples are identical\nsample_results_2b <- purrr::map(1:4, ~ {\n  base::set.seed(84735)\n  fake_news |> \n    dplyr::slice_sample(n = 5) |> \n    dplyr::pull(type)\n}) |>\n  rlang::set_names(paste0(\"sample_\", 1:4)) |>\n  tibble::as_tibble()\n\nsample_results_2b |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|sample_1 |sample_2 |sample_3 |sample_4 |\n|:--------|:--------|:--------|:--------|\n|real     |real     |real     |real     |\n|real     |real     |real     |real     |\n|fake     |fake     |fake     |fake     |\n|fake     |fake     |fake     |fake     |\n|fake     |fake     |fake     |fake     |\n\n\n:::\n:::\n\n\n::: {#cau-002-set-seed-still-radnom .callout-caution}\n###### Using `seed()` still produces numbers randomly\n\nIt is important to understand that these results are still random. Reflecting the potential error and variability in simulation, different seeds would typically give different numerical results though similar conclusions.\n:::\n\n#### Simulate 10.000 article {#sec-002-simulate-prior-probabilities}\n\nNo let’s dream bigger: Let us simulate 10,000 articles and store the results in `article_sim` and display the result as a bar chart.\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-simulation-and-plot}\n: A bar plot of the fake vs real status of 10,000 simulated articles\n:::\n::::\n\n:::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-002-simulate-prior-probabilities .r .cell-code  lst-cap=\"A bar plot of the fake vs real status of 10,000 simulated articles\"}\n# Simulate 10000 articles.\nbase::set.seed(84735)\narticle_sim <-  dplyr::slice_sample(article,\n                                    n = 10000,\n                                    weight_by = prior,\n                                    replace = TRUE)\n\nggplot2::ggplot(article_sim, ggplot2::aes(x = type)) + \n  ggplot2::geom_bar(width = 0.6) +\n  ggplot2::theme(aspect.ratio = 3/1)\n```\n\n::: {.cell-output-display}\n![A bar plot of the fake vs real status of 10,000 simulated articles](002-bayes-rule_files/figure-html/fig-simulate-and-plot-1.png){#fig-simulate-and-plot width=672}\n:::\n:::\n\n\n\n::::\n:::::::\n\nReflecting the model @fig-simulate-and-plot from which these 10,000 articles were generated, *roughly* (but not exactly) 40% are fake:\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-simulation-table}\n: Numbers for the `fake` vs `real` status of 10,000 simulated articles\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-simulation-table .cell tbl-cap='Numbers for the `fake` vs `real` status of 10,000 simulated articles.'}\n\n```{.r .cell-code}\narticle_sim  |>  \n  janitor::tabyl(type)  |>  \n  janitor::adorn_totals(\"row\") |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|type  |     n| percent|\n|:-----|-----:|-------:|\n|fake  |  4031|  0.4031|\n|real  |  5969|  0.5969|\n|Total | 10000|  1.0000|\n\n\n:::\n:::\n\n:::\n::::::\n\n#### Calculate the exclamation point usage {#sec-002-calculate-conditional-probabilites}\n\nNext, let’s caluclate the exclamation point usage among these 10,000 articles. The `data_model` variable specifies that there’s a 26.67% chance that any fake article and a 2.22% chance that any real article uses exclamation points:\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-calculate-exclamation-point-usage}\n: Calculate exclamation point usage\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-calculate-conditional-probabilities}\n\n::: {.cell}\n\n```{.r .cell-code}\narticle_sim <- article_sim |>\n  dplyr::mutate(data_model = dplyr::case_when\n                (type == \"fake\" ~ 0.2667,\n                 type == \"real\" ~ 0.0222)\n                )\n\ndplyr::glimpse(article_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 10,000\n#> Columns: 2\n#> $ type       <chr> \"fake\", \"fake\", \"real\", \"fake\", \"fake\", \"real\", \"real\", \"re…\n#> $ data_model <dbl> 0.2667, 0.2667, 0.0222, 0.2667, 0.2667, 0.0222, 0.0222, 0.0…\n```\n\n\n:::\n:::\n\n\nCalculate exclamation point usage\n:::\n::::\n:::::::\n\n#### Simulate the exclamation point usage {#sec-002-simulate-conditional-probabilities}\n\nFrom this data_model, we can simulate whether each article includes an exclamation point. This syntax is a bit more complicated. First, the `dplyr::group_by()` statement specifies that the exclamation point simulation is to be performed separately for each of the 10,000 articles. Second, we use `base::sample()` to simulate the exclamation point data, `no` or `yes`, based on the `data_model` and store the results as `usage`. Note that `base::sample()` is similar to `slice_sample()` but samples values from vectors instead of rows from data frames.\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-simulate-exclamation-points}\n: Simulate whether each article includes an exclamation point\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-simulate-conditional-probabilities}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define whether there are exclamation points\ndata <- c(\"no\", \"yes\")\n\n# Simulate exclamation point usage with seed 3\nbase::set.seed(3)\narticle_sim2 <- article_sim |> \n  dplyr::group_by(1:dplyr::n()) |>  \n  dplyr::mutate(usage = base::sample(data, size = 1, \n                        prob = c(1 - data_model, data_model))) \narticle_sim2 |> \n  janitor::tabyl(usage, type) |> \n  janitor::adorn_totals(c(\"col\",\"row\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  usage fake real Total\n#>     no 2961 5833  8794\n#>    yes 1070  136  1206\n#>  Total 4031 5969 10000\n```\n\n\n:::\n:::\n\n\nSimulate whether each article includes an exclamation point\n:::\n::::\n:::::::\n\nThe `article_sim` data frame now contains 10,000 simulated articles with different features, summarized in the table below. The patterns here reflect the underlying likelihoods that *roughly* 28% (1070 / 4031) of fake articles and 2% (136 / 5969) of real articles use exclamation points.\n\n::: {#wrn-002-different-seeds.differ-in-magnitude .callout-warning}\n###### Different seeds don't guarantee similar magnitudes\n\nNote that the simulation in @lst-002-simulate-conditional-probabilities uses as seed the number 3 and not the previous seed number 84735. The reason is that with `base::set.seed(84735)` we will get extreme different values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate exclamation point usage with seed 84735\nbase::set.seed(84735)\narticle_sim |> \n  dplyr::group_by(1:dplyr::n()) |>  \n  dplyr::mutate(usage2 = base::sample(data, size = 1, \n                        prob = c(1 - data_model, data_model))) |> \n  janitor::tabyl(usage2, type) |> \n  janitor::adorn_totals(c(\"col\",\"row\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  usage2 fake real Total\n#>      no 1329 5969  7298\n#>     yes 2702    0  2702\n#>   Total 4031 5969 10000\n```\n\n\n:::\n:::\n\n\nWith seed `84735`, we got 0 real articles with exclamation points, which is extremely unlikely given that `data_model = 0.0222` for real articles. We know that different seeds create (small) different random number sequences that propagate through all 10,000 decisions. This random variations can accumulate - with 10,000 samples, small differences in the random sequence can therefore compound.\n\nThe key insight: **Different seeds don't guarantee similar magnitudes - they just ensure reproducibility.** Some seeds will produce results closer to the expected probabilities, while others (like 84735) may produce more extreme outcomes by chance.\n\nIf we want more stable results that better reflect the true probabilities, we could:\n\n-   Use a larger sample (we already have a sample of 10,000, which is good).\n-   Run multiple simulations and average to see the long-run behavior.\n-   Check if specific seeds are creating outliers.\n:::\n\n@fig-exclamation-point-usage-1 provides a visual summary of these article characteristics. Whereas the left plot reflects the relative breakdown of exclamation point usage among real and fake news, the right plot frames this information within the normalizing context that only *roughly* 12% (1206 / 10000) of all articles use exclamation points (see @lst-002-simulate-conditional-probabilities).\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-exclamation-point-usage}\n: Bar plots of exclamation point usage, both within fake vs real news and overall (Version 1)\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot2::ggplot(article_sim2, ggplot2::aes(x = type, fill = usage)) + \n  ggplot2::geom_bar(position = \"fill\", width = 0.6) + \n  ggplot2::theme(aspect.ratio = 3/1) +\n  ggplot2::scale_fill_viridis_d(option = \"E\")\np2 <- ggplot2::ggplot(article_sim2, ggplot2::aes(x = usage)) + \n  ggplot2::geom_bar(width = 0.6) + \n  ggplot2::theme(aspect.ratio = 3/1)\n\npatchwork:::\"-.ggplot\"(p1, p2)\n```\n\n::: {.cell-output-display}\n![Bar plots of exclamation point usage, both within fake vs real news and overall (Version 1).](002-bayes-rule_files/figure-html/fig-exclamation-point-usage-1-1.png){#fig-exclamation-point-usage-1 width=672}\n:::\n:::\n\n:::\n::::::\n\nIn the book the code for the right plot in @fig-exclamation-point-usage-1 is wrong. It says `ggplot2::ggplot(article_sim, ggplot2::aes(x = type))` instead of `ggplot2::ggplot(article_sim, ggplot2::aes(x = usage))` (see: <https://www.bayesrulesbook.com/chapter-2#fig:ch2-bars-articles>).\n\nI believe that even this correction is not enough as the right plot should not only show the total numbers of exclamation points but also their relation to the article types as shown in @fig-exclamation-point-usage-2.\n\n:::::: my-r-code\n:::: my-r-code-header\n<div>\n\n: Bar plots of exclamation point usage, both within fake vs real news and overall (Version2)\n\n</div>\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot 1: Percentage of exclamation usage within each article type\np1 <- ggplot2::ggplot(article_sim2, ggplot2::aes(x = type, fill = usage)) + \n  ggplot2::geom_bar(position = \"fill\", width = 0.6) +\n  ggplot2::theme(aspect.ratio = 3/1) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::labs(\n    title = \"Exclamation Usage\\nby Article Type\",\n    x = \"Article Type\",\n    y = \"Percentage\",\n    fill = \"Usage\"\n  ) +\n  ggplot2::scale_fill_viridis_d(option = \"E\")\n\n# Plot 2: Overall percentage distribution of article types\np2 <- ggplot2::ggplot(article_sim2, ggplot2::aes(x = usage, fill = type)) + \n  ggplot2::geom_bar(width = 0.6) +\n  ggplot2::theme(aspect.ratio = 3/1) +\n  ggplot2::labs(\n    title = \"Article Type\\nby Exclamation Usage\",\n    x = \"Exclamation Usage\",\n    y = \"Count\",\n    fill = \"Type\"\n  ) +\n  ggplot2::scale_fill_viridis_d(option = \"E\")\n\n# Combine plots side by side\npatchwork:::\"-.ggplot\"(p1, p2)\n```\n\n::: {.cell-output-display}\n![Bar plots of exclamation point usage, both within fake vs real news and overall (Version 2).](002-bayes-rule_files/figure-html/fig-exclamation-point-usage-2-1.png){#fig-exclamation-point-usage-2 width=672}\n:::\n:::\n\n:::\n::::::\n\n#### Display posterior probabilities {#sec-002-display-posterior-probabilities}\n\nAmong the 1206 simulated articles that use exclamation points, roughly 88.7% are fake. This approximation is quite close to the actual posterior probability of 0.889. Of course, our posterior assessment of this article would change if we had seen different data, i.e., if the title didn’t have exclamation points. Figure 2.4 reveals a simple rule: If an article uses exclamation points, it’s most likely fake. Otherwise, it’s most likely real (and we should read it).\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-plot-exlamation-usage}\n: Bar plots of real vs fake news, broken down by exclamation point usage\n:::\n::::\n\n:::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-002-display-posterior-probabilities .r .cell-code  lst-cap=\"Bar plots of real vs fake news, broken down by exclamation point usage.\"}\nggplot2::ggplot(article_sim2, ggplot2::aes(x = type)) + \n  ggplot2::geom_bar(width = 0.6) + \n  ggplot2::theme(aspect.ratio = 3/1) +\n  ggplot2::facet_wrap(~ usage) \n```\n\n::: {.cell-output-display}\n![Bar plots of real vs fake news, broken down by exclamation point usage.](002-bayes-rule_files/figure-html/fig-plot-exlamation-usage-1.png){#fig-plot-exlamation-usage width=672}\n:::\n:::\n\n\n\n::::\n:::::::\n\n#### Summary\n\nTo integrate the different simulation steps into the previous seection I will rename the action taken. Instead to focus about the concrete action (article types, exclamation usage) I will address the steps in general Bayesian terminology: From the prior to the posterior distribution we followed different steps of calculations and simulations:\n\n1.  Define the prior probabilities: See @lst-002-define-prior-probabilities.\n2.  Simulate the prior probabilities with many events ($\\approx$ 10000 or more): See @lst-002-simulate-prior-probabilities.\n3.  Specify the data model with the conditional probabilities: See @lst-002-calculate-conditional-probabilities.\n4.  Simulate the conditional probabilities: See @lst-002-simulate-conditional-probabilities.\n5.  Display the posterior probabilities: See @lst-002-display-posterior-probabilities.\n\n## Example: Pop vs soda vs coke\n\n### Introduction\n\nThere exist different preferences for carbonated soft drinks in different region of the United States. We would like to know the (posterior) probability where a person comes from if (s)he says \"Pop\" to fizzy drinks. The population distribution gives us the prior probability.\n\nThe book uses rounded population data from 2020. The actual data from 2024 shows that there was a relative increase of the south population growth by 1 percent at the cost of the other three regions. The US population is growing by one person every 22 seconds.\n\n:::::: my-resource\n:::: my-resource-header\n::: {#lem-002-us-propulation}\n: References to US Population Statistics\n:::\n::::\n\n::: my-resource-container\n-   [Census Regions and Divisions of the United States](https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf)\n-   [US and World Population Clock](https://www.census.gov/popclock/)\n-   [United States Population Growth by Region](https://www.census.gov/popclock/data_tables.php?component=growth)\n:::\n::::::\n\n### Prior model {#sec-002-prior-model-pop-vs-soda}\n\n| Region    |  Population | Percentage |\n|:----------|------------:|-----------:|\n| Northeast |  57,431,458 |      17.3% |\n| Midwest   |  68,984,258 |      20.8% |\n| West      |  78,685,455 |      23.7% |\n| South     | 126,476,549 |      38.1% |\n| Total     | 331,577,720 |     100.0% |\n\n: Prior model of U.S. region in 2020 {#tbl-prior-model-us-region} {.striped}\n\nThis prior model is summarized in @tbl-prior-model-us-region. Notice that the South is the most populous region and the Northeast the least ($P (S) > P (N )$). Thus, based on population statistics alone, there’s a 38% prior probability that the interviewee lives in the South: $P (S) = 0.38$.\n\n### Conditional probability {#sec-002-conditional-probability-pop-vs-soda}\n\nTo combine the prior probability with data about usage of the word \"Pop\" we need the regional distribution of the preferred word for fizzy drinks. This would give us a better estimation where the person comes from.\n\nTo evaluate this data, we can examine the `pop_vs_soda` dataset in the {**bayesrules**} package [@bayesrules] which includes 374250 responses to a volunteer survey conducted at [popvssoda.com](https://popvssoda.com/). To learn more about this dataset, look at [Pop vs Soda vs Coke](https://bayes-rules.github.io/bayesrules/docs/reference/pop_vs_soda.html) and inspect the random small dataset at @lst-002-random-glance-pop_vs_soda-data.\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-random-glance-pop_vs_soda-data}\n: Random glance at the `pop_vs_soda` dataset from {**bayesrules**}\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-random-glance-pop_vs_soda-data}\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_glance_data(bayesrules::pop_vs_soda) |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|    obs|state       |region    |word_for_cola |pop   |\n|------:|:-----------|:---------|:-------------|:-----|\n|      1|alabama     |south     |pop           |TRUE  |\n|  46208|connecticut |northeast |soda          |FALSE |\n|  54425|florida     |south     |coke          |FALSE |\n|  61413|georgia     |south     |soda          |FALSE |\n|  61605|georgia     |south     |coke          |FALSE |\n|  74362|illinois    |midwest   |pop           |TRUE  |\n|  99556|indiana     |midwest   |pop           |TRUE  |\n| 102258|indiana     |midwest   |coke          |FALSE |\n| 335380|texas       |south     |coke          |FALSE |\n| 374250|wyoming     |west      |other         |FALSE |\n\n\n:::\n:::\n\n\nA random glance at the `pop_vs_soda` dataset from {**bayesrules**}\n:::\n::::\n:::::::\n\n### Likelihood {#sec-002-likelihood-pop-vs-soda}\n\nThough the survey participants aren’t directly representative of the regional populations (@tbl-prior-model-us-region), we can use their responses to approximate the likelihood of people using the word pop in each region.\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-summarize-pop-per-region}\n: Summarize the usage of the Word \"Pop\" per region\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-summarize-pop-per-region\n\n::: {#tbl-summarize-pop-per-region .cell layout-ncol=\"2\" tbl-cap='Summarize the usage of the Word \\'Pop\\' per region' tbl-subcap='[\"Absolute\",\"Percentages\"]'}\n\n```{.r .cell-code}\nlibrary(bayesrules)\n\n# Load the data\ndata(pop_vs_soda)\n\n# Summarize pop use by region in absolute\npop_vs_soda |> \n  janitor::tabyl(pop, region) |> \n  janitor::adorn_totals(\"row\") |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|pop   | midwest| northeast| south|  west|\n|:-----|-------:|---------:|-----:|-----:|\n|FALSE |   51552|     56085| 83021| 43615|\n|TRUE  |   93544|     21103|  7143| 18187|\n|Total |  145096|     77188| 90164| 61802|\n\n\n:::\n\n```{.r .cell-code}\n# Summarize pop use by region in percentage\npop_vs_soda |> \n  janitor::tabyl(pop, region) |> \n  janitor::adorn_percentages(\"col\") |> \n  janitor::adorn_totals(\"row\") |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|pop   | midwest| northeast|  south|   west|\n|:-----|-------:|---------:|------:|------:|\n|FALSE |  0.3553|    0.7266| 0.9208| 0.7057|\n|TRUE  |  0.6447|    0.2734| 0.0792| 0.2943|\n|Total |  1.0000|    1.0000| 1.0000| 1.0000|\n\n\n:::\n:::\n\n:::\n::::\n:::::::\n\nLetting $A$ denote the event that a person uses the word \"pop,\" we’ll thus assume the following regional **likelihoods**:\n\n$L(M \\mid A) = 0.6447$, $L(N \\mid A) = 0.2734$, $L(S \\mid A) = 0.0792$, $L(W \\mid A) = 0.2943$.\n\nFor example, 64.47% of people in the Midwest but only 7.92% of people in the South use the term “pop.” Comparatively then, the “pop” data is most likely if the interviewee lives in the Midwest and least likely if they live in the South, with the West and Northeast being in between these two extremes:\n\n$$L(M \\mid A) > L(W \\mid A) > L(N \\mid A) > L(S\\mid A)$$\n\n### Marginal Probability {#sec-002-marginal-probabilities-pop-vs-soda}\n\nWeighing the prior information about regional populations with the data that the interviewee used the word “pop,” what are we to think now? For example, considering the fact that 38% of people live in the South but that “pop” is relatively rare to that region, what’s the posterior probability that the interviewee lives in the South? Per Bayes’ Rule (@eq-002-bayes-rule-1 resp. @eq-002-bayes-rule-general), we can calculate this probability by\n\n$$\nP (S \\mid A) =  \\frac{P (S) \\cdot L(S \\mid A)}{P (A)}\n$$ {#eq-002-bayes-rule-pop}\n\nWe already have two of the three necessary pieces of the puzzle, the <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>prior probability</a> $P (S)$ and the <a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a> $L(S \\mid A)$. Consider the third, the <a class='glossary' title='Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table.'>marginal probability</a> that a person uses the term “pop” across the entire U.S., $P (A)$. By extending the Law of Total Probability (@eq-002-ltp), we can calculate $P (A)$ by combining the likelihoods of using “pop” in each region, while accounting for the regional populations (using the rounded percentages of @tbl-prior-model-us-region). Accordingly, there’s a 28.28% chance that a person in the U.S. uses the word “pop”:\n\n$$\n\\begin{align*}\nP (A) &= L(M \\mid A) \\cdot P (M ) + L(N \\mid A) \\cdot P (N ) + L(S\\mid A) \\cdot P (S) + L(W \\mid A) \\cdot P (W) \\\\\n&= 0.6447 ⋅ 0.21 + 0.2734 ⋅ 0.17 + 0.0792 ⋅ 0.38 + 0.2943 ⋅ 0.24 \\\\\n&\\approx 0.2826\n\\end{align*}\n$$\n\n### Posterior model {#sec-002-posterior-model-pop-vs-soda}\n\nThen plugging into (@eq-002-bayes-rule-pop), there’s a roughly 10.65% posterior chance that the interviewee lives in the South:\n\n$$P (S \\mid A) =  \\frac{0.38 ⋅ 0.0792} {0.2828} ≈ 0.1065$$\n\nWe can similarly update our understanding of the interviewee living in the Midwest, Northeast, or West. @tbl-002-prior-and-posterior-model-pop summarizes the resulting posterior model of region alongside the original prior. Upon hearing the interviewee use “pop,” we now think it’s most likely that they live in the Midwest and least likely that they live in the South, despite the South being the most populous region.\n\n| region                | M      | N      | S      | W      | Total |\n|-----------------------|--------|--------|--------|--------|-------|\n| prior probability     | 0.21   | 0.17   | 0.38   | 0.24   | 1     |\n| posterior probability | 0.4791 | 0.1645 | 0.1065 | 0.2499 | 1     |\n\n: Posterior model of regional usages \"pop\" for carbonated soft drinks {#tbl-002-prior-and-posterior-model-pop} {.striped}\n\n## Building a Bayesian model for numerical random variables {#sec-002-discrete-variable-model}\n\nSo far we had worked with categorical variable. The principles for building Bayesian models of numerical random variables are the same but differ in some details. We will explore these differences with the example of the chess games between Gary Kasparov and the IBM supercomputer Deep Blue. Of the six games played in 1996, Kasparov won three, drew two, and lost one. Thus, Kasparov won the overall match.\n\n### Prior Probability Model {#sec-002-prior-probability-chess-game}\n\nYet Kasparov and Deep Blue were to meet again for a six-game match in 1997. Let $\\pi$ denote Kasparov’s chances of winning any particular game in the re-match. (Greek letters are conventionally used to denote quantitative variables of interest.) Thus,$\\pi$ is a measure of his overall skill relative to Deep Blue. Given the complexity of chess, machines, and humans, $\\pi$ is unknown and can vary or fluctuate over time. Or, in short, $\\mathbf{\\pi}$ is a numerical random variable.\n\nAs with the fake news analysis, our analysis of random variable $\\pi$ will start with a prior model which\n\n(1) identifies what values $\\pi$ can take, and\n(2) assigns a prior weight or probability to each, where\n(3) these probabilities sum to 1.\n\n| $\\mathbf{\\pi}$ | 0.2  | 0.5  | 0.8  | Total |\n|----------------|------|------|------|-------|\n| f($\\pi$)       | 0.10 | 0.25 | 0.65 | 1     |\n\n: Prior model of $\\pi$ Kasparov’s chance of beating Deep Blue {#tbl-002-prior-model-chess}\n\nConsider the prior model defined in @tbl-002-prior-model-chess. We’ll get into how we might build such a prior in later chapters. For now, let’s focus on interpreting and utilizing the given prior:\n\nThe first thing you might notice is that this model greatly simplifies reality. Though Kasparov’s win probability $\\pi$ can *technically* be any number from zero to one, this prior assumes that $\\pi$ has a discrete set of possibilities: Kasparov’s win probability is either 20%, 50%, or 80%. Next, examine the <a class='glossary' title='A probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value. The pmf is a mathematical function that gives the probability that a discrete random variable is exactly equal to some specific value. It is the primary way to define the probability distribution of a discrete random variable. (Brave-AI) Sometimes it is also known as probability function, frequency function or discrete probability density function. (Wikipedia)'>probability mass function</a> (pmf) $f (\\cdot)$ which specifies the prior probability of each possible $\\pi$ value. This pmf reflects the prior understanding that Kasparov learned from the 1996 match-up, and so will most likely improve in 1997. Specifically, this pmf places a 65% chance on Kasparov’s win probability jumping to $\\pi$ = 0.8 and only a 10% chance on his win probability dropping to $\\pi$ = 0.2, i.e., f ($\\pi$ = 0.8) = 0.65 and f ($\\pi$ = 0.2) = 0.10.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-discrete-probability-model}\n: Discrete Probability Model\n:::\n::::\n\n::: my-theorem-container\nLet $Y$ be a discrete random variable. The probability model of $Y$ is specified by a <a class='glossary' title='A probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value. The pmf is a mathematical function that gives the probability that a discrete random variable is exactly equal to some specific value. It is the primary way to define the probability distribution of a discrete random variable. (Brave-AI) Sometimes it is also known as probability function, frequency function or discrete probability density function. (Wikipedia)'>probability mass function</a> (pmf) $f (y)$. This pmf defines the probability of any given outcome $y$,\n\n$$\nf (y) = P (Y = y)\n$$ {#eq-002-discrete-probability-model}\n\nand has the following properties:\n\n-   $0 ≤ f (y) ≤ 1 \\text{ for all y}$; and\n-   $\\sum_{\\text{all y}} f (y) = 1$, i.e., the probabilities of all possible outcomes $y$ sum to $1$.\n:::\n::::::\n\n### Binomial Data Model {#sec-002-conditional-probabilities-chess-game}\n\nIn the second step of our Bayesian analysis, we’ll collect and process data which can inform our understanding of $\\pi$, Kasparov’s skill level relative to that of Deep Blue. Here, our data $Y$ is the number of the six games in the 1997 re-match that Kasparov wins. Since the chess match outcome isn’t predetermined, $Y$ is a **random variable** that can take any value in {0, 1, ..., 6}. Further, $Y$ inherently depends upon Kasparov’s win probability $\\pi$. If $\\pi$ were 0.80, Kasparov’s victories $Y$ would also tend to be high. If $\\pi$ were 0.20, $Y$ would tend to be low. For our formal Bayesian analysis, we must model this dependence of $Y$ on $\\pi$. That is, we must develop a <a class='glossary' title='Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for “conditional on” or “given that”.'>conditional probability model</a> of how $Y$ depends upon or is conditioned upon the value of $\\pi$.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-conditional-propbability-model}\n: Conditional probability model of data $Y$\n:::\n::::\n\n::: my-theorem-container\nLet $Y$ be a discrete random variable and $\\pi$ be a parameter upon which $Y$ depends. Then the conditional probability model of $Y$ given $\\pi$ is specified by conditional pmf $f (y \\mid \\pi)$. This pmf specifies the conditional probability of observing $y$ given $\\pi$,\n\n$$\n\\begin{align*}\nf (y \\mid \\pi) = P (Y = y \\mid \\pi)\n\\end{align*}\n$$ {#eq-002-conditional-probability-model}\n\nand has the following properties:\n\n-   $0 ≤ f (y \\mid \\pi) ≤ 1 \\text{ for all y}$; and\\\n-   $\\sum_{\\text{all y}} f (y \\mid \\pi) = 1$.\n:::\n::::::\n\nIn modeling the dependence of $Y$ on $\\pi$ in our chess example, we first make two assumptions about the chess match:\n\n(1) the outcome of any one game doesn’t influence the outcome of another, i.e., games are **independent**; and\n(2) Kasparov has an **equal probability**, $\\pi$, of winning any game in the match, i.e., his chances don’t increase or decrease as the match goes on.\n\nThis is a common framework in statistical analysis, one which can be represented by the <a class='glossary' title='Binomial model is a foundational framework in Bayesian statistics used to estimate the probability of success in a series of independent trials. It combines a binomial likelihood — which models the number of successes in a fixed number of trials—with a beta prior on the success probability, resulting in a beta posterior. This conjugate relationship allows for analytical computation and intuitive interpretation. (Brave-AI)'>Binomial model</a>.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-binomial-model}\n: Binomial Model\n:::\n::::\n\n::: my-theorem-container\nThe Binomial model Let random variable Y be the number of successes in a fixed number of trials $n$. Assume that the trials are independent and that the probability of success in each trial is $\\p$. Then the conditional dependence of $Y$ on $\\pi$ can be modeled by the <a class='glossary' title='Binomial model is a foundational framework in Bayesian statistics used to estimate the probability of success in a series of independent trials. It combines a binomial likelihood — which models the number of successes in a fixed number of trials—with a beta prior on the success probability, resulting in a beta posterior. This conjugate relationship allows for analytical computation and intuitive interpretation. (Brave-AI)'>Binomial model</a> with parameters $n$ and $\\pi$. In mathematical notation:\n\n$$Y \\mid \\pi \\sim Bin(n, \\pi)$$ {#eq-002-binomial-model}\n\nwhere “∼” can be read as “modeled by.” Correspondingly, the Binomial model is specified by the <a class='glossary' title='The Conditional Probability Mass Function (PMF) is the probability distribution of a discrete random variable X given that another discrete random variable Y has taken a specific value y. It is defined using the joint probability mass function and the marginal probability mass function.'>Conditional Probability Mass Function</a>\n\n$$\n\\begin{align*}\nf (y \\mid \\pi) = \\binom{n}{y} \\pi^y(1 − $\\pi$)^{n−y} \\text{ for y} \\in \\{0, 1, 2, . . . , n\\}\n\\end{align*}\n$$ {#eq-002-CPMF}\n\nwhere $\\binom{n}{y} = \\frac{n!}{y!(n−y)!}$\n:::\n::::::\n\nWe can now say that the dependence of Kasparov’s victories Y in n = 6 games on his win probability $\\pi$ follows a Binomial model,\n\n$$Y \\mid \\pi \\sim Bin(6, \\pi)$$\n\nwith conditional pmf\n\n$$f (y \\mid \\pi) = \\binom{6}{y} \\pi^y(1 − \\pi)^{6−y} \\text{ for y} \\in \\{0, 1, 2, 3, 4, 5, 6\\}$$ {#eq-002-concitional-pmf-chess}\n\nThis pmf summarizes the conditional probability of observing any number of wins $Y = y$ for any given win probability $\\pi$. For example, if Kasparov’s underlying chance of beating Deep Blue were $\\pi$ = 0.8, then there’s a roughly 26% chance he’d win all six games:\n\n$$f (y = 6 \\mid \\pi = 0.8) = \\binom{6}{6} 0.8^6(1 − 0.8)^{6−6} = 1 ⋅ 0.8^6 ⋅ 1 ≈ 0.26$$ {#eq-002-example-binom-1}\n\nAnd a near 0 chance he’d lose all six games:\n\n$$f (y = 0 \\mid \\pi = 0.8) = \\binom{6}{0} 0.80(1 − 0.8)^{6−0} = 1 ⋅ 1 ⋅ 0.2^6 \\approx 0.000064$$ {#eq-002-example-binom-2}\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-example-pmf}\n: Two example PMFs calculated with R\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-example-pmf}\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::dbinom(x = 6, size = 6, prob = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.262144\n```\n\n\n:::\n\n```{.r .cell-code}\nstats::dbinom(x = 0, size = 6, prob = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 6.4e-05\n```\n\n\n:::\n:::\n\n\n6 and 0 wins of 6 trials: PMF when Kasparov’s chance of beating Deep Blue $\\pi$ = 0.8.\n:::\n::::\n:::::::\n\nI have reproduced the manual calculation of @eq-002-example-binom-1, and @eq-002-example-binom-2 with R.\n\n@lst-002-binomial-model-chess plots the conditional pmfs $f (y \\mid \\pi)$, and thus the random outcomes of $Y$ , under each possible value of Kasparov’s win probability $\\pi$. These plots confirm our intuition that Kasparov’s victories $Y$ would tend to be low if Kasparov’s win probability $\\pi$ were low (far left) and high if $\\pi$ were high (far right).\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-binomial-model-chess}\n: The Probability Mass Function of a $Bin(6, \\pi)$ model from @tbl-002-prior-model-chess\n:::\n::::\n\n:::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-002-binomial-model-chess .r .cell-code  lst-cap=\"Probability Mass Function of a $Bin(6, \\pi)$ model\"}\n# Create data frame with all combinations\ndf <- base::expand.grid(\n  x = 0:6,  # number of successes (0 to size)\n  prob = c(0.2, 0.5, 0.8)\n) |> \n  dplyr::mutate(\n    probability = stats::dbinom(x, size = 6, prob = prob),\n    prob_label = base::paste(\"Bin(6, \", prob, \")\"),\n    highlight = dplyr::if_else(x == 1, \"black\", \"gray\")\n  )\n\n# Create lollipop chart with facets\nggplot2::ggplot(df, ggplot2::aes(x = x, y = probability)) +\n  ggplot2::geom_linerange(\n    ggplot2::aes(\n      x = x, \n      ymin = 0, \n      ymax = probability, \n      color = highlight), \n    linewidth = 0.8) +\n  ggplot2::geom_point(ggplot2::aes(color = highlight), size = 3) +\n  ggplot2::scale_color_identity() +\n  ggplot2::facet_wrap(~ prob_label, ncol = 3) +\n  ggplot2::scale_x_continuous(breaks = 0:6) +\n  ggplot2::scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +\n  ggplot2::labs(\n    x = \"Number of Successes\",\n    y = \"Probability\",\n    title = \"Binomial Distribution (n = 6)\"\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(\n    panel.grid.minor.y = ggplot2::element_line(color = \"gray90\", linewidth = 0.2),\n    panel.grid.major.y = ggplot2::element_line(color = \"gray60\", linewidth = 0.5),\n    panel.grid.major.x = ggplot2::element_line(color = \"gray80\", linewidth = 0.3),\n    panel.spacing = grid::unit(1.5, \"lines\"),\n    strip.background = ggplot2::element_rect(fill = \"gray90\", color = \"gray50\"),\n    strip.text = ggplot2::element_text(face = \"bold\", size = 11)\n  )\n```\n\n::: {.cell-output-display}\n![The pmf of a $Bin(6, \\pi)$ model is plotted for each possible value of $\\pi \\in \\{0.2, 0.5, 0.8\\}$. The masses marked by the black lines correspond to the eventual observed data, $Y = 1$ win.](002-bayes-rule_files/figure-html/fig-002-binomial-model-chess-1.png){#fig-002-binomial-model-chess width=672}\n:::\n:::\n\n\n::::\n:::::::\n\n### Binomial Likelihood Function {#sec-002-likelihood-chess-game}\n\nThe Binomial provides a theoretical model of the data $Y$ we might observe. In the end, Kasparov only won one of the six games against Deep Blue in 1997 ($Y = 1$). Thus, the next step in our Bayesian analysis is to determine how compatible this particular data is with the various possible $\\pi$. Put another way, we want to evaluate the <a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a> of Kasparov winning $Y = 1$ game under each possible $\\pi$. It turns out that the answer is staring us straight in the face. Extracting only the masses in @lst-002-binomial-model-chess and @fig-002-binomial-model-chess that correspond to our observed data, $Y = 1$, reveals the likelihood function of $\\pi$ (@fig-002-binomial-likelhood-chess).\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-fig-binomial-likelhood-chess}\n: Binomial Likelihood Function for Kasparov winning $Y = 1$ game under each possible $\\pi$\n:::\n::::\n\n:::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-002-fig-binomial-likelhood-chess .r .cell-code  lst-cap=\"Likelihood function $L(\\pi \\mid y = 1)$ of observing $Y = 1$ win in six games\"}\n# Create a data frame with the probabilities\ndata <- tibble::tibble(\n  prob = c(0.2, 0.5, 0.8),\n  k = 1,\n  size = 6,\n  prob_k = dbinom(1, size = 6, prob = c(0.2, 0.5, 0.8))\n)\n\n# Create the lollipop chart\nggplot2::ggplot(data, ggplot2::aes(x = prob, y = prob_k, group = prob)) +\n  ggplot2::geom_segment(ggplot2::aes(xend = prob, yend = 0), color = \"black\") +\n  ggplot2::geom_point(size = 3, color = \"blue\") +\n  ggplot2::labs(\n    x = \"Probability of Success (p)\", \n    y = \"Probability of Exactly 1 Success\"\n    ) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(aspect.ratio = 3/1) +\n  ggplot2::scale_x_continuous(breaks = data$prob, labels = base::round(data$prob, 2))\n```\n\n::: {.cell-output-display}\n![Likelihood function $L(\\pi \\mid y = 1)$ of observing $Y = 1$ win in six games for any win probability $\\pi \\in \\{0.2, 0.5, 0.8\\}$](002-bayes-rule_files/figure-html/fig-002-binomial-likelhood-chess-1.png){#fig-002-binomial-likelhood-chess width=672}\n:::\n:::\n\n\n::::\n:::::::\n\nJust as the likelihood in our fake news example was obtained by flipping a conditional probability on its head, the formula for the likelihood function follows from evaluating the conditional pmf $f (y \\mid \\pi)$ in @eq-002-concitional-pmf-chess at the observed data $Y = 1. \\text{ For } \\pi \\in \\{0.2, 0.5, 0.8\\}$,\n\n$$\n\\begin{align*}\nL(\\pi \\mid y = 1) = f (y = 1 \\mid \\pi) = \\binom{6}{1} \\pi^1(1 − \\pi)^{6−1} = 6 \\pi (1 − \\pi)^5.\n\\end{align*}\n$$\n\n@tbl-002-binomial-likelhood-chess summarizes the likelihood function evaluated at each possible value of $\\pi$. For example, there’s a low 0.0015 likelihood of Kasparov winning just one game if he were the superior player, i.e., $\\pi = 0.8$:\n\n$$\n\\begin{align*}\nL(\\pi = 0.8 \\mid y = 1) = 6 \\cdot 0.8 \\cdot (1 − 0.8)^5 \\approx 0.0015.\n\\end{align*}\n$$\n\nThere are some not-to-miss details here:\n\n(1) First, though it is equivalent in *formula* to the conditional pmf of $Y , f (y = 1 \\mid \\pi)$, we use the $L(\\pi \\mid y = 1)$ notation to reiterate that the likelihood is a function of the unknown win probability $\\pi$ given the observed $Y = 1$ win data. In fact, the resulting likelihood formula depends only upon $\\pi$.\n(2) Further, the likelihood function does not sum to one across $\\pi$, and thus is not a probability model. Rather, it provides a mechanism by which to compare the compatibility of the observed data $Y = 1$ with different $\\pi$.\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-tbl-binomial-likelhood-chess}\n: Table of the Binomial Likelihood Function for Kasparov winning $Y = 1$ game under each possible $\\pi$.\n:::\n::::\n\n:::: my-r-code-container\n\n::: {#tbl-002-binomial-likelhood-chess .cell tbl-cap='Likelihood function of $\\pi$ given Kasparov won 1 of 6 games'}\n\n```{#lst-002-binomial-likelhood-chess .r .cell-code  lst-cap=\"Likelihood function of $\\pi$ given Kasparov won 1 of 6 games\"}\nlikelihood <- tibble::tibble(data) |> \n  dplyr::select(prob, prob_k) |> \n  tidyr::pivot_wider(\n    names_from = prob,\n    values_from = prob_k\n  ) |> \n  tibble::add_column(\"π\" = \"L(π∣y = 1)\", .before = \"0.2\")\n  \nlikelihood |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|π          |      0.2|     0.5|      0.8|\n|:----------|--------:|-------:|--------:|\n|L(π∣y = 1) | 0.393216| 0.09375| 0.001536|\n\n\n:::\n:::\n\n\n::::\n:::::::\n\nPutting this all together, the likelihood function summarized in @lst-002-fig-binomial-likelhood-chess and @lst-002-binomial-likelhood-chess illustrates that Kasparov’s one game win is *most* consistent with him being the weaker player and *least* consistent with him being the better player: $L(\\pi = 0.2 \\mid y = 1) > L(\\pi = 0.5 \\mid y = 1) > L(\\pi = 0.8\\mid y = 1)$. In fact, it’s nearly impossible that Kasparov would have only won one game if his win probability against Deep Blue were as high as $\\pi = 0.8: L(\\pi = 0.8 \\mid y = 1) \\approx 0$.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-pmf-vs-likelihood-function}\n: Probability Mass Function versus Likelihood Function\n:::\n::::\n\n::: my-theorem-container\nWhen $\\pi$ is known, the <a class='glossary' title='The Conditional Probability Mass Function (PMF) is the probability distribution of a discrete random variable X given that another discrete random variable Y has taken a specific value y. It is defined using the joint probability mass function and the marginal probability mass function.'>conditional pmf</a> $f (\\cdot \\mid \\pi)$ allows us to compare the probabilities of different possible values of data $Y$ (e.g., $y_{1}$ or $y_{2}$) occurring with $\\pi$:\n\n$$\nf (y_{1} \\mid \\pi) \\text{ vs } f (y_{2} \\mid \\pi)\n$$\n\nWhen $Y = y$ is known, the likelihood function $L(\\cdot \\mid y) = f (y\\mid \\cdot)$ allows us to compare the relative <a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a> of observing data y under different possible values of $\\pi$ (e.g., $\\pi_{1}$ or $\\pi_{2}$):\n\n$$\nL(\\pi_{1} \\mid y) \\text{ vs } L(\\pi_{2} \\mid y)\n$$\n\nThus, $L(\\cdot \\mid y)$ provides the tool we need to evaluate the relative compatibility of data $Y = y$ with various $\\pi$ values.\n:::\n::::::\n\n### Normalizing Constant {#sec-002-normalizing-constant-chess-game}\n\nConsider where we are: In contrast to our prior model of $\\pi$ (@tbl-002-prior-model-chess), our 1997 chess match data provides evidence that Deep Blue is now the dominant player (@tbl-002-binomial-likelhood-chess). As Bayesians, we want to *balance* this prior and likelihood information.\n\nOur mechanism for doing so, Bayes’ Rule, requires three pieces of information: the prior, likelihood, and a <a class='glossary' title='In Bayesian statistics, the normalizing constant ensures that the posterior distribution integrates to 1, making it a valid probability distribution. It is also known as the marginal likelihood or evidence, and appears in the denominator of Bayes’ theorem. The integral for the normalizing constant is often intractable for complex models because it involves high-dimensional parameter spaces and there exists no closed-form solution for most realistic models. As a result, approximation methods like Markov Chain Monte Carlo (MCMC), bridge sampling, or variational inference are used to estimate or bypass the normalizing constant. (Brave-AI)'>normalizing constant</a>. We’ve taken care of the first two, now let’s consider the third. To this end, we must determine the <a class='glossary' title='Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes’ Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI)'>total probability</a> that Kasparov would win $Y = 1$ game across all possible win probabilities $\\pi, f (y = 1)$. As we did in our other examples, we can appeal to the <a class='glossary' title='The law of total probability (LTP) is a fundamental rule in probability theory that relates marginal probabilities to conditional probabilities. It expresses the total probability of an event that can occur through several distinct, mutually exclusive, and collectively exhaustive scenarios. It is particularly useful when direct computation of an event’s probability is difficult, and it enables breaking down complex probability problems into simpler, manageable components. (Brave-AI)'>Law of Total Probability</a> (LTP) to calculate $f (y = 1)$.\n\nThe idea is this: The overall probability of Kasparov’s one win outcome ($Y = 1$) is the sum of its parts: the likelihood of observing $Y = 1$ with a win probability $\\pi$ that’s either 0.2, 0.5, or 0.8 *weighted by* the prior probabilities of these $\\pi$ values. Taking a little leap from the LTP for events (@eq-002-bayes-rule-2), this means that\n\n$$f (y = 1) = \\sum_{\\pi \\in \\{0.2,0.5,0.8\\}}{} L(\\pi \\mid y = 1)f (\\pi)$$\n\nor, expanding the summation $\\sum$ and plugging in the prior probabilities and likelihoods from @tbl-002-prior-model-chess and @tbl-002-binomial-likelhood-chess:\n\n$$f (y = 1)  = L(\\pi = 0.2 \\mid y = 1)f (\\pi = 0.2) + $$ $$L(\\pi = 0.5 \\mid y = 1)f (\\pi = 0.5) + $$ {#eq-002-total-probability-chess} $$L(\\pi = 0.8 \\mid y = 1)f (\\pi = 0.8) $$\\\n$$\\approx 0.3932 \\cdot 0.10 + 0.0938 \\cdot 0.25 + 0.0015 \\cdot 0.65 $$ $$\\approx 0.0637$$ {#eq-002-normalizing-constant-chess}\n\nThus, across all possible $\\pi$, there’s only a roughly 6% chance that Kasparov would have won only one game.\n\n### Posterior Probability Model {#sec-002-posterior-model-chess-game}\n\n@fig-002-posterior-probability-model-chess summarizes what we know thus far and where we have yet to go:\n\n1.  Heading into their 1997 re-match, our **prior** model suggested that Kasparov’s win probability against Deep Blue was high (left plot).\n2.  But! Then he only won one of six games, a result that is most **likely** when Kasparov’s win probability is low (middle plot).\n3.  Our updated, **posterior** model (right plot) of Kasparov’s win probability will balance this prior and likelihood.\n\nSpecifically, our formal calculations below will verify that Kasparov’s chances of beating Deep Blue most likely dipped to $\\pi = 0.20$ between 1996 to 1997. It’s also relatively possible that his 1997 losing streak was a fluke, and that he’s more evenly matched with Deep Blue ($\\pi = 0.50$). In contrast, it’s *highly* unlikely that Kasparov is still the superior player ($\\pi = 0.80$).\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-fig-002-posterior-probability-model-chess-figure}\n: Posterior Probability Model: Chess Game Kasparov-Deep Blue (Figure)\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-002-posterior-probability-model-chess-figure .r .cell-code  lst-cap=\"The prior (left), likelihood (middle), and posterior (right) models of $\\pi$.\"}\n# Create data with pi (Kasparov's winning probability per game)\nchess_data <- tibble::tibble(\n  pi = c(0.2, 0.5, 0.8),\n  prior = c(0.10, 0.25, 0.65),\n  likelihood = c(0.393216, 0.09375, 0.001536)\n)\n\n# Calculate posterior probabilities\nnormalizing_constant <- 0.0637\n\nchess_posterior <- chess_data |>\n  dplyr::mutate(\n    # Joint probability = Prior × Likelihood\n    joint = prior * likelihood,\n    # Posterior = Joint / Normalizing Constant\n    posterior = joint / normalizing_constant\n  )\n\n\n# Create long format data for all three models\nchess_long <- chess_posterior |>\n  dplyr::select(pi, prior, likelihood, posterior) |>\n  tidyr::pivot_longer(cols = c(prior, likelihood, posterior), \n               names_to = \"model\", \n               values_to = \"probability\") |>\n  dplyr::mutate(model = factor(model, levels = c(\"prior\", \"likelihood\", \"posterior\")))\n\n# Create three separate lollipop graphs side by side with same y-scale\nggplot2::ggplot(chess_long, ggplot2::aes(x = factor(pi), y = probability, color = model)) +\n  ggplot2::geom_point(size = 4, shape = 21, fill = \"white\") +\n  ggplot2::geom_segment(ggplot2::aes(xend = factor(pi), yend = 0), color = \"black\", alpha = 0.7) +\n  ggplot2::geom_text(ggplot2::aes(label = scales::percent(probability, 1)), \n            vjust = -0.5, color = \"black\", size = 4) +\n  ggplot2::labs(\n    title = \"Bayesian Update: Kasparov's Winning Probability per Game\",\n    x = base::expression(pi ~ \"(Kasparov's probability of winning any game)\"),\n    y = \"Probability\",\n    color = \"Model\"\n  ) +\n  ggplot2::scale_color_manual(\n    values = c(\"prior\" = \"steelblue\", \"likelihood\" = \"darkgreen\", \"posterior\" = \"coral\"),\n    labels = c(\"Prior\", \"Likelihood\", \"Posterior\")\n  ) +\n  ggplot2::scale_y_continuous(\n    breaks = seq(0, 0.7, by = 0.1),\n    labels = scales::percent_format(),\n    limits = c(0, 0.7)\n  ) +\n  ggplot2::facet_wrap(~model, scales = \"free_x\", ncol = 3) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(\n    axis.text.x = ggplot2::element_text(size = 12),\n    axis.text.y = ggplot2::element_text(size = 10),\n    legend.position = \"top\",\n    strip.text = ggplot2::element_text(size = 14),\n    panel.grid.major.y = ggplot2::element_line(color = \"gray60\", linewidth = 0.5),\n    panel.grid.minor.y = ggplot2::element_blank(),\n    panel.spacing = grid::unit(3, \"lines\"),\n    plot.margin = ggplot2::margin(t = 10, r = 10, b = 10, l = 10)\n  )\n```\n\n::: {.cell-output-display}\n![The prior (left), likelihood (middle), and posterior (right) models of $\\pi$.](002-bayes-rule_files/figure-html/fig-002-posterior-probability-model-chess-1.png){#fig-002-posterior-probability-model-chess width=672}\n:::\n:::\n\n:::\n::::::\n\nThe posterior model plotted in @fig-002-posterior-probability-model-chess is specified by the **posterior pmf**\n\n$$f (\\pi \\mid y = 1).$$\n\nConceptually, $f (\\pi \\mid y = 1)$ is the posterior probability of some win probability $\\pi$ given that Kasparov only won one of six games against Deep Blue. Thus, defining the posterior $f (\\pi \\mid y = 1)$ isn’t much different than it was in our previous examples. Just as you might hope, Bayes’ Rule still holds:\n\n$$\\text{posterior} = \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}}$$ In the chess setting, we can translate this as\n\n$$\nf (\\pi \\mid y = 1) = \\frac{f (\\pi)L(\\pi \\mid y = 1)}{f (y = 1)} \\text{ for } \\pi \\in \\{0.2, 0.5, 0.8\\}\n$$ All that remains is a little “plug-and-chug”: the prior f (π) is defined by @tbl-002-prior-model-chess, the likelihood $L(\\pi \\mid y = 1)$ by @tbl-002-binomial-likelhood-chess, and the normalizing constant $f (y = 1)$ by @eq-002-total-probability-chess. The posterior probabilities follow:\n\n$$\n\\begin{align*}\nf (\\pi = 0.2 \\mid y = 1) =  \\frac{0.10 \\cdot 0.3932}{0.0637} \\approx 0.617 \\\\\nf (\\pi = 0.5 \\mid y = 1) =  \\frac{0.25 \\cdot 0.0938}{0.0637} \\approx 0.368 \\\\\nf (\\pi = 0.8 \\mid y = 1) =  \\frac{0.65 \\cdot 0.0015}{0.0637} \\approx 0.015\n\\end{align*}\n$$ {#eq-002-posterior-probability-model-chess}\n\nThis posterior probability model is summarized in @tbl-002-posterior-probability-model-chess along with the prior, likelihood, and joint probability model for comparison. These details confirm the trends in and intuition behind @fig-002-posterior-probability-model-chess. Mainly, though we were fairly confident that Kasparov’s performance would have improved from 1996 to 1997, after winning only one game, the chances of Kasparov being the dominant player ($\\pi = 0.8$) dropped from $0.65$ to $0.015$. In fact, the scenario with the greatest posterior support is that Kasparov is the weaker player, with a win probability of only $0.2$.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-tbl-002-posterior-probability-model-chess-table}\n: Posterior Probability Model: Chess Game Kasparov-Deep Blue (Table)\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-002-posterior-probability-model-chess .cell tbl-cap='Bayesian update showing prior, likelihood, joint and posterior probabilities models of $\\pi$ for Kasparov’s chance of beating Deep Blue in chess.'}\n\n```{.r .cell-code  code-fold=\"true\"}\nchess_posterior |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|  pi| prior| likelihood|  joint| posterior|\n|---:|-----:|----------:|------:|---------:|\n| 0.2|  0.10|     0.3932| 0.0393|    0.6173|\n| 0.5|  0.25|     0.0938| 0.0234|    0.3679|\n| 0.8|  0.65|     0.0015| 0.0010|    0.0157|\n\n\n:::\n:::\n\n:::\n::::::\n\nAbove table used interim result `chess_posterior` from @lst-002-posterior-probability-model-chess-figure.\n\nWe close this section by generalizing the tools we built for the chess analysis.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-bayes-rule-for-variables}\n: Bayes’ rule for variables\n:::\n::::\n\n::: my-theorem-container\nFor any variables $\\pi$ and $Y$ , let $f (\\pi)$ denote the prior pmf of$\\pi$ and \\$L(\\pi \\mid y) denote the likelihood function of \\pi given observed data $Y = y$. Then the posterior pmf of $\\pi$ given data $Y = y$ is\n\n$$\nf (\\pi \\mid y = 1) = \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} = \\frac{f (\\pi)L(\\pi \\mid y)}{f (y)}\n$$ {#eq-002-posterior-pmf-for-variable}\n\nwhere, by the Law of Total Probability, the overall probability of observing data $Y = y$ across all possible $\\pi$ is\n\n$$\nf (y) = \\sum_{all \\pi} f (\\pi)L(\\pi \\mid y).\n$$ {#eq-002-total-probability-for-variable}\n:::\n::::::\n\n### Posterior Shortcut {#sec-002-posterior-shortcut}\n\nWe don't need to calculate the **normalizing constant**. To begin, notice in @eq-002-posterior-probability-model-chess that $f (y = 1) = 0.0637$ appears in the denominator of $f (\\pi \\mid y = 1) \\text{ for each } \\pi \\in \\{0.2, 0.5, 0.8\\}$. This explains the term <a class='glossary' title='In Bayesian statistics, the normalizing constant ensures that the posterior distribution integrates to 1, making it a valid probability distribution. It is also known as the marginal likelihood or evidence, and appears in the denominator of Bayes’ theorem. The integral for the normalizing constant is often intractable for complex models because it involves high-dimensional parameter spaces and there exists no closed-form solution for most realistic models. As a result, approximation methods like Markov Chain Monte Carlo (MCMC), bridge sampling, or variational inference are used to estimate or bypass the normalizing constant. (Brave-AI)'>normalizing constant</a> – its only purpose is to normalize the posterior probabilities so that they sum to one:\n\n$$\nf (\\pi = 0.2 \\mid y = 1) + f (\\pi = 0.5 \\mid y = 1) + f (\\pi = 0.8 \\mid y = 1) = 1\n$$\n\nYet we needn’t actually calculate $f (y = 1)$ to normalize the posterior probabilities. Instead, we can simply note that $f (y = 1)$ is some constant $1/c$, and thus replace @eq-002-posterior-probability-model-chess with\n\n$$\n\\begin{align*}\nf (\\pi = 0.2 \\mid y = 1) = c \\cdot 0.10 \\cdot 0.3932 \\propto 0.039320 \\\\ \nf (\\pi = 0.5 \\mid y = 1) = c \\cdot 0.25 \\cdot 0.0938 \\propto 0.023450 \\\\\nf (\\pi = 0.8 \\mid y = 1) = c \\cdot 0.65 \\cdot 0.0015 \\propto 0.000975\n\\end{align*}\n$$\n\nwhere $\\propto$ denotes “proportional to.” Though these unnormalized posterior probabilities don’t add up to one,\n\n$$0.039320 + 0.023450 + 0.000975 = 0.063745,$$ \n\n@fig-002-demonstrate-proportional-relationship demonstrates that they preserve the proportional relationships of the normalized posterior probabilities.\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-002-demonstrate-proportional-relationship}\n: Compare normalized and unnormalized posterior probabilities\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{#lst-002-demonstrate-proportional-relationship .r .cell-code  lst-cap=\"Demonstration of the same porportional relationship between normalized and unnormalized posterior pmf.\"}\nlollipop_chart <- function(data, y_lab) {\n  ggplot2::ggplot(data, ggplot2::aes(x = x, y = y)) +\n  ggplot2::geom_segment(ggplot2::aes(x = x, xend = x, y = 0, yend = y), \n               color = \"steelblue\", linewidth = 1) +\n  ggplot2::geom_point(color = \"steelblue\", size = 4) +\n  ggplot2::theme_minimal() +\n  ggplot2::labs(x = base::expression(pi), \n       y = y_lab)\n}\n\n# Create the data frame\ndata1 <- data.frame(\n  x = c(0.2, 0.5, 0.8),\n  y = c(0.617, 0.368, 0.015)\n)\n\ndata2 <- data.frame(\n  x = c(0.2, 0.5, 0.8),\n  y = c(0.039320, 0.023450, 0.000975)\n)\n\np1 <- lollipop_chart(data1, \"f(π | y == 1)\")\np2 <- lollipop_chart(data2, paste(\"unnormalized\", \"f(π | y == 1)\"))\n\npatchwork:::\"-.ggplot\"(p1, p2)\n```\n\n::: {.cell-output-display}\n![The normalized posterior pmf of$\\pi$π (left) and the unnormalized posterior pmf of $\\pi$ (right) with different y-axis scales.](002-bayes-rule_files/figure-html/fig-002-demonstrate-proportional-relationship-1.png){#fig-002-demonstrate-proportional-relationship width=480}\n:::\n:::\n\n\n::::\n:::::\n\nThus, to *normalize* these unnormalized probabilities while preserving their relative relationships, we can compare each to the whole. Specifically, we can divide each unnormalized probability by their sum. For example:\n\n$$f (π = 0.2∣y = 1) = \\frac{0.039320}{0.039320 + 0.023450 + 0.000975} \\approx 0.617.$$\n\nThough we’ve just intuited this result, it also follows mathematically by combining @eq-002-posterior-pmf-for-variable and @eq-002-total-probability-for-variable:\n\n$$f (\\pi \\mid y) =  \\frac{f \\pi)L(\\pi \\mid y)}{f (y)} = \\frac{f (\\pi)L(\\pi \\mid y)}{\\sum_{all} \\pi f (\\pi)L(\\pi \\mid y)}.$$\n\nWe state the general form of this proportionality result below.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-proportionality}\n: Proportionality\n:::\n::::\n\n::: my-theorem-container\nSince $f (y)$ is merely a normalizing constant which does not depend on$\\pi$, the posterior pmf $f (\\pi \\mid y)$ is proportional to the product of $f ($\\pi\\$) and $L(\\pi \\mid y)$:\n\n$$\n\\begin{align*}\nf (\\pi \\mid y) =  \\frac{f \\pi)L(\\pi \\mid y)}{f (y)} \\propto f (\\pi)L(\\pi \\mid y)\n\\end{align*}\n$$ {#eq-002-proportionality}\n\nThat is,\n\n$$\\text{posterior} \\propto \\text{prior} \\cdot \\text{likelihood}$$.\n\nThe significance of this proportionality is that all the information we need to build the posterior model is held in the prior and likelihood.\n:::\n::::::\n\n### Posterior Simulation {#sec-002-posterior-simulation-chess-game}\n\nWe’ll conclude this section with a simulation that provides insight into and supports our Bayesian analysis of Kasparov’s chess skills. Ultimately, we’ll simulate 10,000 scenarios of the six-game chess series. To begin, set up the possible values of win probability $\\pi$ and the corresponding prior model $f (\\pi)$:\n\n1.  Define possible win probabilities\n2.  Define the prior model\n3.  Simulate 10,000 possible outcomes of $\\pi$ from the prior model and store the results in the `chess_sim` data frame.\n4.  From each of the 10,000 prior plausible values `pi`, we can simulate six games and record Kasparov’s number of wins, `y`. Since the dependence of `y` on `pi` follows a Binomial model, we can directly simulate `y` using the `stats::rbinom()` function with `size = 6` and `prob = pi`.\n5.  Show 10 randoms outcomes\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-simulate-posterior-chess}\n: Posterior Simulation of Chess Game Kasparov Against Deep Blue\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-002-simulate-posterior-chess .cell tbl-cap='10 random rows of simulated 10000 chess outcomes'}\n\n```{#lst-002-simulate-posterior-chess .r .cell-code  lst-cap=\"Simulate 10000 chess outcomes\"}\n# 1. Define possible win probabilities\nchess <- tibble::tibble(pi = c(0.2, 0.5, 0.8))\n\n# 2. Define the prior model\nprior <- c(0.10, 0.25, 0.65)\n\n# 3. Simulate 10000 values of pi from the prior\nbase::set.seed(84735)\nchess_sim <- dplyr::slice_sample(\n  chess, n = 10000, \n  weight_by = prior, \n  replace = TRUE)\n\n# 4. Simulate 10000 match outcomes\nchess_sim <- chess_sim |>  \n  dplyr::mutate(y = stats::rbinom(10000, size = 6, prob = pi))\n\n# 5. Check it out\nmy_glance_data(chess_sim) |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|   obs|  pi|  y|\n|-----:|---:|--:|\n|     1| 0.5|  3|\n|   356| 0.5|  1|\n|  1252| 0.8|  6|\n|  2369| 0.5|  2|\n|  3954| 0.8|  5|\n|  5273| 0.8|  5|\n|  7700| 0.5|  5|\n|  8826| 0.8|  5|\n|  9290| 0.5|  3|\n| 10000| 0.8|  4|\n\n\n:::\n:::\n\n:::\n::::::\n\nThe combined 10,000 simulated `pi` values closely approximate the prior model $f (\\pi)$ (@tbl-002-prior-model-chess):\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-combined-simulated-pi-values}\n: Combined simulates `pi` values\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-002-combined-simulated-pi-values .cell tbl-cap='Combined simulated `pi` values'}\n\n```{#lst-002-combined-simulated-pi-values .r .cell-code  lst-cap=\"Combined simulated `pi` values\"}\n# Summarize the prior\nchess_sim |> \n  janitor::tabyl(pi) |> \n  janitor::adorn_totals(\"row\") |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|pi    |     n| percent|\n|:-----|-----:|-------:|\n|0.2   |  1017|  0.1017|\n|0.5   |  2521|  0.2521|\n|0.8   |  6462|  0.6462|\n|Total | 10000|  1.0000|\n\n\n:::\n:::\n\n:::\n::::::\n\nFurther, the 10,000 simulated match outcomes `y` illuminate the dependence of these outcomes on Kasparov’s win probability `pi`, closely mimicking the <a class='glossary' title='The Conditional Probability Mass Function (PMF) is the probability distribution of a discrete random variable X given that another discrete random variable Y has taken a specific value y. It is defined using the joint probability mass function and the marginal probability mass function.'>conditional pmfs</a> $f (y \\mid \\pi)$ from @fig-002-binomial-model-chess.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-mimicking-conditional-pmfs}\n: Mimicking conditional PMFs\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-002-mimicking-conditional-pmfs .r .cell-code  lst-cap=\"A bar plot of simulated win outcomes y under each possible win probability $\\pi$\"}\n# Plot y by pi\nggplot2::ggplot(chess_sim, ggplot2::aes(x = y)) + \n  ggplot2::stat_count(ggplot2::aes(y = ggplot2::after_stat(prop))) + \n  ggplot2::facet_wrap(~ pi)\n```\n\n::: {.cell-output-display}\n![A bar plot of simulated win outcomes y under each possible win probability $\\pi$](002-bayes-rule_files/figure-html/fig-002-mimicking-conditional-pmfs-1.png){#fig-002-mimicking-conditional-pmfs width=672}\n:::\n:::\n\n:::\n::::::\n\nFinally, let’s focus on the simulated outcomes that match the observed data that Kasparov won one game. Among these simulations, the majority (60.4%) correspond to the scenario in which Kasparov’s win probability $\\pi$ was 0.2 and very few (1.8%) correspond to the scenario in which $\\pi$ was 0.8. These observations very closely approximate the posterior model of $\\pi$ which we formally built above (@tbl-002-posterior-probability-model-chess).\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-summarize-posterior-approximation}\n: Summarizing the Posterior Approximation\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-summarize-posterior-approximation .cell tbl-cap='Summarizing the posterior appropximation'}\n\n```{#lst-summarize-posterior-approximation .r .cell-code  lst-cap=\"Summarizing the posterior appropximation\"}\n# Focus on simulations with y = 1\nwin_one <- chess_sim |> \n  dplyr::filter(y == 1)\n\n# Summarize the posterior approximation\nwin_one  |>  \n  janitor::tabyl(pi) |>  \n  janitor::adorn_totals(\"row\") |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|pi    |   n|   percent|\n|:-----|---:|---------:|\n|0.2   | 404| 0.6038864|\n|0.5   | 253| 0.3781764|\n|0.8   |  12| 0.0179372|\n|Total | 669| 1.0000000|\n\n\n:::\n:::\n\n:::\n::::::\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-plot-posterior-approximation}\n: Plot of the posterior approximation\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-plot-posterior-approximation .r .cell-code  lst-cap=\"A bar plot of 10,000 simulated $\\pi$ values which approximates the posterior model.\"}\n# Plot the posterior approximation\nggplot2::ggplot(win_one, ggplot2::aes(x = pi)) + \n  ggplot2::geom_bar()\n```\n\n::: {.cell-output-display}\n![A bar plot of 10,000 simulated $\\pi$ values which approximates the posterior model.](002-bayes-rule_files/figure-html/fig-plot-posterior-approximation-1.png){#fig-plot-posterior-approximation width=288}\n:::\n:::\n\n:::\n::::::\n\n## Summary\n\n### Common Steps of a Bayesian Analysis\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-002-bayesian-analysis-steps}\n: Four common steps of a Bayesian analysis\n::::::\n:::\n::::{.my-procedure-container}\n\n Every Bayesian analysis consists of four common steps:  \n \n 1. **Prior model**: Construct a <a class='glossary' title='In Bayesian statistics, a prior model refers to the specification of a prior distribution, which represents the initial beliefs or assumptions about the parameters of a statistical model before observing any data. This prior distribution quantifies the uncertainty about the parameters.'>prior model</a> for your variable of interest, $\\pi$. The prior model specifies two important pieces of information: \n    (a) the possible values of $\\pi$ and \n    (b) the relative prior plausibility of each.  \n2. **Data model**: Summarize the dependence of data $Y$ on $\\pi$ via a <a class='glossary' title='The Conditional Probability Mass Function (PMF) is the probability distribution of a discrete random variable X given that another discrete random variable Y has taken a specific value y. It is defined using the joint probability mass function and the marginal probability mass function.'>conditional pmf</a> $f (y \\mid \\pi)$.  \n3. **Likelihood function**: Upon observing data $Y = y$, define the <a class='glossary' title='The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI)'>likelihood function</a> $L(\\pi \\mid y) = f (y \\mid \\pi)$ which encodes the relative likelihood of observing data $Y = y$ under different $\\pi$ values.  \n4. **Posterior model**: Build the <a class='glossary' title='The posterior model represents the updated beliefs about the parameters of a model after incorporating observed data. It is derived by combining the prior distribution, which reflects initial beliefs about the parameter, with the likelihood of the observed data using Bayes’ theorem.'>posterior model</a> of $\\pi$ via Bayes’ Rule which balances the prior and likelihood.\n\n::::\n:::::\n\nSteps 2 and 3 have interim steps under different names:\n\nThe conditional pmf is defined using the <a class='glossary' title='Joint probability is a statistical measure that calculates the likelihood of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as P(A∩B), which is read as “the probability of A and B”. The joint probability of two independent events A and B is computed as the product of their individual probabilities: P(A∩B)=P(A)×P(B). This formula applies when the occurrence of one event does not influence the other. For dependent events, the joint probability is calculated using conditional probability: P(A∩B)=P(A)×P(B∣A) where P(B∣A) is the probability of event B occurring given that event A has already occurred. (Brave-AI)'>joint probability</a> mass function and the <a class='glossary' title='Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table.'>marginal probability</a> also called the <a class='glossary' title='Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes’ Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI)'>total probability</a>.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-002-bayes-theorem-summary}\n: Posterior model as an application of Bayes’ Rule balances prior and likelihood\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\n\\text{posterior} =  \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\propto \\text{prior} \\cdot \\text{likelihood}\n$$ {#eq-002-bayes-rule-summary-in-words}\n\nMore technically,\n\n$$\nf (\\pi \\mid y) =  \\frac{f (\\pi)L(\\pi \\mid y)}{f (y)} \\propto f (\\pi)L(\\pi \\mid y)\n$$ {#eq-002-bayes-rule-summaryas-formula}\n\n\n\n::::\n:::::\n\n### Links to the different steps\n\nThe chapter introduces Bayesian analysis with three different examples:\n\n1. Building a Bayesian model for events (yes or no?)\n    - Categorical boolean status of an article: fake or real (fake: yes or no)\n2. Building a Bayesian model for events (which region?)\n    - Categorical outcome of an interviewee’s region: Midwest, Northeast, South, or West.\n3. Building a Bayesian model for unknown random changes (level of chess skills?)\n    - Unknown (random) chances of winning a particular game in a six game competition: Simplified to three discrete numerical variables of 0.2, 0.5 and 0.8 representing the overall chess skills.\n    \n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-002-analysis-step-table}\n: Overview of the different analysis steps for each example\n::::::\n:::\n::::{.my-procedure-container}\n\n\n\n\n| Step of Analysis     | Interim step           | fake news                                  | pop vs. soda                                 | chess game                                    |\n|----------------------|------------------------|--------------------------------------------|----------------------------------------------|-----------------------------------------------|\n| prior model          |                        | @sec-002-prior-probability-fake-news       | @sec-002-prior-model-pop-vs-soda             | @sec-002-prior-probability-chess-game         |\n| conditional pmf      |                        | @sec-002-conditional-probability-fake-news | @sec-002-conditional-probability-pop-vs-soda | @sec-002-conditional-probabilities-chess-game |\n| likelihood           |                        | @sec-002-likelihood-fake-news              | @sec-002-likelihood-pop-vs-soda              | @sec-002-likelihood-chess-game                |\n|                      | joint probabilities    | @sec-002-joint-probabilities-fake-news     |                                              |                                               |\n|                      | normalizing constant   | @sec-002-normalizing-constant-fake-news    |                                              | @sec-002-normalizing-constant-chess-game      |\n|                      | marginal probabilities |                                            | @sec-002-marginal-probabilities-pop-vs-soda  |                                               |\n| posterior model      |                        | @sec-002-posterior-probability-fake-news   | @sec-002-posterior-model-pop-vs-soda         | @sec-002-posterior-model-chess-game           |\n| posterior simulation |                        | @sec-002-posterior-simulation-fake-news    |                                              | @sec-002-posterior-simulation-chess-game      |\n: Links to the sections of the different analysis steps for each example {#tbl-002-analysis-step}\n\n::::\n:::::\n\n\n\n## Glossary Entries {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Bayes’ Theorem </td>\n   <td style=\"text-align:left;\"> This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes' Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Binomial Model </td>\n   <td style=\"text-align:left;\"> Binomial model is a foundational framework in Bayesian statistics used to estimate the probability of success in a series of independent trials. It combines a binomial likelihood — which models the number of successes in a fixed number of trials—with a beta prior on the success probability, resulting in a beta posterior. This conjugate relationship allows for analytical computation and intuitive interpretation. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Conditional_Probability </td>\n   <td style=\"text-align:left;\"> Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for \"conditional on\" or \"given that\". </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Conditional-Probability-Function </td>\n   <td style=\"text-align:left;\"> The conditional probability function describes the probability of an event occurring given that another event has already occurred. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> CPMF </td>\n   <td style=\"text-align:left;\"> The Conditional Probability Mass Function (PMF) is the probability distribution of a discrete random variable X given that another discrete random variable Y has taken a specific value y. It is defined using the joint probability mass function and the marginal probability mass function. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Joint-Probability </td>\n   <td style=\"text-align:left;\"> Joint probability is a statistical measure that calculates the likelihood of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as P(A∩B), which is read as \"the probability of A and B\". The joint probability of two independent events A and B is computed as the product of their individual probabilities: P(A∩B)=P(A)×P(B). This formula applies when the occurrence of one event does not influence the other. For dependent events, the joint probability is calculated using conditional probability: P(A∩B)=P(A)×P(B∣A) where P(B∣A) is the probability of event B occurring given that event A has already occurred. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Likelihood_x </td>\n   <td style=\"text-align:left;\"> The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E &amp;#124; H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Likelihood-Function </td>\n   <td style=\"text-align:left;\"> The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> LTP </td>\n   <td style=\"text-align:left;\"> The law of total probability (LTP) is a fundamental rule in probability theory that relates marginal probabilities to conditional probabilities. It expresses the total probability of an event that can occur through several distinct, mutually exclusive, and collectively exhaustive scenarios. It is particularly useful when direct computation of an event's probability is difficult, and it enables breaking down complex probability problems into simpler, manageable components. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Marginal_Probability </td>\n   <td style=\"text-align:left;\"> Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model_x </td>\n   <td style=\"text-align:left;\"> A statistical model (both in frequentist and Bayesian statistics) is a mathematical representation that specifies how observed data are generated, often involving assumptions about the underlying processes and parameters. Both approaches use models to represent data-generating processes, but they differ fundamentally in how they treat uncertainty and parameters: frequentist models treat parameters as fixed and focus on the sampling distribution of statistics, while Bayesian models treat parameters as random variables and update beliefs based on data and prior knowledge. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Normalizing Constant </td>\n   <td style=\"text-align:left;\"> In Bayesian statistics, the normalizing constant ensures that the posterior distribution integrates to 1, making it a valid probability distribution. It is also known as the marginal likelihood or evidence, and appears in the denominator of Bayes' theorem. The integral for the normalizing constant is often intractable for complex models because it involves high-dimensional parameter spaces and there exists no closed-form solution for most realistic models. As a result, approximation methods like Markov Chain Monte Carlo (MCMC), bridge sampling, or variational inference are used to estimate or bypass the normalizing constant. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Posterior_Model </td>\n   <td style=\"text-align:left;\"> The posterior model represents the updated beliefs about the parameters of a model after incorporating observed data. It is derived by combining the prior distribution, which reflects initial beliefs about the parameter, with the likelihood of the observed data using Bayes' theorem. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Posterior_Probability </td>\n   <td style=\"text-align:left;\"> Posterior Probability, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes' theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data.  It's essentially learning from experience - your understanding becomes more refined as you incorporate new information. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Prior_Model </td>\n   <td style=\"text-align:left;\"> In Bayesian statistics, a prior model refers to the specification of a prior distribution, which represents the initial beliefs or assumptions about the parameters of a statistical model before observing any data. This prior distribution quantifies the uncertainty about the parameters. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Prior_Probability </td>\n   <td style=\"text-align:left;\"> The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. It quantifies how likely our initial belief is: P(belief). (BF, Chap.8) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Probability_Model </td>\n   <td style=\"text-align:left;\"> A probability model is a mathematical representation that describes the likelihood of various outcomes in a random experiment. It consists of a sample space, which includes all possible outcomes, and a probability assignment that assigns a probability to each outcome. The probabilities assigned in a probability model must always sum to 1, reflecting the certainty that one of the outcomes will occur. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Probability-Mass-Function </td>\n   <td style=\"text-align:left;\"> A probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value. The pmf is a mathematical function that gives the probability that a discrete random variable is exactly equal to some specific value. It is the primary way to define the probability distribution of a discrete random variable. (Brave-AI) Sometimes it is also known as probability function, frequency function or discrete probability density function. (&lt;a href=\"https://en.wikipedia.org/wiki/Probability_mass_function\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Simulation_x </td>\n   <td style=\"text-align:left;\"> Simulation is a way to model random events, such that simulated outcomes closely match real-world outcomes. By observing simulated outcomes, researchers gain insight on the real world. (&lt;a href=\"https://stattrek.com/experiments/simulation#\"&gt;Stat Trek&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Total_Probability </td>\n   <td style=\"text-align:left;\"> Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes' Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Unconditional_Probabilities </td>\n   <td style=\"text-align:left;\"> Unconditional probability, also known as marginal probability, refers to the likelihood of an event occurring without consideration of any other preceding or future events. It is a stand-alone probability that is not dependent on the occurrence of another event and is denoted as P(A). </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Session Info {.unnumbered}\n\n::::: my-r-code\n::: my-r-code-header\nSession Info\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.5.2 (2025-10-31)\n#>  os       macOS Tahoe 26.2\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2026-02-15\n#>  pandoc   3.9 @ /opt/homebrew/bin/ (via rmarkdown)\n#>  quarto   1.8.27 @ /usr/local/bin/quarto\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  ! package        * version  date (UTC) lib source\n#>  P abind            1.4-8    2024-09-12 [?] RSPM\n#>  P backports        1.5.0    2024-05-23 [?] RSPM\n#>    base64enc        0.1-6    2026-02-02 [1] CRAN (R 4.5.2)\n#>  P bayesplot        1.15.0   2025-12-12 [?] RSPM\n#>    bayesrules     * 0.0.3    2026-01-20 [1] RSPM\n#>  P boot             1.3-32   2025-08-29 [?] CRAN (R 4.5.2)\n#>    checkmate        2.3.4    2026-02-03 [1] RSPM\n#>  P class            7.3-23   2025-01-01 [?] CRAN (R 4.5.2)\n#>  P cli              3.6.5    2025-04-23 [?] CRAN (R 4.5.0)\n#>  P codetools        0.2-20   2024-03-31 [?] CRAN (R 4.5.2)\n#>  P colourpicker     1.3.0    2023-08-21 [?] RSPM\n#>  P commonmark       2.0.0    2025-07-07 [?] RSPM\n#>  P crosstalk        1.2.2    2025-08-26 [?] RSPM\n#>  P curl             7.0.0    2025-08-19 [?] RSPM\n#>  P DiagrammeR       1.0.11   2024-02-02 [?] RSPM\n#>  P digest           0.6.39   2025-11-19 [?] CRAN (R 4.5.2)\n#>    distributional   0.6.0    2026-01-14 [1] RSPM\n#>  P dplyr            1.2.0    2026-02-03 [?] RSPM\n#>  P DT               0.34.0   2025-09-02 [?] RSPM\n#>  P dygraphs         1.1.1.6  2018-07-11 [?] RSPM\n#>    e1071            1.7-17   2025-12-18 [1] RSPM\n#>  P evaluate         1.0.5    2025-08-27 [?] CRAN (R 4.5.1)\n#>  P farver           2.1.2    2024-05-13 [?] RSPM\n#>  P fastmap          1.2.0    2024-05-15 [?] CRAN (R 4.5.0)\n#>  P generics         0.1.4    2025-05-09 [?] RSPM\n#>  P ggplot2          4.0.2    2026-02-03 [?] RSPM\n#>  P glossary       * 1.0.0    2023-05-30 [?] RSPM\n#>  P glue             1.8.0    2024-09-30 [?] CRAN (R 4.5.0)\n#>  P gridExtra        2.3      2017-09-09 [?] RSPM\n#>  P groupdata2       2.0.5    2024-12-18 [?] RSPM\n#>  P gtable           0.3.6    2024-10-25 [?] RSPM\n#>  P gtools           3.9.5    2023-11-20 [?] RSPM\n#>  P htmltools        0.5.9    2025-12-04 [?] CRAN (R 4.5.2)\n#>  P htmlwidgets      1.6.4    2023-12-06 [?] RSPM\n#>  P httpuv           1.6.16   2025-04-16 [?] RSPM\n#>  P igraph           2.2.1    2025-10-27 [?] RSPM\n#>  P inline           0.3.21   2025-01-09 [?] RSPM\n#>  P janitor          2.2.1    2024-12-22 [?] RSPM\n#>  P jsonlite         2.0.0    2025-03-27 [?] CRAN (R 4.5.0)\n#>  P kableExtra       1.4.0    2024-01-24 [?] RSPM\n#>  P knitr            1.51     2025-12-20 [?] CRAN (R 4.5.2)\n#>  P labeling         0.4.3    2023-08-29 [?] RSPM\n#>    later            1.4.5    2026-01-08 [1] RSPM\n#>    lattice          0.22-9   2026-02-09 [1] CRAN (R 4.5.2)\n#>  P lifecycle        1.0.5    2026-01-08 [?] RSPM\n#>  P litedown         0.9      2025-12-18 [?] RSPM\n#>    lme4             1.1-38   2025-12-02 [1] RSPM\n#>  P loo              2.9.0    2025-12-23 [?] RSPM\n#>    lubridate        1.9.5    2026-02-04 [1] RSPM\n#>    magrittr         2.0.4    2025-09-12 [1] RSPM\n#>  P markdown         2.0      2025-03-23 [?] RSPM\n#>  P MASS             7.3-65   2025-02-28 [?] CRAN (R 4.5.2)\n#>  P Matrix           1.7-4    2025-08-28 [?] CRAN (R 4.5.2)\n#>  P matrixStats      1.5.0    2025-01-07 [?] RSPM\n#>  P mime             0.13     2025-03-17 [?] CRAN (R 4.5.0)\n#>  P miniUI           0.1.2    2025-04-17 [?] RSPM\n#>  P minqa            1.2.8    2024-08-17 [?] RSPM\n#>  P nlme             3.1-168  2025-03-31 [?] CRAN (R 4.5.2)\n#>  P nloptr           2.2.1    2025-03-17 [?] RSPM\n#>  P otel             0.2.0    2025-08-29 [?] RSPM\n#>  P patchwork        1.3.2    2025-08-25 [?] RSPM\n#>  P pillar           1.11.1   2025-09-17 [?] RSPM\n#>  P pkgbuild         1.4.8    2025-05-26 [?] RSPM\n#>  P pkgconfig        2.0.3    2019-09-22 [?] RSPM\n#>    plyr             1.8.9    2023-10-02 [1] RSPM\n#>  P posterior        1.6.1    2025-02-27 [?] RSPM\n#>  P promises         1.5.0    2025-11-01 [?] RSPM\n#>    proxy            0.4-29   2025-12-29 [1] CRAN (R 4.5.2)\n#>    purrr            1.2.1    2026-01-09 [1] CRAN (R 4.5.2)\n#>    QuickJSR         1.9.0    2026-01-25 [1] RSPM\n#>  P R6               2.6.1    2025-02-15 [?] CRAN (R 4.5.0)\n#>    rbibutils        2.4.1    2026-01-21 [1] CRAN (R 4.5.2)\n#>  P RColorBrewer     1.1-3    2022-04-03 [?] RSPM\n#>    Rcpp             1.1.1    2026-01-10 [1] RSPM\n#>  P RcppParallel     5.1.11-1 2025-08-27 [?] RSPM\n#>    Rdpack           2.6.6    2026-02-08 [1] CRAN (R 4.5.2)\n#>    reformulas       0.4.4    2026-02-02 [1] RSPM\n#>    renv             1.1.7    2026-01-27 [1] RSPM\n#>  P repr             1.1.7    2024-03-22 [?] RSPM\n#>  P reshape2         1.4.5    2025-11-12 [?] RSPM\n#>  P rlang            1.1.7    2025-12-20 [?] Github (tidyverse/rlang@7a519a2)\n#>  P rmarkdown        2.30     2025-09-28 [?] CRAN (R 4.5.0)\n#>  P rstan            2.32.7   2025-03-10 [?] RSPM\n#>  P rstanarm         2.32.2   2025-09-30 [?] RSPM\n#>    rstantools       2.6.0    2026-01-10 [1] RSPM\n#>    rstudioapi       0.18.0   2026-01-16 [1] RSPM\n#>    S7               0.2.1    2025-11-14 [1] CRAN (R 4.5.2)\n#>  P scales           1.4.0    2025-04-24 [?] RSPM\n#>  P sessioninfo      1.2.3    2025-02-05 [?] RSPM\n#>  P shiny            1.12.1   2025-12-09 [?] RSPM\n#>    shinyjs          2.1.1    2026-01-15 [1] RSPM\n#>  P shinystan        2.7.0    2025-12-12 [?] RSPM\n#>  P shinythemes      1.2.0    2021-01-25 [?] RSPM\n#>    skimr            2.2.2    2026-01-10 [1] RSPM\n#>  P snakecase        0.11.1   2023-08-27 [?] RSPM\n#>  P StanHeaders      2.32.10  2024-07-15 [?] RSPM\n#>  P stringi          1.8.7    2025-03-27 [?] RSPM\n#>  P stringr          1.6.0    2025-11-04 [?] RSPM\n#>    survival         3.8-6    2026-01-16 [1] CRAN (R 4.5.2)\n#>  P svglite          2.2.2    2025-10-21 [?] RSPM\n#>  P systemfonts      1.3.1    2025-10-01 [?] RSPM\n#>  P tensorA          0.36.2.1 2023-12-13 [?] RSPM\n#>  P textshaping      1.0.4    2025-10-10 [?] RSPM\n#>  P threejs          0.3.4    2025-04-21 [?] RSPM\n#>    tibble           3.3.1    2026-01-11 [1] RSPM\n#>    tidyr            1.3.2    2025-12-19 [1] RSPM\n#>  P tidyselect       1.2.1    2024-03-11 [?] RSPM\n#>    timechange       0.4.0    2026-01-29 [1] RSPM\n#>  P V8               8.0.1    2025-10-10 [?] RSPM\n#>    vctrs            0.7.1    2026-01-23 [1] RSPM\n#>    viridisLite      0.4.3    2026-02-04 [1] RSPM\n#>  P visNetwork       2.1.4    2025-09-04 [?] RSPM\n#>  P webexercises   * 1.1.0    2023-05-15 [?] RSPM\n#>  P withr            3.0.2    2024-10-28 [?] RSPM\n#>    xfun             0.56     2026-01-18 [1] RSPM\n#>    xml2             1.5.2    2026-01-17 [1] RSPM\n#>  P xtable           1.8-4    2019-04-21 [?] RSPM\n#>    xts              0.14.1   2024-10-15 [1] RSPM\n#>  P yaml             2.3.12   2025-12-10 [?] CRAN (R 4.5.2)\n#>    zoo              1.8-15   2025-12-15 [1] RSPM\n#> \n#>  [1] /Users/petzi/Documents/Meine-Repos/bayes-rules/renv/library/macos/R-4.5/aarch64-apple-darwin20\n#>  [2] /Users/petzi/Library/Caches/org.R-project.R/R/renv/sandbox/macos/R-4.5/aarch64-apple-darwin20/4cd76b74\n#> \n#>  * ── Packages attached to the search path.\n#>  P ── Loaded and on-disk path mismatch.\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n:::\n:::::\n",
    "supporting": [
      "002-bayes-rule_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.9/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/viz-1.8.2/viz.js\"></script>\n<link href=\"site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/grViz-binding-1.0.11/grViz.js\"></script>\n<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}