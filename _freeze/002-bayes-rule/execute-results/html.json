{
  "hash": "4dad7a6939ec4690c6882d0ff1fb1e70",
  "result": {
    "engine": "knitr",
    "markdown": "# Bayes’ Rule {#sec-chap-002}\n\n\n\n## Introduction {.unnumbered}\n\n:::::: {#obj-chap-002}\n::::: my-objectives\n::: my-objectives-header\nObjectives\n:::\n\n::: my-objectives-container\n-   Explore foundational probability tools such as marginal, conditional, and joint probability models and the Binomial model.\n-   Conduct your first formal Bayesian analysis! You will construct your first prior and data models and, from these, construct your first posterior models via Bayes’ Rule.\n-   Practice your Bayesian grammar. You’ll practice the formal notation and terminology central to Bayesian grammar.\n-   Simulate Bayesian models. Simulation is integral to building intuition for and supporting Bayesian analyses. You’ll conduct your first simulation, using the R statistical software.\n:::\n:::::\n::::::\n\nWe start this chapter with the examination of a sample of 150 articles which were posted on Facebook and fact checked by five [BuzzFeed](https://www.buzzfeed.com/) journalists [@shu-2017]. Information about each article is stored in the `fake_news` dataset in the {**bayesrules**} package. To learn more about this dataset, type `?fake_news` in your console.\n\n::: {#tip-002-skim-data .callout-tip}\n##### Explore and Summarize data\n\nTo learn more about this dataset, type `?fake_news` in your console.\n\nIn addition to typing `?fake_news` in your console, I recommend to use the `skim()` function from {**skimr**} package to get a summary of the dataset.\n:::\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-load-fake-data}\n: Load relevant packages and the `fake_news` dataset\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-load-fake-data}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(bayesrules)\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(janitor))\n\n# Import article data\ndata(fake_news)\n\n# Skim data\nskimr::skim(fake_news)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |          |\n|:------------------------|:---------|\n|Name                     |fake_news |\n|Number of rows           |150       |\n|Number of columns        |30        |\n|_______________________  |          |\n|Column type frequency:   |          |\n|character                |4         |\n|factor                   |1         |\n|logical                  |1         |\n|numeric                  |24        |\n|________________________ |          |\n|Group variables          |None      |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min|   max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|-----:|-----:|--------:|----------:|\n|title         |         0|          1.00|  29|   136|     0|      149|          0|\n|text          |         0|          1.00|  31| 31860|     0|      149|          0|\n|url           |         5|          0.97|  21|   169|     0|      145|          0|\n|authors       |        27|          0.82|   6|   135|     0|       84|          0|\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts       |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------|\n|type          |         0|             1|FALSE   |        2|rea: 90, fak: 60 |\n\n\n**Variable type: logical**\n\n|skim_variable  | n_missing| complete_rate| mean|count             |\n|:--------------|---------:|-------------:|----:|:-----------------|\n|title_has_excl |         0|             1| 0.12|FAL: 132, TRU: 18 |\n\n\n**Variable type: numeric**\n\n|skim_variable           | n_missing| complete_rate|    mean|      sd|    p0|     p25|     p50|     p75|     p100|hist  |\n|:-----------------------|---------:|-------------:|-------:|-------:|-----:|-------:|-------:|-------:|--------:|:-----|\n|title_words             |         0|             1|   11.18|    3.54|  5.00|    9.00|   11.00|   14.00|    22.00|▆▇▆▂▁ |\n|text_words              |         0|             1|  533.35|  596.44|  5.00|  253.00|  389.00|  534.00|  5392.00|▇▁▁▁▁ |\n|title_char              |         0|             1|   67.95|   20.54| 29.00|   54.00|   65.50|   82.75|   136.00|▅▇▆▂▁ |\n|text_char               |         0|             1| 3282.56| 3604.07| 31.00| 1527.50| 2385.00| 3339.25| 31860.00|▇▁▁▁▁ |\n|title_caps              |         0|             1|    0.67|    1.07|  0.00|    0.00|    0.00|    1.00|     7.00|▇▂▁▁▁ |\n|text_caps               |         0|             1|    4.77|    6.75|  0.00|    1.00|    3.00|    5.75|    56.00|▇▁▁▁▁ |\n|title_caps_percent      |         0|             1|    5.37|    8.34|  0.00|    0.00|    0.00|   10.00|    43.75|▇▂▁▁▁ |\n|text_caps_percent       |         0|             1|    1.10|    1.32|  0.00|    0.29|    0.68|    1.52|    10.00|▇▁▁▁▁ |\n|title_excl              |         0|             1|    0.15|    0.42|  0.00|    0.00|    0.00|    0.00|     2.00|▇▁▁▁▁ |\n|text_excl               |         0|             1|    0.36|    0.88|  0.00|    0.00|    0.00|    0.00|     8.00|▇▁▁▁▁ |\n|title_excl_percent      |         0|             1|    0.20|    0.61|  0.00|    0.00|    0.00|    0.00|     3.77|▇▁▁▁▁ |\n|text_excl_percent       |         0|             1|    0.02|    0.07|  0.00|    0.00|    0.00|    0.00|     0.61|▇▁▁▁▁ |\n|anger                   |         0|             1|    1.61|    0.92|  0.00|    0.95|    1.52|    2.04|     4.66|▅▇▆▂▁ |\n|anticipation            |         0|             1|    1.62|    0.71|  0.00|    1.23|    1.56|    2.03|     4.10|▂▇▇▂▁ |\n|disgust                 |         0|             1|    0.97|    0.58|  0.00|    0.54|    0.88|    1.39|     2.54|▅▇▆▃▁ |\n|fear                    |         0|             1|    1.94|    1.21|  0.00|    0.95|    1.66|    2.70|     5.13|▇▇▇▃▂ |\n|joy                     |         0|             1|    1.07|    0.61|  0.00|    0.61|    1.06|    1.52|     3.12|▆▇▆▂▁ |\n|sadness                 |         0|             1|    1.36|    0.80|  0.00|    0.82|    1.29|    1.85|     4.66|▆▇▃▁▁ |\n|surprise                |         0|             1|    0.94|    0.51|  0.00|    0.61|    0.95|    1.16|     2.33|▃▇▇▂▁ |\n|trust                   |         0|             1|    3.08|    1.75|  0.54|    2.19|    2.89|    3.63|    20.00|▇▁▁▁▁ |\n|negative                |         0|             1|    3.13|    1.36|  0.00|    2.21|    3.09|    4.04|     8.47|▃▇▆▁▁ |\n|positive                |         0|             1|    4.06|    1.23|  0.00|    3.35|    4.03|    4.55|     9.22|▁▆▇▂▁ |\n|text_syllables          |         0|             1|  912.55| 1006.86| 10.00|  409.50|  648.00|  945.25|  8875.00|▇▁▁▁▁ |\n|text_syllables_per_word |         0|             1|    1.72|    0.11|  1.48|    1.63|    1.71|    1.77|     2.12|▂▇▅▂▁ |\n\n\n:::\n:::\n\n\nLoad relevant packages and the `fake_news` dataset\n:::\n::::\n:::::::\n\nThe help file on `fake_news` describes the format: \"A data frame with 150 rows and 6 variables\". But as you can see from @cnj-002-load-fake-data it has 30 columns. I do not know where the dataset comes from, because the cited reference originated from an [article on BuzzFeed](https://www.buzzfeed.com/craigsilverman/partisan-fb-pages-analysis) uses a [different dataset](https://github.com/BuzzFeedNews/2016-10-facebook-fact-check/blob/master/data/facebook-fact-check.csv) with 12 columns.\n\n:::::: my-remark\n:::: my-remark-header\n::: {#rem-002-using-notes}\n: Added personal notes\n:::\n::::\n\n::: my-remark-container\nTo understand better the different concepts and their relation to each other, I have added personal notes. These notes reflect my understanding at the time of this writing. It could therefore be the case that they contain some errors.\n:::\n::::::\n\nWe can already see from the output of the `skimr::skim()` function that the `type` variable (`real` or `fake`) has the relation 90:60. Using the `tabyl()` function in the {**janitor**} package [@janitor], shows also the percentage (60%:40%).\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-relation-real-to-fake}\n: Relation of `real` to `fake` articles\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-relation-real-to-fake}\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_news |>\n  janitor::tabyl(type) |>\n  janitor::adorn_totals(\"row\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   type   n percent\n#>   fake  60     0.4\n#>   real  90     0.6\n#>  Total 150     1.0\n```\n\n\n:::\n:::\n\n\nRelation of `real` to `fake` articles\n:::\n::::\n:::::::\n\n::: {#nte-002-marginal-probabilities .callout-note}\n###### Unconditional, total or marginal probabilities\n\nTo understand better the following text it is important to note that @cnj-002-relation-real-to-fake produces <a class='glossary' title='Unconditional probability, also known as marginal probability, refers to the likelihood of an event occurring without consideration of any other preceding or future events. It is a stand-alone probability that is not dependent on the occurrence of another event and is denoted as P(A).'>unconditional</a> or <a class='glossary' title='Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes’ Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI)'>total probabilities</a> also called as <a class='glossary' title='Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table.'>marginal probabilities</a>.\n\n\\begin{align*}\nP(\\text{fake}) &= 0.4 \\\\\nP(\\text{real}) &= 0.6 \\\\\nP(\\text{Total}) &= 1.0\n\\end{align*}\n\nThere is more information about these different concepts in the following sections. But it helped my understanding to note these different names for @lst-002-relation-real-to-fake.\n:::\n\nIf we take the result of @cnj-002-relation-real-to-fake, we could say: \"Since most articles are real, we should read and believe all articles\". If we follow this rule we wouldn't miss no real article, but at the cost reading many fake articles. 4 out of 10 articles would be fake news.\n\nWe need to create a better filter that is not only supported by the overall relation but also by some features of the articles it selves. One of these features would be an exclamation sign for the headlines. Exclamation points in headlines are often perceived as shouting or juvenile, which can make the content seem less credible.\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-exclamation-usage}\n: Tabulate article type and exclamation usage = Conditional Probability\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-exclamation-usage}\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_news |>\n  janitor::tabyl(type, title_has_excl) |>\n  janitor::adorn_totals(where = c(\"row\", \"col\")) |>\n  janitor::adorn_percentages() |>\n  janitor::adorn_pct_formatting(digits = 3) |>\n  janitor::adorn_ns(position = \"front\") |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|type  |FALSE         |TRUE         |Total          |\n|:-----|:-------------|:------------|:--------------|\n|fake  |44 (73.333%)  |16 (26.667%) |60 (100.000%)  |\n|real  |88 (97.778%)  |2  (2.222%)  |90 (100.000%)  |\n|Total |132 (88.000%) |18 (12.000%) |150 (100.000%) |\n\n\n:::\n:::\n\n\nExclamation usage\n:::\n::::\n:::::::\n\n::: {#nte-002-conditional-probabilities .callout-note}\n###### Conditional probabilities\n\n@lst-002-exclamation-usage shows <a class='glossary' title='Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for “conditional on” or “given that”.'>Conditional probabilities</a> assessing how exclamation point usage depends on the article type.\n\n\\begin{align*}\nP(\\text{excl} \\mid \\text{fake}) &= 16/60 &= 26.67\\%  \\\\\nP(\\text{excl} \\mid \\text{real}) &=  2/90 &= 2.22\\% \\\\\nP(\\text{excl} \\mid \\text{total}) &=  18/150 &= 12.00\\%  \n\\end{align*}\n\n$P(\\text{excl} \\mid \\text{total})$ is the normalizing constant! See @sec-002-normalizing-constant.\n\n------------------------------------------------------------------------\n\nHow to pronounce the above equations? I take as an example the first line:\n\n-   The probability of an exclamation sign in the headline of a fake article is 26.67%, or more formal:\n-   The probability to see an exclamation sign in the headline given the article is of type `fake` is 26.67%.\n\nImportant here is that we know the type of article. But normally we are interesting in the reverse operation. If we see an exclamations sign in the headline, e.g., we know there is an exclamation sign in the headline, what is the probability that the article type is `fake`? See for more detail and a general treatment @sec-002-conditional-probability-and-likelihood.\n:::\n\nThe usage of an exclamation point might seem like an odd choice for a real news article. The data backs up this instinct – in our article collection, 26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point.\n\nOr formulated differently: Only 18 articles has an exclamation sign in the title. But almost all of them (with only two exceptions) are used in the titles of fake news articles.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-diagram-fake-news}\n: Bayesian knowledge-building diagram for whether or not the article is fake.\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n#| label: diagram-fake-news\n\nDiagrammeR::grViz(\"\ndigraph real_or_fake{\n\n# node statement\nnode [shape = oval]\na [label = 'Prior\\n40% of articles are fake'];\nb [label = 'Data\\n! more common among fake news'];\nc [label = 'Posterior\\nIs the article fake or not?'];\n\n# edge statement\na -> c b -> c\n}\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-31d61e63924a9068893d\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-31d61e63924a9068893d\">{\"x\":{\"diagram\":\"\\ndigraph real_or_fake{\\n\\n# node statement\\nnode [shape = oval]\\na [label = \\\"Prior\\n40% of articles are fake\\\"];\\nb [label = \\\"Data\\n! more common among fake news\\\"];\\nc [label = \\\"Posterior\\nIs the article fake or not?\\\"];\\n\\n# edge statement\\na -> c b -> c\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n:::\n::::::\n\n@cnj-002-diagram-fake-news shows the Bayesian thinking process.\n\n1.  We start with a **prior undertanding**: From all the articles only 40% are `fake`. From this vantage point it would be feasible to judge a random article as `real`. After all the probability that it is an article of type `real` is higher than that it would be `fake`.\n2.  But looking at exclamation signs in the title give us another more detailed perspective. The new **data changes our hypothesis**.\n3.  We have now another, a **posterior understanding**: Depending if the article has an exclamation point in the title or not, we can build a more probable hypotheses. Instead of 6:4 or 1.5 we have now a relation of 26.67:2.22 or 12. Based on the appearances of exclamations sign in the title our hypothesis is now 8 times more probable.\n\n## Building a Bayesian Model for Events\n\n### Prior Probability Model\n\nAs a first step in our Bayesian analysis, we’ll formalize our prior understanding of whether the new article is fake.\n\nBefore even reading the new article, there’s a 0.4 <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>prior probability</a> that it’s fake and a 0.6 prior probability it’s *not*.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-prior-probability-model}\n: Prior probability model\n:::\n::::\n\n::: my-theorem-container\n(1) We can represent this information using mathematical notation. Letting $B$ denote the event that an article is `fake` and $B^c$ (read \"B complement\" or \"B not\" denote the event that it’s not `fake`, e.g., that it is `real.`\n(2) Additionally I have added the specific formula for the example at hand.\n\n$$\n\\begin{align*}\n&P(B) &= 0.40 \\text{ and } &P(B^c) &= 0.60 \\text{ (1)} \\\\\n&P(\\text{fake}) &= 0.40 \\text{ and } &P(\\text{real}) &= 0.60 \\text{ (2)}\n\\end{align*} \n$$ {#eq-002-prior-probability-model}\n:::\n::::::\n\nAs a collection, @eq-002-prior-probability-model $P(B)$ and $P(B^c)$ specify the simple <a class='glossary' title='In Bayesian statistics, a prior model refers to the specification of a prior distribution, which represents the initial beliefs or assumptions about the parameters of a statistical model before observing any data. This prior distribution quantifies the uncertainty about the parameters.'>prior model</a> of fake news summarized in @tbl-002-prior-probability-model.\n\nThere are three requirements of a valid probability model:\n\n(1) it accounts for all possible events (all articles must be fake or real);\n(2) it assigns (prior) probabilities to each event; and\n(3) these probabilities sum to one.\n\n| event       | $\\mathbf{B}$ | $\\mathbf{B^c}$ | Total |\n|:------------|--------------|----------------|-------|\n| probability | 0.4          | 0.6            | 1     |\n\n: Prior model of fake news {#tbl-002-prior-probability-model}\n\n::: {#nte-002-total-probabilities .callout-note}\n###### Prior Probability Model\n\n@tbl-002-prior-probability-model shows again the unconditional, marginal or total probabilities as I have already shown with @cnj-002-relation-real-to-fake with the result of @lst-002-relation-real-to-fake. See also my @nte-002-marginal-probabilities.\n\n\\begin{align*}\nP(\\text{fake}) &= 0.4 \\\\\nP(\\text{real}) &= 0.6 \\\\\nP(\\text{Total}) &= 1.0\n\\end{align*}\n:::\n\n### Conditional Probability and Likelihood {#sec-002-conditional-probability-and-likelihood}\n\nIn the second step of our Bayesian analysis, we’ll summarize the insights from the data we collected on the new article. Specifically, we’ll formalize our observation that the exclamation point data is more compatible with fake news than with real news.\n\n#### Conditional Probability\n\nConditional probabilities are fundamental to Bayesian analyses, and thus a quick pause to absorb this concept is worth it. In general, comparing the conditional vs unconditional probabilities, $P (A \\mid B) \\text{ vs } P (A)$, reveals the extent to which information about $B$ informs our understanding of $A$.\n\n-   In some cases, the certainty of an event $A$ might increase in light of new data $B$.\n-   In other cases, the certainty of an event might decrease in light of new data.\n\nRecall that *if* an article is fake, *then* there’s a roughly 26.67% chance it uses exclamation points in the title. In contrast, *if* an article is real, *then* there’s only a roughly 2.22% chance it uses exclamation points. When stated this way, it’s clear that the occurrence of exclamation points depends upon, or is *conditioned* upon, whether the article is fake.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-conditional-probability}\n: Conditional Probability\n:::\n::::\n\n::: my-theorem-container\nThis dependence is specified by the following conditional probabilities of exclamation point usage in the title ($A$) *given* an article’s fake status ($B$) or ($B^c$):\n\n$$\n\\begin{align*}\nP(A \\mid B) &= 0.2667 \\text{ and } P(A \\mid B^c) &= 0.0222 \\text{ (1)} \\\\\nP(\\text{excl} \\mid \\text{fake}) &= 0.2667 \\text{ and } P(\\text{excl} \\mid \\text{real}) &= 0.0222 \\text{ (2)} \\\\\n\\\\\nP(A^c \\mid B) &= 0.7333 \\text{ and } P(A^c \\mid B^c) &= 0.9778 \\text{ (1)} \\\\\nP(\\text{!excl} \\mid \\text{fake}) &= 0.7333 \\text{ and } P(\\text{!excl} \\mid \\text{real}) &= 0.9778 \\text{ (2)}\n\\end{align*}\n$$ {#eq-conditional-probability-1}\n:::\n::::::\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-conditional-probability}\n: Conditional Probability\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\ncond_prob <- fake_news |>\n  janitor::tabyl(type, title_has_excl) |>\n  janitor::adorn_totals(where = c(\"row\", \"col\")) |>\n  janitor::adorn_percentages() |>\n  janitor::adorn_pct_formatting(digits = 3) \n\ncond_prob |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|type  |FALSE   |TRUE    |Total    |\n|:-----|:-------|:-------|:--------|\n|fake  |73.333% |26.667% |100.000% |\n|real  |97.778% |2.222%  |100.000% |\n|Total |88.000% |12.000% |100.000% |\n\n\n:::\n:::\n\n:::\n::::::\n\nLet $A$ and $B$ be two events.\n\n-   The <a class='glossary' title='Unconditional probability, also known as marginal probability, refers to the likelihood of an event occurring without consideration of any other preceding or future events. It is a stand-alone probability that is not dependent on the occurrence of another event and is denoted as P(A).'>unconditional probability</a> of $A$, $P (A)$, measures the probability of observing $A$, without any knowledge of $B$.\n-   In contrast, the <a class='glossary' title='Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for “conditional on” or “given that”.'>conditional probability</a> of $A$ given $B$, $P (A \\mid B)$, measures the probability of observing $A$ in light of the information that $B$ occurred.\n\nIn general, comparing the conditional vs unconditional probabilities, $P (A \\mid B)$ vs $P (A)$, reveals the extent to which information about $B$ informs our understanding of $A$.\n\nThe *order* of conditioning is also important. Since they measure two different phenomena, it’s typically the case that $P (A \\mid B) ≠ P (B \\mid A)$. For instance, roughly 100% of puppies are adorable. Thus, if the next object you pass on the street is a puppy, $P (adorable \\mid puppy) = 1$. However, the reverse is not true. Not every adorable object is a puppy, thus $P (puppy \\mid adorable) < 1$.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-independent-events}\n: Independent Events\n:::\n::::\n\n::: my-theorem-container\nInformation about $B$ doesn’t always change our understanding of $A$. For example, suppose your friend has a yellow pair of shoes and a blue pair of shoes, thus four shoes in total. They choose a shoe at random and don’t show it to you.\n\n-   Without actually seeing the shoe, there’s a 0.5 probability that it goes on the right foot: $P (\\text{right foot}) = 2/4$.\n-   And even if they tell you that they happened to get one of the two yellow shoes, there’s still a 0.5 probability that it goes on the right foot: $P (\\text{right foot} \\mid \\text{yellow}) = 1/2$.\n\nThat is, information about the shoe’s color tells us nothing about which foot it fits – shoe color and foot are independent.\n\n$$\n\\begin{align*}\nP (A \\mid B) = P (A) \\text{ (1)} \\\\\nP (\\text{right foot} \\mid \\text{yellow}) &= 1/2 \\\\= P (\\text{right foot}) = 2/4 &= 1/2 \\text{ (2)}\n\\end{align*}\n$$ {#eq-002-independent-events}\n:::\n::::::\n\n#### Likelihood\n\nLet’s reexamine our fake news example with these conditional concepts in place. The conditional probabilities we derived above, $P (A \\mid B) = 0.2667$ and $P (A \\mid B^c) = 0.0222$, indicate that a whopping 26.67% of fake articles versus a mere 2.22% of real articles use exclamation points. Since exclamation point usage is so much more **likely** among fake news than real news, this data provides some evidence that the article is fake.\n\nWith the above observation we’ve evaluated the exclamation point data by flipping the conditional probabilities $P (A \\mid B)$ and $P (A \\mid B^c)$ on their heads. **Flipping the conditional probabilities results in the likelihood function!** See @thm-002-probability-vs-likelihood.\n\nFor example, on its face, the conditional probability $P (A \\mid B)$ measures the uncertainty in event $A$ given we know event $B$ occurs. However, we find ourselves in the opposite situation. We *know* that the incoming article used exclamation points, $A$. What we *don’t* know is whether or not the article is fake, $B$ or $B^c$. Thus, in this case, we compared $P (A \\mid B)$ and $P (A \\mid B^c)$ to ascertain the relative <a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihoods</a> of observing data $A$ under different scenarios of the *uncertain* article status.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-probability-vs-likelihood}\n: Probability function versus likelihood function\n:::\n::::\n\n::: my-theorem-container\nTo help distinguish this application of conditional probability calculations from that when $A$ is uncertain and $B$ is known, we’ll utilize the following <a class='glossary' title='The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI)'>likelihood function</a> notation $L(\\cdot \\mid A)$:\n\n$$\n\\begin{align*}\nL(B \\mid A) = P (A \\mid B) \\text{ and } L(B^c \\mid A) = P (A \\mid B^c) \\text{ (1)} \\\\\nL(\\text{fake} \\mid \\text{excl}) = P(\\text{excl} \\mid \\text{fake}) \\text{ and } L(\\text{real} \\mid \\text{excl}) = P(\\text{excl} \\mid \\text{real}) \\text{ (2)}\n\\end{align*}\n$$ {#eq-002-probability-vs-likelihood}\n\n------------------------------------------------------------------------\n\n**Conditional Probability Function: Compare probabilities of an unknown event**\n\nWhen $B$ is known, the <a class='glossary' title='The conditional probability function describes the probability of an event occurring given that another event has already occurred.'>conditional probability function</a> $P (⋅∣B)$ allows us to compare the probabilities of an unknown event, $A$ or $A^c$, occurring with $B$:\n\n$$\n\\begin{align*}\nP (A \\mid B) &\\text{ vs } P (A^c \\mid B) \\text{ (1)} \\\\\nP (\\text{excl} \\mid \\text{fake}) &\\text{ vs } P (\\text{!excl} \\mid \\text{fake}) \\text{ (2)}\n\\end{align*}\n$$ {#eq-002-compare-probability-of-unknown-events}\n\n**Likelihood Function: Evaluate the relative compatibility of data with events**\n\nWhen A is known, the <a class='glossary' title='The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI)'>likelihood function</a> $L(⋅∣A) = P (A∣⋅)$ allows us to evaluate the relative compatibility of data $A$ with events $B$ or $B^c$:\n\n$$\n\\begin{align*}\nL(B \\mid A) &\\text{ vs } L(B^c \\mid A) \\text{ (1)} \\\\\nL(\\text{fake} \\mid \\text{excl}) &\\text{ vs } L(\\text{real} \\mid \\text{excl})\n\\end{align*}\n$$ {#eq-002-evaluate-relative-compatibility-of-data-with-events}\n:::\n::::::\n\n@tbl-002-prior-model-and-likelihood summarizes the information that we’ve amassed thus far, including the prior probabilities and likelihoods associated with the new article being fake or real, $B$ or $B^c$. Notice that the prior probabilities add up to 1 but the likelihoods do not. **The likelihood function is not a probability function**, but rather provides a framework to compare the relative compatibility of our exclamation point data with $B$ and $B^c$. Thus, whereas the prior evidence suggested the article is most likely real ($P (B) < P (B^c)$), the data is more consistent with the article being fake ($L(B∣A) > L(B^c∣A)$).\n\n| event       | $\\mathbf{B}$ | $\\mathbf{B^c}$ | Total  |\n|:------------|:-------------|:---------------|:-------|\n| probability | 0.4          | 0.6            | 1      |\n| likelihood  | 0.2667       | 0.0222         | 0.2889 |\n\n: Prior probabilities and likelihoods of fake news. {#tbl-002-prior-model-and-likelihood}\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-prior-probabilities-and-likelihood}\n: Prior probabilities and likelihoods of fake news\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-prior-probabilities-and-likelihood}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prior probability\nrow_prior <- fake_news |>  \n  count(type) |>  \n  mutate(prop = n / sum(n)) |>  \n  select(-n) |>  \n  pivot_wider(names_from = type, values_from = prop)\n\n# Likelihood\nrow_likelihood <- fake_news |>  \n  count(type, title_has_excl) |>  \n  pivot_wider(names_from = title_has_excl, values_from = n) |>  \n  mutate(likelihood = `TRUE` / (`TRUE` + `FALSE`)) |>  \n  select(-c(`FALSE`, `TRUE`)) |> \n  pivot_wider(names_from = type, values_from = likelihood)\n\n# build table\nbind_cols(Statistic = c(\"Prior probability\", \"Likelihood\"),\n          bind_rows(row_prior, row_likelihood)) |>  \n  mutate(Total = fake + real) |>  \n  rename(`Fake ($\\\\mathbf{B}$)` = fake, \n         `Real ($\\\\mathbf{B^c}$)` = real) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|Statistic         | Fake ($\\mathbf{B}$)| Real ($\\mathbf{B^c}$)|  Total|\n|:-----------------|-------------------:|---------------------:|------:|\n|Prior probability |              0.4000|                0.6000| 1.0000|\n|Likelihood        |              0.2667|                0.0222| 0.2889|\n\n\n:::\n:::\n\n\nPrior probabilities and likelihoods of fake news (taken from [bayesf22 class](https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/02-chapter.html#likelihood))\n:::\n::::\n:::::::\n\n::: {#nte-002-likelihood .callout-note}\n###### Prior probabilities and likelihoods\n\nThe prior probability was already calculated in @lst-002-relation-real-to-fake. Here I have repeated the calculation with column and row reversed.\n\nThe interesting and new part here is the **<a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a>**.\n\n\\begin{align*}\nP(A \\mid B) &= 0.2667 \\\\\nP(A \\mid B^c) &= 0.0222 \\\\\n\\end{align*}\n\n26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point.\n\n**Likelihood compares how well data supports hypotheses**. Here, we calculate the likelihood of observing an exclamation point under each type:\n\n\\begin{align*}\nL(\\text{fake} \\mid \\text{excl}) &= P(\\text{excl} \\mid \\text{fake}) &= 16 / 60 &= 0.2667 \\\\\nL(\\text{real} \\mid \\text{excl}) &= P(\\text{excl} \\mid \\text{real}) &= 2 / 90 &= 0.0222 \\\\\nP(\\text{excl} \\mid \\text{fake}) &+ P(\\text{excl} \\mid \\text{real}) &= 0.2667 + 0.0222 &= 0.2889\n\\end{align*}\n\nAgain, **the likelihood function is not a probability function, it does not sum up to 1.00**. It provides a framework to compare the relative compatibility of our exclamation point data with $B$ and $B^c$.\n\nCompare the manual created @tbl-002-prior-model-and-likelihood with my @lst-002-exclamation-usage, my personal @nte-002-conditional-probabilities and the above @lst-002-prior-probabilities-and-likelihood.\n:::\n\n### Joint Probabilites and Normalizing Constants\n\n#### Joint Probabilites\n\nThe <a class='glossary' title='Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table.'>marginal probability</a> of observing exclamation points across all news articles, $P (A)$, provides an important point of comparison.\n\nWe’ll first use our prior model and likelihood function to fill in the table below. This table summarizes the possible **joint occurrences** of the fake news and exclamation point variables.\n\n|   | $\\mathbf{B} \\text{ (fake)}$ | $\\mathbf{B^c} \\text{ (real)}$ | Total |\n|:-----------------|:-----------------|:-----------------|:-----------------|\n| $A$ (excl) |  |  |  |\n| $A^C$ (!excl) |  |  |  |\n| Total | 0.4 | 0.6 | 1 |\n\nFirst, focus on the $B$ (articles of type `fake` column which splits fake articles into two groups:\n\n(1) those that are fake *and* use exclamation points, denoted $A \\cap B$; and\n(2) those that are fake *and* don’t use exclamation points, denoted $A^c \\cap B$.\n\n::: {#tip-002-defintion-pronounciation-cap-cup .callout-tip}\n###### How to define and pronounce $\\cap$ and $\\cup$ (Brave-AI)\n\n-   The formula $A \\cap B$ is pronounced as \"A intersect B\". The symbol $\\cap$ represents the intersection of two sets, indicating the elements common to both sets $A$ and $B$. It refers to the set of elements shared between two or more sets. The word \"and\" is often used as a synonym for intersection in this context, reflecting the logical condition that an element must belong to both sets simultaneously.\n-   The formula $A \\cup B$ is pronounced as \"A union B\". The symbol $\\cup$ represents the union operation in set theory, which combines all elements from sets $A$ and $B$ into a single set containing all unique elements from both. The term \"union\" is also referred to as the \"logical sum\" or simply \"sum,\" although these terms are considered old-fashioned and are not commonly used today. The symbol $\\cup$ is used to denote the union of two sets, and the operation is sometimes described as combining all elements from either set $A$ or set $B$ (or both).\n:::\n\nTo determine the probabilities of these **joint events**, note that 40% of articles are fake and 26.67% of fake articles use exclamation points, $P (B) = 0.4 \\text{ and } P (A \\mid B) = 0.2667$. It follows that across all articles, 26.67% of 40%, or 10.67%, are fake with exclamation points.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-joint-probability}\n: Joint Probability\n:::\n::::\n\n::: my-theorem-container\n**Co-occurrence of (not) exclamation point and fake article**\n\nThat is, the <a class='glossary' title='Joint probability is a statistical measure that calculates the likelihood of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as P(A∩B), which is read as “the probability of A and B”. The joint probability of two independent events A and B is computed as the product of their individual probabilities: P(A∩B)=P(A)×P(B). This formula applies when the occurrence of one event does not influence the other. For dependent events, the joint probability is calculated using conditional probability: P(A∩B)=P(A)×P(B∣A) where P(B∣A) is the probability of event B occurring given that event A has already occurred. (Brave-AI)'>joint probability</a> of observing both $A$ and $B$ is\n\n$$\n\\begin{align*}\nP (A \\cap B) = P (A \\mid B) \\cdot P (B) &= 0.2667 \\cdot 0.4 = 0.1067 \\text{ (1)} \\\\\nP (\\text{excl} \\cap \\text{fake}) = P (\\text{excl} \\mid\\text{fake}) \\cdot P (\\text{fake}) &= 0.2667 \\cdot 0.4 = 0.1067 \\text{ (2)}\n\\end{align*}\n$$ {#eq-joint-probability-fake}\n\nFurther, since 26.67% of fake articles use exclamation points, 73.33% do not. That is, the <a class='glossary' title='Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for “conditional on” or “given that”.'>conditional probability</a> that an article does not use exclamation points ($A^c$) given it’s fake ($B$) is:\n\n$$\n\\begin{align*}\nP (A^c \\mid B) = 1 − P (A \\mid B) &= 1 − 0.2667 = 0.7333 \\text{ (1)} \\\\\nP (\\text{!excl} \\mid \\text{fake}) = 1 - P (\\text{excl} \\mid \\text{fake}) &= 1 − 0.2667 = 0.7333 \\text{ (2)}\n\\end{align*}\n$$ {#eq-conditional-probabilities-1}\n\nIt follows that 73.33% of 40%, or 29.33%, of all articles are fake without exclamation points:\n\n$$\n\\begin{align*}\nP (A^c \\cap B) = P (A^c \\mid B) \\cdot P (B) &= 0.7333 \\cdot 0.4 = 0.2933 \\text{ (1)} \\\\\nP (\\text{!excl} \\cap \\text{fake}) = P (\\text{!excl} \\mid \\text{fake}) \\cdot P (\\text{fake}) &= 0.7333 \\cdot 0.4 = 0.2933 \\text{ (2)}\n\\end{align*}\n$$ {#eq-conditional-probabilities-2}\n\nIn summary, the <a class='glossary' title='Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes’ Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI)'>total probability</a> of observing a fake article is the sum of its parts:\n\n$$\n\\begin{align*}\nP (B) &= P (A \\cap B) + P (A^c \\cap B) &= 0.1067 + 0.2933 = 0.4 \\text{ (1)} \\\\\nP (\\text{fake}) &= P (\\text{excl} \\cap \\text{fake}) + P (\\text{!excl} \\cap \\text{fake}) &= 0.1067 + 0.2933 = 0.4 \\text{ (2)}\n\\end{align*}\n$$ {#eq-total-probability-fake}\n\n**Co-occurrence of (not) exclamation point and real article**\n\nWe can similarly break down real articles into those that do and those that don’t use exclamation points. Across all articles, only 1.33% (2.22% of 60%) are real and use exclamation points whereas 58.67% (97.78% of 60%) are real without exclamation points:\n\n$$\n\\begin{align*}\nP (A \\cap B^c) &= P (A \\mid B^c) \\cdot P (B^c) &= 0.0222 \\cdot 0.6 &= 0.0133 \\text{ (1a)} \\\\\nP (\\text{excl} \\cap \\text{real}) &= P (\\text{excl} \\mid \\text{real}) \\cdot P (\\text{real}) &= 0.0222 \\cdot 0.6 &= 0.0133 \\text{ (2a)} \\\\\n\\\\\nP (A^c \\cap B^c) &= P (A^c \\mid B^c) \\cdot P (B^c) &= 0.9778 \\cdot 0.6 &= 0.5867 \\text{ (1b)} \\\\\nP (\\text{!excl} \\cap \\text{real}) &= P (\\text{!excl} \\mid \\text{real}) \\cdot P (\\text{real}) &= 0.9778 \\cdot 0.6 &= 0.5867 \\text{ (2b)}\n\\end{align*}\n$$ {#eq-joint-probability-real}\n\nThus, the <a class='glossary' title='Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes’ Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI)'>total probability</a> of observing a real article is again the sum of these two parts:\n\n$$\n\\begin{align*}\nP (B^c) = P (A \\cap B^c) + P (A^c \\cap B^c) &= 0.0133 + 0.5867 = 0.6 \\text{ (1)} \\\\\nP (\\text{real}) = P (\\text{excl} \\cap \\text{real}) + P (\\text{!excl} \\cap \\text{real} &= 0.0133 + 0.5867 = 0.6) \\text{ (2)}\n\\end{align*}\n$$ {#eq-total-probability-real}\n:::\n::::::\n\n:::::::::::::::::: my-code-collection\n::::: my-code-collection-header\n::: my-code-collection-icon\n:::\n\n::: {#exm-002-joint-probabilities}\n: Joint Probabilities\n:::\n:::::\n\n:::::::::::::: my-code-collection-container\n::::::::::::: panel-tabset\n###### Version 1\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-joint-probabilities-1}\n: Joint Probabilities (my table)\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-joint-probabilities-1}\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_news |> \n  janitor::tabyl(title_has_excl, type) |>\n  janitor::adorn_totals(where = c(\"row\", \"col\")) |> \n  janitor::adorn_percentages(\"all\") |> \n  janitor::adorn_pct_formatting(digits = 3) |>\n  janitor::adorn_ns(position = \"front\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  title_has_excl         fake         real          Total\n#>           FALSE 44 (29.333%) 88 (58.667%) 132  (88.000%)\n#>            TRUE 16 (10.667%)  2  (1.333%)  18  (12.000%)\n#>           Total 60 (40.000%) 90 (60.000%) 150 (100.000%)\n```\n\n\n:::\n:::\n\n\nJoint probabilities (my own table)\n:::\n::::\n:::::::\n\n###### Version 2\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-joint-probabilities-2}\n: Joint Probability (Brave-AI & Positron Assistant)\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-joint-probabilities-2}\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_prob <- fake_news  |>  \n  janitor::tabyl(type)  |>  \n  dplyr::mutate(prop = n / sum(n))  |>  \n  dplyr::select(type, prior = prop)\n\nconditional_prob <- fake_news |>  \n  janitor::tabyl(title_has_excl, type) |>  \n  janitor::adorn_percentages(\"col\") |>  \n  dplyr::filter(title_has_excl == TRUE) |>  \n  dplyr::select(-title_has_excl) |>\n  tidyr::pivot_longer(cols = everything(), \n                      names_to = \"type\", \n                      values_to = \"conditional\") |>\n  dplyr::mutate(type = factor(type, levels = c(\"fake\", \"real\")))\n\njoint_prob <- prior_prob |> \n  dplyr::left_join(conditional_prob, by = \"type\") |> \n  dplyr::mutate(joint = prior * conditional)\n\njoint_prob  |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type | prior| conditional|  joint|\n|:----|-----:|-----------:|------:|\n|fake |   0.4|      0.2667| 0.1067|\n|real |   0.6|      0.0222| 0.0133|\n\n\n:::\n:::\n\n\nJoint probabilities (Brave-AI, middle part corrected by Positron Assistant)\n:::\n::::\n:::::::\n:::::::::::::\n::::::::::::::\n::::::::::::::::::\n\n::: {#nte-002-joint-probabilities .callout-note}\n####### Joint probabilities\n\n<a class='glossary' title='Joint probability is a statistical measure that calculates the likelihood of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as P(A∩B), which is read as “the probability of A and B”. The joint probability of two independent events A and B is computed as the product of their individual probabilities: P(A∩B)=P(A)×P(B). This formula applies when the occurrence of one event does not influence the other. For dependent events, the joint probability is calculated using conditional probability: P(A∩B)=P(A)×P(B∣A) where P(B∣A) is the probability of event B occurring given that event A has already occurred. (Brave-AI)'>Joint probabilities</a> measure the co-occurrence of two events. In the case of @exm-002-joint-probabilities it measures the co-occurrence of fake article *and* using an exclamation sign in the title.\n\nThis table is missing in the original book text. I have added it to improve my understanding.\n\n\\begin{align*}\nP(\\text{fake} \\cap \\text{excl}) = 0.40 \\cdot 0.2667 = 0.1067 \\\\\nP(\\text{real} \\cap \\text{excl}) = 0.60 \\cdot 0.0222 = 0.0133\n\\end{align*}\n:::\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-joint-and-conditional-probability}\n: Calculating joint and conditional probabilities\n:::\n::::\n\n::: my-theorem-container\nFor events $A$ and $B$, the joint probability of $A \\cap B$ is calculated by weighting the conditional probability of $A$ given $B$ by the marginal probability of $B$:\n\n$$P (A \\cap B) = P (A \\mid B) \\cdot P (B)$$ {#eq-002-joint-probability}\n\nThus, when $A$ and $B$ are *independent*,\n\n$$P (A \\cap B) = P (A) \\cdot P (B)$$\n\nDividing both sides of @eq-002-joint-probability by $P (B)$, and assuming $P (B) ≠ 0$, reveals the definition of the conditional probability of $A$ given $B$:\n\n$$P (A \\mid B) =  \\frac{P (A \\cap B) }{ P (B)}$$ {#eq-002-conditional-probability}\n\nThus, to evaluate the chance that $A$ occurs in light of information$B$, we can consider the chance that they occur together, $P (A \\cap B)$, relative to the chance that $B$ occurs at all, $P (B)$.\n:::\n::::::\n\n@tbl-002-joint-probability-model summarizes our new understanding of the joint behavior of our two article variables. The fact that the grand total of this table is one confirms that our calculations are reasonable. @tbl-002-joint-probability-model also provides the point of comparison we sought: 12% of *all* news articles use exclamation points, $P (A) = 0.12$.\n\nSo that we needn’t always build similar marginal probabilities from scratch, let’s consider the theory behind this calculation. As usual, we can start by recognizing the two ways that an article can use exclamation points: if it is fake ($A \\cap B$) and if it is not fake ($A \\cap B^c$).\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-law-of-total-probability-ltp}\n: Law of Total Probability (LTP)\n:::\n::::\n\n::: my-theorem-container\nThus, the **total probability** of observing $A$ is the combined probability of these distinct parts:\n\n$$\n\\begin{align*}\nP (A) &= P (A \\cap B) &+ P (A \\cap B^c) \\text{ (1)} \\\\\nP (\\text{excl}) &= P (\\text{excl} \\cap \\text{fake}) &+ P (\\text{excl} \\cap \\text{real}) \\text{ (2)}\n\\end{align*}\n$$ {#eq-chapXY-formula}\n\nBy @eq-002-joint-probability, we can compute the two pieces of this puzzle using the information we have about exclamation point usage among fake and real news, $P (A \\mid B)$ and $P (A \\mid B^c)$, weighted by the prior probabilities of fake and real news, $P (B)$ and $P (B^c)$:\n\n$$\n\\begin{align*}\nP (A) &= P (A \\cap B) + P (A \\cap B^c) \\\\\n&= P (A \\mid B) \\cdot P (B) + P (A \\mid B^c) \\cdot P (B^c) &\\text{ (1)} \\\\\nP (\\text{excl}) &= P (\\text{excl} \\cap \\text{fake}) + P(\\text{excl} \\cap \\text{real}) \\\\\n&= P (\\text{excl} \\mid \\text{fake}) \\cdot P (\\text{fake}) + P (\\text{excl} \\mid \\text{real}) \\cdot P (\\text{real}) &\\text{ (2)} \n\\end{align*}\n$$ {#eq-002-total-probability}\n:::\n::::::\n\n::: {#nte-002-ltp .callout-note}\n###### Law of Total Probability (LTP)\n\nFinally, plugging in, we can confirm that roughly 12% of all articles use exclamation points:\n\n$$\n\\begin{align*}\nP(\\text{excl}) &= (P(\\text{excl} \\mid \\text{fake}) \\cdot P(\\text{fake})) &+ (P(\\text{excl} \\mid {real}) \\cdot P(\\text{real})) &\\text{ (1)} \\\\\nP(\\text{excl}) &= (0.2667 \\cdot 0.4) &+ (0.0222 \\cdot 0.6) = 0.12 &\\text{ (2)}\n\\end{align*} \n$$ {#eq-002-ltp}\n:::\n\nThe formula we’ve built to calculate $P (A)$ here is a special case of the aptly named <a class='glossary' title='The law of total probability (LTP) is a fundamental rule in probability theory that relates marginal probabilities to conditional probabilities. It expresses the total probability of an event that can occur through several distinct, mutually exclusive, and collectively exhaustive scenarios. It is particularly useful when direct computation of an event’s probability is difficult, and it enables breaking down complex probability problems into simpler, manageable components. (Brave-AI)'>Law of Total Probability</a> (LTP).\n\n:::::: my-resource\n:::: my-resource-header\n::: {#lem-002-ltp}\n: Law of Total Probability (LTP)\n:::\n::::\n\n::: my-resource-container\n-   [Law of total probability \\| Wikipedia](https://www.wikiwand.com/en/articles/Law_of_total_probability)\n-   [Law of Total Probability \\| BYJU'S](https://byjus.com/maths/total-probability-theorem/)\n:::\n::::::\n\n|       | $\\mathbf{B}$ | $\\mathbf{B^c}$ | Total |\n|:------|:-------------|:---------------|:------|\n| $A$   | 0.1067       | 0.0133         | 0.12  |\n| $A^C$ | 0.2933       | 0.5867         | 0.88  |\n| Total | 0.4000       | 0.6000         | 1.00  |\n\n: A joint probability model of the fake status and exclamation point usage across all articles. {#tbl-002-joint-probability-model}\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-joint-probability-model}\n: Joint Probability Model\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-joint-probability-model}\n\n::: {.cell}\n\n```{.r .cell-code}\njoint_prob_model <- fake_news |> \n  janitor::tabyl(title_has_excl, type) |>\n  janitor::adorn_totals(where = c(\"col\", \"row\")) |> \n  janitor::adorn_percentages(\"all\") |> \n  dplyr::arrange(fake)\n\njoint_prob_model  |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|title_has_excl |   fake|   real| Total|\n|:--------------|------:|------:|-----:|\n|TRUE           | 0.1067| 0.0133|  0.12|\n|FALSE          | 0.2933| 0.5867|  0.88|\n|Total          | 0.4000| 0.6000|  1.00|\n\n\n:::\n:::\n\n\nJoint probability model of the fake status and exclamation point usage across all articles.\n:::\n::::\n:::::::\n\n#### Normalizing Constant {#sec-002-normalizing-constant}\n\n::: {#nte-002-normalizing-constant .callout-note}\n###### Normalizing constant\n\nThe last piece we need is the marginal probability of observing exclamation points across all articles, or $P(A)$, which is the normalizing constant. For the calculation see the calculated values in @tbl-002-prior-model-and-likelihood and @lst-002-prior-probabilities-and-likelihood.\n\n$$\n\\begin{align*}\nP (B) \\cdot L (B \\mid A) &+ P (B^c) \\cdot L (B^c \\mid A) \\text{ (1)} \\\\\nP (\\text{fake}) \\cdot L (\\text{fake} \\mid \\text{excl}) &+ P (\\text{real}) \\cdot L (\\text{real} \\mid \\text{excl}) \\text{ (2)} \\\\\n0.4 \\cdot 0.2667 &+ 0.6 \\cdot 0.0222 = \\\\\n0.1067 &+ 0.0133 = 0.1199 \\approx 0.12 \\text{ (3)} \\\\\n\\\\\nP(\\text{excl}) = P(\\text{excl} \\mid \\text{fake}) \\cdot P(\\text{fake}) &+ P(\\text{excl} \\mid {real}) \\cdot P(\\text{real}) \\text{ (4)} \\\\\nP(\\text{excl}) = (0.2667 \\cdot 0.4) &+ (0.0222 \\cdot 0.6) = 0.1199 \\approx 0.12 \\text{ (5)}\n\\end{align*} \n$$ {#eq-002-normalizing-constant}\n\nWe filled in (3) and (5) the figures applying the law of total probability (LTP) as outlined in @nte-002-ltp.\n:::\n\n:::::::::::::::: my-code-collection\n::::: my-code-collection-header\n::: my-code-collection-icon\n:::\n\n::: {#exm-002-normalizing-constant}\n: Normalizing constant\n:::\n:::::\n\n:::::::::::: my-code-collection-container\n::::::::::: panel-tabset\n###### bayesf22 Notebook\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-normalizing-constant-1}\n: Normalizing constant ([bayesf22](https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/02-chapter.html#normalizing-constants))\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_news |> \n  count(type, title_has_excl) |> \n  mutate(prop = n / sum(n)) |> \n  filter(title_has_excl == TRUE) |>  \n  summarize(normalizing_constant = sum(prop))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   normalizing_constant\n#> 1                 0.12\n```\n\n\n:::\n\n```{.r .cell-code}\n##   normalizing_constant\n## 1                 0.12\n```\n:::\n\n:::\n::::::\n\n###### Brave-AI\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-normalizing-constant-2}\n: Normalizing constant (Brave-AI)\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\ntotal_prob <- joint_prob %>% \n  summarise(total = sum(joint)) %>% \n  pull(total)\n\ntotal_prob\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.12\n```\n\n\n:::\n\n```{.r .cell-code}\n# Result: 0.1067 + 0.0133 = 0.12   \n```\n:::\n\n:::\n::::::\n:::::::::::\n::::::::::::\n::::::::::::::::\n\n### Posterior probability model\n\n#### Bayes rule!\n\nWe’re now in a position to answer the ultimate question: What’s the probability that the latest article is fake? Formally speaking, we aim to calculate the <a class='glossary' title='Posterior Probability, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes’ theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data.  It’s essentially learning from experience - your understanding becomes more refined as you incorporate new information.'>posterior probability</a> that the article is fake given that it uses exclamation points, $P (B \\mid A)$.\n\nTo build some intuition, let’s revisit @tbl-002-joint-probability-model and @lst-002-joint-probability-model. Since our article uses exclamation points, we can zoom in on the 12% of articles that fall into the $A$ row resp. in the `TRUE` row. Among these articles, proportionally 88.9% (0.1067 / 0.12) are fake and 11.1% (0.0133 / 0.12) are real. **This is the answer we were seeking: there’s an 88.9% posterior chance that this latest article is fake.**\n\n::: {#nte-002-bayes-rule .callout-note}\n###### We built Bayes’ Rule from scratch!\n\nStepping back from the details, we’ve accomplished something big: we built <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ Rule</a> from scratch! In short, Bayes’ Rule provides the mechanism we need to put our Bayesian thinking into practice. It defines a <a class='glossary' title='Posterior Probability, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes’ theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data.  It’s essentially learning from experience - your understanding becomes more refined as you incorporate new information.'>posterior probability model</a> for an event $B$ from two pieces: the <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>prior probability</a> of $B$ and the <a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a> of observing data $A$ if $B$ were to occur.\n:::\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-002-bayes-rule}\n: Bayes’ Rule for Events\n:::\n::::\n\n::: my-theorem-container\nFor events $A$ and $B$, the posterior probability of $B$ given $A$ follows by combining (@eq-002-conditional-probability) with (@eq-002-joint-probability) and recognizing that we can evaluate data $A$ through the likelihood function, $L(B \\mid A) = P (A \\mid B)$ and $L(B^c \\mid A) = P (A \\mid B^c)$:\n\n$$\n\\begin{align*}\nP (B \\mid A) = \\frac{P (A \\cap B)}{P (A)} = \\frac{P (B) \\cdot L (B \\mid A)}{P (A)}\n\\end{align*}\n$$ {#eq-bayes-rule-1}\n\n-   $P (B \\mid A)$ is the posterior probability,\n-   $P (B)$ is the prior,\n-   $L (B \\mid A)$ is the likelihood, and\n-   $P (A)$ is the marginal likelihood or evidence.\n\nwhere by the Law of Total Probability (@eq-002-ltp)\n\n$$P (A) = P (B) \\cdot L (B \\mid A) + P (B^c) \\cdot L (B^c \\mid A)$$ {#eq-bayes-rule-2}\n\nMore generally,\n\n$$\\text{posterior} = \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}}$$ {#eq-bayes-rule-general}\n\nTo convince ourselves that Bayes’ Rule works, let’s directly apply it to our news analysis. Into (@eq-bayes-rule-1), we can plug the prior information that 40% of articles are fake, the 26.67% likelihood that a fake article would use exclamation points, and the 12% marginal probability of observing exclamation points across all articles. The resulting posterior probability that the incoming article is fake is roughly 0.889, just as we calculated from @tbl-002-joint-probability-model resp. @lst-002-joint-probability-model:\n\n$$\n\\begin{align*}\nP (B \\mid A) = \\frac{P (B) \\cdot L (B \\mid A)}{P (A)} = \\frac{0.4 \\cdot 0.2667}{0.12} = \\frac{0.1067}{0.12} = 0.889\n\\end{align*}\n$$ {#eq-bayes-rule-with-data}\n:::\n::::::\n\n#### From Prior to Posterior\n\n@tbl-prior-posterior-model summarizes our news analysis journey, from the prior to the posterior model. We started with a prior understanding that there’s only a 40% chance that the incoming article would be fake. Yet upon observing the use of an exclamation point in the title “The president has a funny secret!”, a feature that’s more common to fake news, our posterior understanding evolved quite a bit – the chance that the article is fake jumped to 88.9%.\n\n| Event                 | $\\mathbf{B}$ | $\\mathbf{B^c}$ | Total |\n|:----------------------|-------------:|---------------:|------:|\n| prior probability     |        0.400 |          0.600 |   1.0 |\n| posterior probability |        0.889 |          0.111 |   1.0 |\n\n: The prior and posterior models of fake news. {#tbl-prior-posterior-model}\n\nThe following calculation of the posterior probability has used Positron Assistant: P(fake \\| title has exclamation) using Bayes' rule.\n\nThe posterior probability using Bayes’ rule is calculated with:\n\n$$P(\\text{fake} \\mid \\text{excl}) = P(\\text{excl} \\mid \\text{fake}) \\cdot P(\\text{fake}) / P(\\text{excl})$$\n\n::::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-prior-and-posterior-probability}\n: Prior and Posterior Probability\n:::\n::::\n\n:::: my-r-code-container\n::: {#lst-002-prior-and-posterior-probability}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate prior probabilities P(type)\nprior_prob <- fake_news |>  \n  janitor::tabyl(type) |>  \n  dplyr::rename(prior = percent) |> \n  dplyr::select(type, prior)\n\n# Calculate conditional probabilities P(excl | type)\nconditional_prob <- fake_news |>  \n  janitor::tabyl(title_has_excl, type) |>  \n  janitor::adorn_percentages(\"col\") |>  \n  dplyr::filter(title_has_excl == TRUE) |>  \n  tidyr::pivot_longer(cols = c(fake, real), \n                      names_to = \"type\", \n                      values_to = \"conditional\")\n\n# Calculate joint probabilities P(excl AND type)\n# prior probability * conditional probability\njoint_prob <- prior_prob |> \n  dplyr::left_join(conditional_prob, by = \"type\") |> \n  dplyr::mutate(joint = prior * conditional)\n\n# Calculate marginal probability P(excl)\nmarginal_prob <- sum(joint_prob$joint)\n\n# Calculate posterior probabilities P(type | excl)\nposterior_prob <- joint_prob |>\n  dplyr::mutate(posterior = joint / marginal_prob) |>\n  dplyr::select(type, posterior)\n\n# Create the final table\nresult_table <- tibble(\n  Event = c(\"Prior Probability\", \"Posterior Probability\"),\n  fake = c(\n    prior_prob |> filter(type == \"fake\") |> pull(prior),\n    posterior_prob |> filter(type == \"fake\") |> pull(posterior)\n  ),\n  real = c(\n    prior_prob |> filter(type == \"real\") |> pull(prior),\n    posterior_prob |> filter(type == \"real\") |> pull(posterior)\n  )\n) |>\n  dplyr::mutate(Total = fake + real)\n\nresult_table |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|Event                 |   fake|   real| Total|\n|:---------------------|------:|------:|-----:|\n|Prior Probability     | 0.4000| 0.6000|     1|\n|Posterior Probability | 0.8889| 0.1111|     1|\n\n\n:::\n:::\n\n\nPrior and Posterior Probability of news articles with an exclamation sign in the title\n:::\n::::\n:::::::\n\n#### Step-by-Step\n\n@lst-002-prior-and-posterior-probability gives a nice summary of all the things we learned in this section. It separates and names the different steps to calculate the posterior probability.\n\n-   **1. Prior Probability**: The <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>Prior Probability</a>, also called the Prior, is the assumed probability distribution before we have seen the data. In the case of our example with the `fake_news` dataset it integrates already some data, namely the proportion of `fake` to `real` articles (04 : 06). The prior quantifies how likely our initial belief is: $P(B) = P(\\text{fake}) = 0.4$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_prob |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type | prior|\n|:----|-----:|\n|fake |   0.4|\n|real |   0.6|\n\n\n:::\n:::\n\n\n-   **2. Conditional probability**: The <a class='glossary' title='Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for “conditional on” or “given that”.'>Conditional Probability</a> is a measure of the likelihood of an event occurring given that another event has already occurred. The mathematical notation uses the pipe symbol for \"conditional on\" or \"given that\". It is denoted as $P(A \\mid B)$, which represents the probability of event $A$ occurring given that event $B$ has already occurred. In our example it is the probability of an exclamation sign in the title given that we are inspecting a fake article: $P(A \\mid B) = P(\\text{excl} \\mid \\text{fake}) = 0.2667$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_prob |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|title_has_excl |type | conditional|\n|:--------------|:----|-----------:|\n|TRUE           |fake |      0.2667|\n|TRUE           |real |      0.0222|\n\n\n:::\n:::\n\n\n-   **3. Joint probability:** <a class='glossary' title='Joint probability is a statistical measure that calculates the likelihood of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as P(A∩B), which is read as “the probability of A and B”. The joint probability of two independent events A and B is computed as the product of their individual probabilities: P(A∩B)=P(A)×P(B). This formula applies when the occurrence of one event does not influence the other. For dependent events, the joint probability is calculated using conditional probability: P(A∩B)=P(A)×P(B∣A) where P(B∣A) is the probability of event B occurring given that event A has already occurred. (Brave-AI)'>Joint Probability</a> is a statistical measure that calculates the <a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a> of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as $P(A \\cap B)$, which is read as \"the probability of A and B\". The joint probability of two independent events $A$ and $B$ is computed as the product of their individual probabilities: $P(A \\cap B) = P(A) \\cdot P(B)$. In our example: $P(\\text{excl} \\cap \\text{fake}) = P(\\text{excl}) \\cdot P(\\text{fake}) = 0.2667 \\cdot 0.4 = 0.1067$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njoint_prob |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type | prior|title_has_excl | conditional|  joint|\n|:----|-----:|:--------------|-----------:|------:|\n|fake |   0.4|TRUE           |      0.2667| 0.1067|\n|real |   0.6|TRUE           |      0.0222| 0.0133|\n\n\n:::\n:::\n\n\n-   **4. Marginal probability**: <a class='glossary' title='Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table.'>Marginal Probability</a> refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as $P(A)$ or $P(B)$, and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table. The summary of the joint probabilities $P(\\text{excl} \\cap \\text{fake}) + P(\\text{excl} \\cap \\text{real}) = 0.1067 + 0.0133 = 0.12$ is the <a class='glossary' title='In Bayesian statistics, the normalizing constant ensures that the posterior distribution integrates to 1, making it a valid probability distribution. It is also known as the marginal likelihood or evidence, and appears in the denominator of Bayes’ theorem. The integral for the normalizing constant is often intractable for complex models because it involves high-dimensional parameter spaces and there exists no closed-form solution for most realistic models. As a result, approximation methods like Markov Chain Monte Carlo (MCMC), bridge sampling, or variational inference are used to estimate or bypass the normalizing constant. (Brave-AI)'>normalizing constant</a>. It ensures that the posterior distribution integrates to 1, making it a valid probability distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njoint_prob |> \n  tibble::as_tibble() |> \n  janitor::adorn_totals(\"row\") |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type  | prior|title_has_excl | conditional|  joint|\n|:-----|-----:|:--------------|-----------:|------:|\n|fake  |   0.4|TRUE           |      0.2667| 0.1067|\n|real  |   0.6|TRUE           |      0.0222| 0.0133|\n|Total |   1.0|-              |      0.2889| 0.1200|\n\n\n:::\n:::\n\n\n-   **5. Posterior Probability**: The <a class='glossary' title='Posterior Probability, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes’ theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data.  It’s essentially learning from experience - your understanding becomes more refined as you incorporate new information.'>Posterior Probability</a>, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes' theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data. It's essentially learning from experience - your understanding becomes more refined as you incorporate new information.\n\nThe @eq-bayes-rule-1 can be expressed for our example as\n\n$$\n\\begin{align*}\nP (\\text{fake} \\mid \\text{excl}) &= \\frac{P (\\text{excl} \\cap \\text{fake})}{P (\\text{excl})} &= \\\\ \nP (\\text{fake} \\mid \\text{excl}) &= \\frac{P (\\text{fake}) \\cdot L (\\text{fake} \\mid \\text{excl})}{P (\\text{excl})} &= \\\\\nP (\\text{fake} \\mid \\text{excl}) &= \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} &= \\\\\nP (\\text{fake} \\mid \\text{excl}) &= \\frac{0.4 \\cdot 0.2667}{0.12} = \\mathbf{0.889} \n\\end{align*}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_prob <- joint_prob |>\n  dplyr::mutate(posterior = joint / marginal_prob) |>\n  dplyr::select(type, posterior) |> \n  tibble::as_tibble() |> \n  janitor::adorn_totals(\"row\") \n  \ncomplete_prob <- joint_prob |>\n  dplyr::left_join(posterior_prob, by = join_by(type)) |> \n  tibble::as_tibble() |> \n  janitor::adorn_totals(\"row\")\n\ncomplete_prob |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|type  | prior|title_has_excl | conditional|  joint| posterior|\n|:-----|-----:|:--------------|-----------:|------:|---------:|\n|fake  |   0.4|TRUE           |      0.2667| 0.1067|    0.8889|\n|real  |   0.6|TRUE           |      0.0222| 0.0133|    0.1111|\n|Total |   1.0|-              |      0.2889| 0.1200|    1.0000|\n\n\n:::\n:::\n\n\n### Posterior Simulation\n\nIt’s important to keep in mind that the <a class='glossary' title='A probability model is a mathematical representation that describes the likelihood of various outcomes in a random experiment. It consists of a sample space, which includes all possible outcomes, and a probability assignment that assigns a probability to each outcome. The probabilities assigned in a probability model must always sum to 1, reflecting the certainty that one of the outcomes will occur. (Brave-AI)'>probability models</a> we built for our news analysis above are just that – <a class='glossary' title='A statistical model (both in frequentist and Bayesian statistics) is a mathematical representation that specifies how observed data are generated, often involving assumptions about the underlying processes and parameters. Both approaches use models to represent data-generating processes, but they differ fundamentally in how they treat uncertainty and parameters: frequentist models treat parameters as fixed and focus on the sampling distribution of statistics, while Bayesian models treat parameters as random variables and update beliefs based on data and prior knowledge. (Brave-AI)'>models</a>. They provide theoretical representations of what we observe in practice. To build intuition for the connection between the articles that might actually be posted to social media and their underlying models, let’s run a <a class='glossary' title='Simulation is a way to model random events, such that simulated outcomes closely match real-world outcomes. By observing simulated outcomes, researchers gain insight on the real world. (Stat Trek)'>simulation</a>.\n\n#### Define article `type` and prior probabilities\n\nDefine the possible article `type`, `real` or `fake`, and their corresponding prior probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define possible articles\narticle <- data.frame(type = c(\"real\", \"fake\"))\n\n# Define the prior model\nprior <- c(0.6, 0.4)\n```\n:::\n\n\n#### Randomly sample rows from the article data frame\n\nThe book recommends the function `dplyr::sample_n()` which is superseded by `dplyr::slice_sample()`. We need three more information to get the desired simulation:\n\n-   We must specify the sample size `n`.\n-   We need to sample with `replacement` ensuring that we start with a fresh set of possibilities for each article – any article can either be fake or real.\n-   Finally we have to specify that there’s a 60% chance an article is real and a 40% chance it’s fake. This is done by the argument `weight_by = prior`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate 5 articles\ndplyr::slice_sample(article, n = 5, weight_by = prior, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   type\n#> 1 real\n#> 2 real\n#> 3 real\n#> 4 real\n#> 5 fake\n```\n\n\n:::\n:::\n\n\nIf you would run the code above several times you would see that the result changes each time. I want to demonstrate this behavior with a more complex code using a loop.\n\n\n::: {.cell}\n\n```{#lst-sample-results-1b .r .cell-code  lst-cap=\"Generate 5 articles in 4 samples using map and slice_sample\"}\n# Generate 5 articles in 4 samples using purrr::map and dplyr::slice_sample\nsample_results_1b <- purrr::map(1:4, ~ fake_news |> \n                        dplyr::slice_sample(n = 5) |> \n                        dplyr::pull(type)) |>\n  rlang::set_names(paste0(\"sample_\", 1:4)) |>\n  tibble::as_tibble()\n\nsample_results_1b |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|sample_1 |sample_2 |sample_3 |sample_4 |\n|:--------|:--------|:--------|:--------|\n|real     |real     |real     |real     |\n|real     |real     |real     |real     |\n|fake     |real     |fake     |fake     |\n|real     |fake     |real     |fake     |\n|real     |real     |fake     |real     |\n\n\n:::\n:::\n\n\nGenerate 5 articles in 4 samples using map and slice_sample\n\nWe ran the above code four times to see that the result changes will every run.\n\n#### Set the seed\n\nEvery time we run the above code the random number generator (RNG) “starts” at a new place: the random seed. Starting at different seeds can thus produce different samples. To secure reproducibility we have to set the seed. The simulation produces still a random sample but with our seed other people will get the same random values. The book authors apply the number 84735 for the `base::set.seed()` function. The number 84735 is a funny [reference to the name BAYES](https://bayes-rules.github.io/posts/fun/#why-84735).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the seed. Simulate 5 articles.\n\nbase::set.seed(84735)\ndplyr::slice_sample(article, n = 5, weight_by = prior, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   type\n#> 1 fake\n#> 2 fake\n#> 3 real\n#> 4 fake\n#> 5 fake\n```\n\n\n:::\n:::\n\n\nRun the above code several times to see that the values will *not* change. To demonstrate this behavior I will use the same code as in @lst-sample-results-1b but this time with the `set.seed()` function inside the loop.\n\n\n::: {.cell}\n\n```{#sample-results-2b .r .cell-code}\n# Each iteration uses the same seed, so all 5 samples are identical\nsample_results_2b <- purrr::map(1:4, ~ {\n  base::set.seed(84735)\n  fake_news |> \n    dplyr::slice_sample(n = 5) |> \n    dplyr::pull(type)\n}) |>\n  rlang::set_names(paste0(\"sample_\", 1:4)) |>\n  tibble::as_tibble()\n\nsample_results_2b |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|sample_1 |sample_2 |sample_3 |sample_4 |\n|:--------|:--------|:--------|:--------|\n|real     |real     |real     |real     |\n|real     |real     |real     |real     |\n|fake     |fake     |fake     |fake     |\n|fake     |fake     |fake     |fake     |\n|fake     |fake     |fake     |fake     |\n\n\n:::\n:::\n\n\n::: {#cau-002-set-seed-still-radnom .callout-caution}\n###### Using `seed()` still produces numbers randomly\n\nIt is important to understand that these results are still random. Reflecting the potential error and variability in simulation, different seeds would typically give different numerical results though similar conclusions.\n:::\n\n#### Simulate 10.000 article\n\nNo let’s dream bigger: Let us simulate 10,000 articles and store the results in `article_sim` and display the result as a bar chart.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-simulation-and-plot}\n: A bar plot of the fake vs real status of 10,000 simulated articles\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate 10000 articles.\nbase::set.seed(84735)\narticle_sim <-  dplyr::slice_sample(article,\n                                    n = 10000,\n                                    weight_by = prior,\n                                    replace = TRUE)\n\nggplot2::ggplot(article_sim, ggplot2::aes(x = type)) + \n  ggplot2::geom_bar(width = 0.6) +\n  ggplot2::theme(aspect.ratio = 3/1)\n```\n\n::: {.cell-output-display}\n![A bar plot of the fake vs real status of 10,000 simulated articles.](002-bayes-rule_files/figure-html/fig-simulate-and-plot-1.png){#fig-simulate-and-plot width=672}\n:::\n:::\n\n:::\n::::::\n\nReflecting the model @fig-simulate-and-plot from which these 10,000 articles were generated, *roughly* (but not exactly) 40% are fake:\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-simulation-table}\n: Numbers for the `fake` vs `real` status of 10,000 simulated articles\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-simulation-table .cell tbl-cap='Numbers for the `fake` vs `real` status of 10,000 simulated articles.'}\n\n```{.r .cell-code}\narticle_sim  |>  \n  tabyl(type)  |>  \n  adorn_totals(\"row\") |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|type  |     n| percent|\n|:-----|-----:|-------:|\n|fake  |  4031|  0.4031|\n|real  |  5969|  0.5969|\n|Total | 10000|  1.0000|\n\n\n:::\n:::\n\n:::\n::::::\n\n#### Calculate the exclamation point usage\n\nNext, let’s caluclate the exclamation point usage among these 10,000 articles. The `data_model` variable specifies that there’s a 26.67% chance that any fake article and a 2.22% chance that any real article uses exclamation points:\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-calculate-exclamation-point-usage}\n: Calculate exclamation point usage\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\narticle_sim <- article_sim |>\n  dplyr::mutate(data_model = dplyr::case_when\n                (type == \"fake\" ~ 0.2667,\n                 type == \"real\" ~ 0.0222)\n                )\n\ndplyr::glimpse(article_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 10,000\n#> Columns: 2\n#> $ type       <chr> \"fake\", \"fake\", \"real\", \"fake\", \"fake\", \"real\", \"real\", \"re…\n#> $ data_model <dbl> 0.2667, 0.2667, 0.0222, 0.2667, 0.2667, 0.0222, 0.0222, 0.0…\n```\n\n\n:::\n:::\n\n:::\n::::::\n\nFrom this data_model, we can simulate whether each article includes an exclamation point. This syntax is a bit more complicated. First, the `dplyr::group_by()` statement specifies that the exclamation point simulation is to be performed separately for each of the 10,000 articles. Second, we use `base::sample()` to simulate the exclamation point data, `no` or `yes`, based on the `data_model` and store the results as `usage`. Note that `base::sample()` is similar to `slice_sample()` but samples values from vectors instead of rows from data frames.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-002-simulate-exclamation-points}\n: Simulate whether each article includes an exclamation point\n:::\n::::\n\n::: my-r-code-container\n\n::: {#lst-simulate-exclamation-points}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define whether there are exclamation points\ndata <- c(\"no\", \"yes\")\n\n# Simulate exclamation point usage with seed 3\nbase::set.seed(3)\narticle_sim2 <- article_sim |> \n  dplyr::group_by(1:n()) |>  \n  dplyr::mutate(usage = base::sample(data, size = 1, \n                        prob = c(1 - data_model, data_model))) \narticle_sim2 |> \n  tabyl(usage, type) |> \n  adorn_totals(c(\"col\",\"row\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  usage fake real Total\n#>     no 2961 5833  8794\n#>    yes 1070  136  1206\n#>  Total 4031 5969 10000\n```\n\n\n:::\n:::\n\n\nSimulate whether each article includes an exclamation point\n\n:::\n:::\n::::::\n\nThe `article_sim` data frame now contains 10,000 simulated articles with different features, summarized in the table below. The patterns here reflect the underlying likelihoods that *roughly* 28% (1070 / 4031) of fake articles and 2% (136 / 5969) of real articles use exclamation points.\n\n\n::: {.callout-warning #wrn-002-different-seeds.differ-in-magnitude}\n###### Different seeds don't guarantee similar magnitudes\n\nNote that the simulation in @lst-simulate-exclamation-points uses as seed the number 3 and not the previous seed number 84735. The reason is that with `base::set.seed(84735)` we will get extreme different values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate exclamation point usage with seed 84735\nbase::set.seed(84735)\narticle_sim |> \n  dplyr::group_by(1:n()) |>  \n  dplyr::mutate(usage2 = base::sample(data, size = 1, \n                        prob = c(1 - data_model, data_model))) |> \n  tabyl(usage2, type) |> \n  adorn_totals(c(\"col\",\"row\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  usage2 fake real Total\n#>      no 1329 5969  7298\n#>     yes 2702    0  2702\n#>   Total 4031 5969 10000\n```\n\n\n:::\n:::\n\n\nWith seed `84735`, we got 0 real articles with exclamation points, which is extremely unlikely given that `data_model = 0.0222` for real articles. We know that different seeds create (small) different random number sequences that propagate through all 10,000 decisions. This random variations can accumulate - with 10,000 samples, small differences in the random sequence can therefore compound.\n\nThe key insight: **Different seeds don't guarantee similar magnitudes - they just ensure reproducibility.** Some seeds will produce results closer to the expected probabilities, while others (like 84735) may produce more extreme outcomes by chance.\n\nIf we want more stable results that better reflect the true probabilities, we could:\n\n- Use a larger sample (we already have a sample of 10,000, which is good).\n- Run multiple simulations and average to see the long-run behavior.\n- Check if specific seeds are creating outliers.\n\n:::\n\n@fig-exclamation-point-usage-1 provides a visual summary of these article characteristics. Whereas the left plot reflects the relative breakdown of exclamation point usage among real and fake news, the right plot frames this information within the normalizing context that only *roughly* 12% (1206 / 10000) of all articles use exclamation points.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-002-exclamation-point-usage}\n: Bar plots of exclamation point usage, both within fake vs real news and overall (Version 1)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot2::ggplot(article_sim2, ggplot2::aes(x = type, fill = usage)) + \n  ggplot2::geom_bar(position = \"fill\", width = 0.6) + \n  ggplot2::theme(aspect.ratio = 3/1) +\n  ggplot2::scale_fill_viridis_d(option = \"E\")\np2 <- ggplot2::ggplot(article_sim2, aes(x = usage)) + \n  ggplot2::geom_bar(width = 0.6) + \n  ggplot2::theme(aspect.ratio = 3/1)\n\npatchwork:::\"-.ggplot\"(p1, p2)\n```\n\n::: {.cell-output-display}\n![Bar plots of exclamation point usage, both within fake vs real news and overall (Version 1).](002-bayes-rule_files/figure-html/fig-exclamation-point-usage-1-1.png){#fig-exclamation-point-usage-1 width=672}\n:::\n:::\n\n\n::::\n:::::\n\nIn the book is the code for the right plot in @fig-exclamation-point-usage-1 wrong. It says `ggplot2::ggplot(article_sim, aes(x = type))` instead of `ggplot2::ggplot(article_sim, aes(x = usage))` (see: <https://www.bayesrulesbook.com/chapter-2#fig:ch2-bars-articles>).\n\nI believe that even this correction is enough as the right plot should not only show the total numbers of exclamation points but also their relation to the article types as shown in @fig-exclamation-point-usage-2.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-002-exclamation-point-usage}\n: Bar plots of exclamation point usage, both within fake vs real news and overall (Version2)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot 1: Percentage of exclamation usage within each article type\np1 <- ggplot2::ggplot(article_sim2, ggplot2::aes(x = type, fill = usage)) + \n  ggplot2::geom_bar(position = \"fill\", width = 0.6) +\n  ggplot2::theme(aspect.ratio = 3/1) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::labs(\n    title = \"Exclamation Usage by Article Type\",\n    x = \"Article Type\",\n    y = \"Percentage\",\n    fill = \"Usage\"\n  ) +\n  ggplot2::scale_fill_viridis_d(option = \"E\")\n\n# Plot 2: Overall percentage distribution of article types\np2 <- ggplot2::ggplot(article_sim2, aes(x = usage, fill = type)) + \n  ggplot2::geom_bar(width = 0.6) +\n  ggplot2::theme(aspect.ratio = 3/1) +\n  ggplot2::labs(\n    title = \"Article Type by Exclamation Usage\",\n    x = \"Exclamation Usage\",\n    y = \"Count\",\n    fill = \"Type\"\n  ) +\n  ggplot2::scale_fill_viridis_d(option = \"E\")\n\n# Combine plots side by side\npatchwork:::\"-.ggplot\"(p1, p2)\n```\n\n::: {.cell-output-display}\n![Bar plots of exclamation point usage, both within fake vs real news and overall (Version 2).](002-bayes-rule_files/figure-html/fig-exclamation-point-usage-2-1.png){#fig-exclamation-point-usage-2 width=672}\n:::\n:::\n\n\n::::\n:::::\n\nAmong the 1206 simulated articles that use exclamation points, roughly 88.7% are fake. This approximation is quite close to the actual posterior probability of 0.889. Of course, our posterior assessment of this article would change if we had seen different data, i.e., if the title didn’t have exclamation points. Figure 2.4 reveals a simple rule: If an article uses exclamation points, it’s most likely fake. Otherwise, it’s most likely real (and we should read it).\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-002-plot-exlamation-usage}\n: Bar plots of real vs fake news, broken down by exclamation point usage\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot(article_sim2, ggplot2::aes(x = type)) + \n  ggplot2::geom_bar(width = 0.6) + \n  ggplot2::theme(aspect.ratio = 3/1) +\n  ggplot2::facet_wrap(~ usage) \n```\n\n::: {.cell-output-display}\n![Bar plots of real vs fake news, broken down by exclamation point usage.](002-bayes-rule_files/figure-html/fig-plot-exlamation-usage-1.png){#fig-plot-exlamation-usage width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n\n\n## Glossary Entries {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Bayes’ Theorem </td>\n   <td style=\"text-align:left;\"> This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes' Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Conditional_Probability </td>\n   <td style=\"text-align:left;\"> Conditional probability is a measure of the likelihood of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which represents the probability of event A occurring given that event B has already occurred. The mathematical notation uses the pipe symbol for \"conditional on\" or \"given that\". </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Conditional-Probability-Function </td>\n   <td style=\"text-align:left;\"> The conditional probability function describes the probability of an event occurring given that another event has already occurred. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Joint_Probability </td>\n   <td style=\"text-align:left;\"> Joint probability is a statistical measure that calculates the likelihood of two or more events occurring simultaneously at the same point in time. It represents the probability of the intersection of two events, denoted mathematically as P(A∩B), which is read as \"the probability of A and B\". The joint probability of two independent events A and B is computed as the product of their individual probabilities: P(A∩B)=P(A)×P(B). This formula applies when the occurrence of one event does not influence the other. For dependent events, the joint probability is calculated using conditional probability: P(A∩B)=P(A)×P(B∣A) where P(B∣A) is the probability of event B occurring given that event A has already occurred. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Likelihood_x </td>\n   <td style=\"text-align:left;\"> The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E &amp;#124; H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Likelihood-Function </td>\n   <td style=\"text-align:left;\"> The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> LTP </td>\n   <td style=\"text-align:left;\"> The law of total probability (LTP) is a fundamental rule in probability theory that relates marginal probabilities to conditional probabilities. It expresses the total probability of an event that can occur through several distinct, mutually exclusive, and collectively exhaustive scenarios. It is particularly useful when direct computation of an event's probability is difficult, and it enables breaking down complex probability problems into simpler, manageable components. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Marginal_Probability </td>\n   <td style=\"text-align:left;\"> Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model_x </td>\n   <td style=\"text-align:left;\"> A statistical model (both in frequentist and Bayesian statistics) is a mathematical representation that specifies how observed data are generated, often involving assumptions about the underlying processes and parameters. Both approaches use models to represent data-generating processes, but they differ fundamentally in how they treat uncertainty and parameters: frequentist models treat parameters as fixed and focus on the sampling distribution of statistics, while Bayesian models treat parameters as random variables and update beliefs based on data and prior knowledge. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Normalizing Constant </td>\n   <td style=\"text-align:left;\"> In Bayesian statistics, the normalizing constant ensures that the posterior distribution integrates to 1, making it a valid probability distribution. It is also known as the marginal likelihood or evidence, and appears in the denominator of Bayes' theorem. The integral for the normalizing constant is often intractable for complex models because it involves high-dimensional parameter spaces and there exists no closed-form solution for most realistic models. As a result, approximation methods like Markov Chain Monte Carlo (MCMC), bridge sampling, or variational inference are used to estimate or bypass the normalizing constant. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Posterior_Probability </td>\n   <td style=\"text-align:left;\"> Posterior Probability, also called the Posterior, is the updated probability of a hypothesis after incorporating new evidence, calculated using Bayes' theorem. It combines the prior probability—the initial belief about the hypothesis before observing data—with the likelihood, which is the probability of observing the evidence given the hypothesis. The posterior reflects a revised degree of belief in the hypothesis, integrating both background knowledge and observed data.  It's essentially learning from experience - your understanding becomes more refined as you incorporate new information. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Prior_Model </td>\n   <td style=\"text-align:left;\"> In Bayesian statistics, a prior model refers to the specification of a prior distribution, which represents the initial beliefs or assumptions about the parameters of a statistical model before observing any data. This prior distribution quantifies the uncertainty about the parameters. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Prior_Probability </td>\n   <td style=\"text-align:left;\"> The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. It quantifies how likely our initial belief is: P(belief). (BF, Chap.8) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Probability_Model </td>\n   <td style=\"text-align:left;\"> A probability model is a mathematical representation that describes the likelihood of various outcomes in a random experiment. It consists of a sample space, which includes all possible outcomes, and a probability assignment that assigns a probability to each outcome. The probabilities assigned in a probability model must always sum to 1, reflecting the certainty that one of the outcomes will occur. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Simulation_x </td>\n   <td style=\"text-align:left;\"> Simulation is a way to model random events, such that simulated outcomes closely match real-world outcomes. By observing simulated outcomes, researchers gain insight on the real world. (&lt;a href=\"https://stattrek.com/experiments/simulation#\"&gt;Stat Trek&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Total_Probability </td>\n   <td style=\"text-align:left;\"> Total probability is the product of the individual probabilities. Total probability refers to the marginal probability of an observed event B, denoted P(B), which accounts for all possible ways B could occur across mutually exclusive and exhaustive scenarios. This value is crucial because it serves as a normalization constant in Bayes' Theorem, ensuring that the resulting posterior probabilities sum to one. (Brave AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Unconditional_Probabilities </td>\n   <td style=\"text-align:left;\"> Unconditional probability, also known as marginal probability, refers to the likelihood of an event occurring without consideration of any other preceding or future events. It is a stand-alone probability that is not dependent on the occurrence of another event and is denoted as P(A). </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Session Info {.unnumbered}\n\n::::: my-r-code\n::: my-r-code-header\nSession Info\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.5.2 (2025-10-31)\n#>  os       macOS Tahoe 26.2\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2026-01-13\n#>  pandoc   3.8.3 @ /opt/homebrew/bin/ (via rmarkdown)\n#>  quarto   1.8.26 @ /usr/local/bin/quarto\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  ! package        * version  date (UTC) lib source\n#>  P abind            1.4-8    2024-09-12 [?] RSPM\n#>  P backports        1.5.0    2024-05-23 [?] RSPM\n#>  P base64enc        0.1-3    2015-07-28 [?] CRAN (R 4.5.0)\n#>  P bayesplot        1.15.0   2025-12-12 [?] RSPM\n#>  P bayesrules     * 0.0.2    2021-09-25 [?] RSPM\n#>  P boot             1.3-32   2025-08-29 [?] CRAN (R 4.5.2)\n#>  P checkmate        2.3.3    2025-08-18 [?] RSPM\n#>  P class            7.3-23   2025-01-01 [?] CRAN (R 4.5.2)\n#>  P cli              3.6.5    2025-04-23 [?] CRAN (R 4.5.0)\n#>  P codetools        0.2-20   2024-03-31 [?] CRAN (R 4.5.2)\n#>  P colourpicker     1.3.0    2023-08-21 [?] RSPM\n#>  P commonmark       2.0.0    2025-07-07 [?] RSPM\n#>  P crosstalk        1.2.2    2025-08-26 [?] RSPM\n#>  P curl             7.0.0    2025-08-19 [?] RSPM\n#>  P DiagrammeR       1.0.11   2024-02-02 [?] RSPM\n#>  P digest           0.6.39   2025-11-19 [?] CRAN (R 4.5.2)\n#>  P distributional   0.5.0    2024-09-17 [?] RSPM\n#>  P dplyr          * 1.1.4    2023-11-17 [?] RSPM\n#>  P DT               0.34.0   2025-09-02 [?] RSPM\n#>  P dygraphs         1.1.1.6  2018-07-11 [?] RSPM\n#>  P e1071            1.7-17   2025-12-18 [?] RSPM\n#>  P evaluate         1.0.5    2025-08-27 [?] CRAN (R 4.5.1)\n#>  P farver           2.1.2    2024-05-13 [?] RSPM\n#>  P fastmap          1.2.0    2024-05-15 [?] CRAN (R 4.5.0)\n#>  P forcats        * 1.0.1    2025-09-25 [?] RSPM\n#>  P generics         0.1.4    2025-05-09 [?] RSPM\n#>  P ggplot2        * 4.0.1    2025-11-14 [?] RSPM\n#>  P glossary       * 1.0.0    2023-05-30 [?] RSPM\n#>  P glue             1.8.0    2024-09-30 [?] CRAN (R 4.5.0)\n#>  P gridExtra        2.3      2017-09-09 [?] RSPM\n#>  P groupdata2       2.0.5    2024-12-18 [?] RSPM\n#>  P gtable           0.3.6    2024-10-25 [?] RSPM\n#>  P gtools           3.9.5    2023-11-20 [?] RSPM\n#>  P hms              1.1.4    2025-10-17 [?] RSPM\n#>  P htmltools        0.5.9    2025-12-04 [?] CRAN (R 4.5.2)\n#>  P htmlwidgets      1.6.4    2023-12-06 [?] RSPM\n#>  P httpuv           1.6.16   2025-04-16 [?] RSPM\n#>  P igraph           2.2.1    2025-10-27 [?] RSPM\n#>  P inline           0.3.21   2025-01-09 [?] RSPM\n#>  P janitor        * 2.2.1    2024-12-22 [?] RSPM\n#>  P jsonlite         2.0.0    2025-03-27 [?] CRAN (R 4.5.0)\n#>  P kableExtra       1.4.0    2024-01-24 [?] RSPM\n#>  P knitr            1.51     2025-12-20 [?] CRAN (R 4.5.2)\n#>  P labeling         0.4.3    2023-08-29 [?] RSPM\n#>  P later            1.4.5    2026-01-08 [?] RSPM\n#>  P lattice          0.22-7   2025-04-02 [?] CRAN (R 4.5.2)\n#>  P lifecycle        1.0.5    2026-01-08 [?] RSPM\n#>  P litedown         0.9      2025-12-18 [?] RSPM\n#>  P lme4             1.1-38   2025-12-02 [?] RSPM\n#>  P loo              2.9.0    2025-12-23 [?] RSPM\n#>  P lubridate      * 1.9.4    2024-12-08 [?] RSPM\n#>    magrittr         2.0.4    2025-09-12 [1] RSPM\n#>  P markdown         2.0      2025-03-23 [?] RSPM\n#>  P MASS             7.3-65   2025-02-28 [?] CRAN (R 4.5.2)\n#>  P Matrix           1.7-4    2025-08-28 [?] CRAN (R 4.5.2)\n#>  P matrixStats      1.5.0    2025-01-07 [?] RSPM\n#>  P mime             0.13     2025-03-17 [?] CRAN (R 4.5.0)\n#>  P miniUI           0.1.2    2025-04-17 [?] RSPM\n#>  P minqa            1.2.8    2024-08-17 [?] RSPM\n#>  P nlme             3.1-168  2025-03-31 [?] CRAN (R 4.5.2)\n#>  P nloptr           2.2.1    2025-03-17 [?] RSPM\n#>  P otel             0.2.0    2025-08-29 [?] RSPM\n#>  P patchwork        1.3.2    2025-08-25 [?] RSPM\n#>  P pillar           1.11.1   2025-09-17 [?] RSPM\n#>  P pkgbuild         1.4.8    2025-05-26 [?] RSPM\n#>  P pkgconfig        2.0.3    2019-09-22 [?] RSPM\n#>    plyr             1.8.9    2023-10-02 [1] RSPM\n#>  P posterior        1.6.1    2025-02-27 [?] RSPM\n#>  P promises         1.5.0    2025-11-01 [?] RSPM\n#>  P proxy            0.4-29   2025-12-29 [?] RSPM\n#>  P purrr          * 1.2.1    2026-01-09 [?] RSPM\n#>  P QuickJSR         1.8.1    2025-09-20 [?] RSPM\n#>  P R6               2.6.1    2025-02-15 [?] CRAN (R 4.5.0)\n#>  P rbibutils        2.4      2025-11-07 [?] RSPM\n#>  P RColorBrewer     1.1-3    2022-04-03 [?] RSPM\n#>  P Rcpp             1.1.0    2025-07-02 [?] RSPM\n#>  P RcppParallel     5.1.11-1 2025-08-27 [?] RSPM\n#>  P Rdpack           2.6.4    2025-04-09 [?] RSPM\n#>  P readr          * 2.1.6    2025-11-14 [?] RSPM\n#>  P reformulas       0.4.3.1  2026-01-08 [?] RSPM\n#>    renv             1.1.5    2025-07-24 [1] RSPM (R 4.5.0)\n#>  P repr             1.1.7    2024-03-22 [?] RSPM\n#>  P reshape2         1.4.5    2025-11-12 [?] RSPM\n#>  P rlang            1.1.7    2025-12-20 [?] Github (tidyverse/rlang@7a519a2)\n#>  P rmarkdown        2.30     2025-09-28 [?] CRAN (R 4.5.0)\n#>  P rstan            2.32.7   2025-03-10 [?] RSPM\n#>  P rstanarm         2.32.2   2025-09-30 [?] RSPM\n#>  P rstantools       2.5.0    2025-09-01 [?] RSPM\n#>  P rstudioapi       0.17.1   2024-10-22 [?] RSPM\n#>  P S7               0.2.1    2025-11-14 [?] RSPM\n#>  P scales           1.4.0    2025-04-24 [?] RSPM\n#>  P sessioninfo      1.2.3    2025-02-05 [?] RSPM\n#>  P shiny            1.12.1   2025-12-09 [?] RSPM\n#>  P shinyjs          2.1.0    2021-12-23 [?] RSPM\n#>  P shinystan        2.7.0    2025-12-12 [?] RSPM\n#>  P shinythemes      1.2.0    2021-01-25 [?] RSPM\n#>  P skimr            2.2.1    2025-07-26 [?] RSPM\n#>  P snakecase        0.11.1   2023-08-27 [?] RSPM\n#>  P StanHeaders      2.32.10  2024-07-15 [?] RSPM\n#>  P stringi          1.8.7    2025-03-27 [?] RSPM\n#>  P stringr        * 1.6.0    2025-11-04 [?] RSPM\n#>  P survival         3.8-3    2024-12-17 [?] CRAN (R 4.5.2)\n#>  P svglite          2.2.2    2025-10-21 [?] RSPM\n#>  P systemfonts      1.3.1    2025-10-01 [?] RSPM\n#>  P tensorA          0.36.2.1 2023-12-13 [?] RSPM\n#>  P textshaping      1.0.4    2025-10-10 [?] RSPM\n#>  P threejs          0.3.4    2025-04-21 [?] RSPM\n#>  P tibble         * 3.3.0    2025-06-08 [?] RSPM\n#>  P tidyr          * 1.3.2    2025-12-19 [?] RSPM\n#>  P tidyselect       1.2.1    2024-03-11 [?] RSPM\n#>  P tidyverse      * 2.0.0    2023-02-22 [?] RSPM\n#>  P timechange       0.3.0    2024-01-18 [?] RSPM\n#>  P tzdb             0.5.0    2025-03-15 [?] RSPM\n#>  P V8               8.0.1    2025-10-10 [?] RSPM\n#>  P vctrs            0.6.5    2023-12-01 [?] RSPM\n#>  P viridisLite      0.4.2    2023-05-02 [?] RSPM\n#>  P visNetwork       2.1.4    2025-09-04 [?] RSPM\n#>  P withr            3.0.2    2024-10-28 [?] RSPM\n#>  P xfun             0.55     2025-12-16 [?] CRAN (R 4.5.2)\n#>  P xml2             1.5.1    2025-12-01 [?] RSPM\n#>  P xtable           1.8-4    2019-04-21 [?] RSPM\n#>  P xts              0.14.1   2024-10-15 [?] RSPM\n#>  P yaml             2.3.12   2025-12-10 [?] CRAN (R 4.5.2)\n#>  P zoo              1.8-15   2025-12-15 [?] RSPM\n#> \n#>  [1] /Users/petzi/Documents/Meine-Repos/bayes-rules/renv/library/macos/R-4.5/aarch64-apple-darwin20\n#>  [2] /Users/petzi/Library/Caches/org.R-project.R/R/renv/sandbox/macos/R-4.5/aarch64-apple-darwin20/4cd76b74\n#> \n#>  * ── Packages attached to the search path.\n#>  P ── Loaded and on-disk path mismatch.\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n:::\n:::::\n",
    "supporting": [
      "002-bayes-rule_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.9/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/viz-1.8.2/viz.js\"></script>\n<link href=\"site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/grViz-binding-1.0.11/grViz.js\"></script>\n<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}