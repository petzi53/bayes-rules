{
  "hash": "a6826b35004edc4a9a5ffd1691ba5a71",
  "result": {
    "engine": "knitr",
    "markdown": "# Beta-Binomial Bayesian Model {#sec-chap-003}\n\n\n\n:::::: {#obj-chap-003}\n::::: my-objectives\n::: my-objectives-header\nObjectives\n:::\n\n::: my-objectives-container\n-   **Utilize and tune continuous priors.** Learn how to interpret\n    and tune a continuous Beta prior model to reflect the prior\n    information about $\\pi$.\\\n-   **Interpret and communicate features of prior and posterior models**\n    using properties such as mean, mode, and variance.\\\n-   **Construct the fundamental Beta-Binomial model** for proportion $\\pi$.\n:::\n:::::\n::::::\n\n## Introduction {.unnumbered}\n\nIn this chapter we analyse as campaign manager for Minnesota the support\nfor Michelle, who has decided to run for president. We’ve conducted 30\ndifferent polls throughout the election season. Though Michelle’s\nsupport has hovered around 45%, she polled at around 35% in the\ndreariest days and around 55% in the best days on the campaign trail.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-intro-poll-results}\n: Result of 30 polls results (left) and the corresponding prior model\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-intro-poll-results .r .cell-code  lst-cap=\"Result of 30 polls results (left) and the corresponding prior model (right)\"}\n# Set seed for reproducibility\nbase::set.seed(84735)\n\n# Simulate 30 poll results\nn_polls <- 30\nn_voters <- 50\n\n# Generate true support levels from Beta(45, 55)\ntrue_support <- stats::rbeta(n_polls, 45, 55)\n\n# Simulate poll results (number of supporters out of 50)\npoll_counts <- stats::rbinom(n_polls, size = n_voters, prob = true_support)\n\n# Convert to proportions\npoll_proportions <- poll_counts / n_voters\n\n# Create a data frame for plotting\npoll_data <- tibble::tibble(\n  support = poll_proportions\n)\n\n# LEFT PLOT: Histogram of poll results\n# Alternate version to get more control over the bins\np1 <- ggplot2::ggplot(poll_data, ggplot2::aes(x = support)) +\n  ggplot2::geom_histogram(breaks = base::seq(0.3, 0.7, by = 0.05), \n                 fill = \"steelblue\", color = \"white\") +\n  ggplot2::scale_x_continuous(limits = base::c(0, 1), breaks = base::seq(0, 1, 0.1)) +\n  ggplot2::labs(\n    title = \"Distribution of 30 Previous Polls\",\n    x = \"Proportion Supporting Michelle\",\n    y = \"Count\"\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5))\n\n# RIGHT PLOT: Beta(45, 55) prior density\npi_values <- base::seq(0, 1, length.out = 1000)\nprior_density <- stats::dbeta(pi_values, 45, 55)\n\nprior_data <- tibble::tibble(\n  pi = pi_values,\n  density = prior_density\n)\n\np2 <- ggplot2::ggplot(prior_data, ggplot2::aes(x = pi, y = density)) +\n  ggplot2::geom_line(color = \"black\", linewidth = 1) +\n  ggplot2::geom_area(fill = \"lightyellow\", alpha = 0.5) +\n  ggplot2::scale_x_continuous(limits = base::c(0, 1), breaks = base::seq(0, 1, 0.1)) +\n  ggplot2::labs(\n    title = \"Prior Model for π\",\n    x = \"π (Michelle's Current Support)\",\n    y = \"density f(π)\"\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5))\n\n# Display plots side by side\npatchwork:::\"-.ggplot\"(p1, p2)\n```\n\n::: {.cell-output-display}\n![The results of 30 previous polls of Minnesotans’ support of Michelle for president (left) and a corresponding continuous prior model for π, her current election support (right).](003-beta-binomial_files/figure-html/fig-003-intro-poll-results-1.png){#fig-003-intro-poll-results width=672}\n:::\n:::\n\n:::\n::::::\n\nIn @sec-002-discrete-variable-model we assumed that $\\pi$ could only be\n0.2, 0.5, or 0.8 and we therefore defined with the chess example a\n*discrete* probability model. However, in the reality of Michelle’s\nelection support and Kasparov’s chess skill, $\\pi$ can be *any* value\nbetween 0 and 1. We can reflect this reality and conduct a more nuanced\nBayesian analysis by constructing a **continuous prior probability\nmodel** of $\\pi$. A reasonable prior is represented by the curve in\n@fig-003-intro-poll-results (right). Michelle’s support $\\pi$ can be\nanywhere between 0 and 1, but is most likely around 0.45.\n\nWe will dig into the details and build Michelle’s election model. We’ll\nthen generalize this work to the fundamental **Beta-Binomial Bayesian\nmodel**. The power of the Beta-Binomial lies in its broad applications.\nMichelle’s election support $\\pi$ isn’t the only variable of interest\nthat lives on \\[0,1\\]. You might also imagine Bayesian analyses in which\nwe’re interested in modeling the proportion of people that use public\ntransit, the proportion of trains that are delayed, the proportion of\npeople that prefer cats to dogs, and so on. The Beta-Binomial model\nprovides the tools we need to study the proportion of interest, $\\pi$,\nin each of these settings.\n\n## Beta prior model\n\nIn building the Bayesian election model of Michelle’s election support\namong Minnesotans, $\\pi$, we begin as usual: with the prior. Our\n*continuous* prior probability model of $\\pi$ is specified by the\n<a class='glossary' title=''>probability density function</a>\n(pdf) in @fig-003-intro-poll-results. Though it looks quite different,\nthe role of this continuous pdf is the same as for the discrete\n<a class='glossary' title=''>probability mass function</a>\n(pmf) $f (\\pi)$ in @tbl-002-prior-model-chess: to specify all possible\nvalues of $\\pi$ and the relative plausibility of each. That is,\n$f (\\pi)$ answers: What values can $\\pi$ take and which are more\nplausible than others? Further, in accounting for *all* possible\noutcomes of $\\pi$, the pdf *integrates* to or has an area of 1, much\nlike a discrete pmf *sums* to 1.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-003-continuous-probability-models}\n: Continuous probability models\n:::\n::::\n\n::: my-theorem-container\nLet $\\pi$ be a continuous random variable with probability density\nfunction $f (\\pi)$. Then $f (\\pi)$ has the following properties:\n\n-   $f (\\pi) >= 0$;\\\n-   $\\int_\\pi f (\\pi)d\\pi = 1$, i.e., the area under $f (\\pi)$ is 1;\n    and\\\n-   $P (a < \\pi < b) = \\int_{a}^{b} f (\\pi)d\\pi$ when $a <= b$, i.e.,\n    the area between any two possible values a and b corresponds to the\n    probability of $\\pi$ being in this range.\n\n**Interpreting** $f (\\pi)$\n\nIt’s possible that $f (\\pi) > 1$, thus a continuous pdf cannot be\ninterpreted as a probability. Rather, $f (\\pi)$ can be used to compare\nthe plausibility of two different values of $\\pi$: the greater\n$f (\\pi)$, the more plausible the corresponding value of $\\pi$.\n:::\n::::::\n\n### Beta foundation\n\nThe next step is to translate the *picture* of our prior in\n@fig-003-intro-poll-results (right) into a formal probability model of\n$\\pi$. That is, we must specify a formula for the pdf $f (\\pi)$. In the\nworld of probability, there are a variety of common “named” models, the\npdfs and properties of which are well studied. Among these, it’s natural\nto focus on the <a class='glossary' title='The Beta Probability Model is a continuous probability distribution defined on the interval [0, 1], widely used to model probabilities, proportions, or fractions.  It is particularly valuable in Bayesian inference as the conjugate prior for binomial, Bernoulli, and geometric distributions.'>Beta probability model</a> here. Like\nMichelle’s support $\\pi$, a Beta random variable is continuous and\nrestricted to live on \\[0,1\\]. In this section, you’ll explore the\n*properties* of the Beta model and how to **tune** the Beta to reflect\nour prior understanding of Michelle’s support $\\pi$. Let’s begin with a\ngeneral definition of the Beta probability model.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-003-beta-model}\n: Beta model\n:::\n::::\n\n::: my-theorem-container\nLet $\\pi$ be a random variable which can take any value between 0 and 1,\ni.e., $\\pi \\in [0, 1]$. Then the variability in π might be well modeled\nby a Beta model with shape\n<a class='glossary' title='In the Bayesian framework, a hyperparameter is a parameter of a prior distribution.  It defines the shape, location, or scale of the prior belief about a model parameter before observing any data. Unlike model parameters, which are estimated from the data (e.g., the mean or variance of a distribution), hyperparameters are set before the analysis begins and are not directly learned from the data. (Brave-AI)'>hyperparameters</a> $\\alpha > 0$ and\n$\\beta > 0$:\n\n$$\\pi ∼ Beta(\\alpha, \\beta)$$ The Beta model is specified by continuous\npdf\n\n$$\nf (\\pi) =  \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi_{\\alpha-1}(1-\\beta)^{\\beta-1} \\text{ for } \\pi \\in [0, 1]\n$$ {#eq-003-beta-model}\n\nwhere\n$\\Gamma(z) = \\int_{0}^{\\infty}x^{z−1}e^{−y}dx \\text{ and } \\Gamma(z + 1) = z\\Gamma(z)$.\nFun fact: when $z$ is a positive integer, then $\\Gamma(z)$ simplifies to\n$\\Gamma(z) = (z − 1)!$.\n:::\n::::::\n\nThis model is best understood by playing around.\n@fig-003-beta-model-variants plots the Beta pdf $f (\\pi)$ under a\nvariety of shape hyperparameters, $\\alpha$ and $\\beta$. Check out the\nvarious shapes the Beta pdf can take. This flexibility means that we can\ntune the Beta to reflect our prior understanding of $\\pi$ by tweaking\n$\\alpha$ and $\\beta$. For example, notice that when we set\n$\\alpha =\\beta = 1$ (middle left plot), the Beta model is flat from $0$\nto $1$. In this setting, the Beta model is equivalent to perhaps a more\nfamiliar model, the standard\n<a class='glossary' title='The standard uniform distribution is a continuous probability distribution where all values in the interval [0, 1] are equally likely.  It is a special case of the continuous uniform distribution with parameters a = 0 and b = 1, often denoted as U(0, 1). (Brave-AI)'>Uniform model</a>.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-003-uniform-model}\n: The standard Uniform model\n:::\n::::\n\n::: my-theorem-container\nWhen it’s equally plausible for $\\pi$ to take on any value between 0 and\n1, we can model $\\pi$ by the standard Uniform model\n\n$$\\pi ∼ Unif(0, 1)$$ with pdf $f (\\pi) = 1 \\text{ for } \\pi \\in [0, 1]$.\nThe $Unif(0,1)$ model is a special case of $Beta(\\alpha, \\beta)$ when\n$\\alpha = \\beta = 1$.\n:::\n::::::\n\n::: {#nte-plot_beta-warning .callout-note}\n###### Warnings with `bayesrules::plot_beta()`\n\nAt first I wanted for @fig-003-beta-model-variants to use the\n`bayesrules::plot_beta()` function, introduced in\n@sec-003-tuning-beta-prior. But when this function is used with the\nparameters `mean = TRUE` and `mode = TRUE` warnings are issued:\n\n> Warning in geom_segment(aes(x = mean, y = 0, xend = mean, yend =\n> dbeta(mean, : All aesthetics have length 1, but the data has 2 rows. ℹ\n> Please consider using `annotate()` or provide this layer with data\n> containing a single row.\n\nSo I decided to create my own `my_plot_beta()` function. For the\nBeta(1,1) I also got a warning:\n\n> Removed 100 rows containing missing values or values outside the scale\n> range (`geom_vline()`).\n\nFor a more smooth display I turned in @fig-003-beta-model-variants\nwarnings with `warning = false` off:\n:::\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-beta-model-variants}\n: Plot 9 different variants of the Beta model\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-beta-model-variants .r .cell-code  lst-cap=\"`my_plot_beta()` function plotted with 9 examples of Beta models\"}\np1 <- my_plot_beta(1,5, show_mean = TRUE, show_mode = TRUE)\np2 <- my_plot_beta(1,2, show_mean = TRUE, show_mode = TRUE)\np3 <- my_plot_beta(3,7, show_mean = TRUE, show_mode = TRUE)\np4 <- my_plot_beta(1,1, show_mean = TRUE, show_mode = TRUE)\np5 <- my_plot_beta(5,5, show_mean = TRUE, show_mode = TRUE)\np6 <- my_plot_beta(20,20, show_mean = TRUE, show_mode = TRUE)\np7 <- my_plot_beta(7,3, show_mean = TRUE, show_mode = TRUE)\np8 <- my_plot_beta(2,1, show_mean = TRUE, show_mode = TRUE)\np9 <- my_plot_beta(5,1, show_mean = TRUE, show_mode = TRUE)\n\n\nlibrary(patchwork)\ncombined_plot <- p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n                 plot_layout(ncol = 3)\n\n# Display the result\ncombined_plot\n```\n\n::: {.cell-output-display}\n![Beta(α, β) pdfs f (π) under a variety of shape hyperparameters α and β (black curve). The mean and mode are represented by a thinner darkblue solid line and dashed line, respectively.](003-beta-binomial_files/figure-html/fig-003-beta-model-variants-1.png){#fig-003-beta-model-variants width=672}\n:::\n:::\n\n:::\n::::::\n\n:::::: my-assessment\n:::: my-assessment-header\n::: {#cor-003-identify-patterns}\n: Identify patters in @fig-003-beta-model-variants\n:::\n::::\n\n::: my-assessment-container\nTake a minute to see if you can identify some other patterns in how\nshape hyperparameters $\\alpha$ and $\\beta$ reflect the typical values of\n$\\pi$ as well as the variability in $\\pi$.\n\n1.  How would you describe the typical behavior of a\n    $\\text{Beta}(\\alpha, \\beta)$ variable $\\pi$ when $\\alpha = \\beta$?\n\n\n::: {.cell}\n\n:::\n\n\n<div class='webex-radiogroup' id='radio_QJUSPKPVOZ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_QJUSPKPVOZ\" value=\"\"></input> <span>Right-skewed with π tending to be less than 0.5.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_QJUSPKPVOZ\" value=\"answer\"></input> <span>Symmetric with π tending to be around 0.5.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_QJUSPKPVOZ\" value=\"\"></input> <span>Left-skewed with π tending to be greater than 0.5.</span></label></div>\n\n\n2.  Using the same options as above, how would you describe the typical\n    behavior of a $\\text{Beta}(\\alpha, \\beta)$ variable $\\pi$ when\n    $\\alpha > \\beta$?\n\n\n::: {.cell}\n\n:::\n\n\n<div class='webex-radiogroup' id='radio_ILBQWLXGOU'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ILBQWLXGOU\" value=\"\"></input> <span>Right-skewed with π tending to be less than 0.5.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ILBQWLXGOU\" value=\"\"></input> <span>Symmetric with π tending to be around 0.5.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_ILBQWLXGOU\" value=\"answer\"></input> <span>Left-skewed with π tending to be greater than 0.5.</span></label></div>\n\n\n2.  For which model is there greater variability in the plausible values\n    of $\\pi$, $\\text{Beta}(20,20)$ or $\\text{Beta}(5,5)$?\n\n\n::: {.cell}\n\n:::\n\n\n<div class='webex-radiogroup' id='radio_QFQCDAVDCT'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_QFQCDAVDCT\" value=\"\"></input> <span>Beta(20,20)</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_QFQCDAVDCT\" value=\"answer\"></input> <span>Beta(5,5)</span></label></div>\n\n:::\n::::::\n\nWe can support our observations of the behavior in $\\pi$ with numerical\nmeasurements. The **mean** (or “expected value”) and **mode** of $\\pi$\nprovide **measures of central tendency**, or what’s *typical.*\nConceptually speaking, the mean captures the average value of $\\pi$,\nwhereas the mode captures the most *plausible* value of $\\pi$, i.e., the\nvalue of $\\pi$ at which pdf $f (\\pi)$ is maximized. These measures are\nrepresented by the solid and dashed vertical lines, respectively, in\n@fig-003-beta-model-variants. Notice that when $\\alpha$ is less\nthan$\\beta$ (top row), the Beta pdf is right skewed, thus the mean\nexceeds the mode of $\\pi$ and both are below 0.5. The opposite is true\nwhen $\\alpha$ is greater than $\\beta$ (bottom row). When $\\alpha$ and\n$\\beta$ are equal (center row), the Beta pdf is symmetric around a\ncommon mean and mode of 0.5. These trends reflect the formulas for the\nmean, denoted $E(\\pi)$, and mode for a $\\text{Beta}(\\alpha, \\beta)$\nvariable $\\pi%$:\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-003-beta-mean-mode}\n: Formulae for Beta mean and mode\n:::\n::::\n\n::: my-theorem-container\n$$\n\\begin{align*}\nE (\\pi) &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\text{Mode} (\\pi) &= \\frac{\\alpha -1}{\\alpha + \\beta - 2} \\text{ when } \\alpha, \\beta > 1\n\\end{align*}\n$$ {#eq-003-beta-mean-mode}\n:::\n::::::\n\nFor example, the central tendency of a $\\text{Beta}(5, 5)$ variable\n$\\pi$ can be described by\n\n$$\nE (\\pi) = \\frac{5}{5 + 5} = 0.5 \\text{ and }  \\text{  Mode}(\\pi) = \\frac{5 -1}{5 + 5 - 2} = 0.5\n$$\n\n@fig-003-beta-model-variants also reveals patterns in the variability of\n$\\pi$. For example, with values that tend to be closer to the mean of\n0.5, the variability in$\\pi$ is smaller for the $\\text{Beta}(20,20)$\nmodel than for the \\text{Beta}(5,5)\\$ model.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-003-beta-var-sd}\n: Formulae for Beta variance and standard deviation\n:::\n::::\n\n::: my-theorem-container\nWe can measure the variability of a $\\text{Beta}(\\alpha, \\beta)$ random\nvariable $\\pi$ by **variance**\n\n$$\n\\begin{align*}\nVar(\\pi) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(α + β + 1)}\n\\end{align*}\n$$ {#eq-003-beta-variance}\n\nRoughly speaking, variance measures the typical *squared* distance of π\nvalues from the mean, E($\\pi$). Since the variance thus has squared\nunits, it’s typically easier to work with the **standard deviation**\nwhich measures the typical *unsquared* distance of $\\pi$ values from\nE($\\pi$):\n\n$$\nSD(\\pi) ∶=  \\sqrt{Var(π)}\n$$ {#eq-003-beta-sd}\n:::\n::::::\n\nFor example, the values of a $\\text{Beta}(5, 5)$ variable $\\pi$ tend to\ndeviate from their mean of 0.5 by 0.151, whereas the values of a\n$\\text{Beta}(20, 20)$ variable tend to deviate from 0.5 by only 0.078:\n\n$$\n\\sqrt{\\frac{5 \\cdot 5}{(5 + 5)^2(5+5+1)}} = 0.151 \\text{ and } \\sqrt{\\frac{20 \\cdot 20}{(20 + 20)^2(20+20+1)}} = 0.078\n$$\n\nThe formulas above don’t magically pop out of nowhere. They are obtained\nby applying general definitions of mean, mode, and variance to the Beta\npdf @eq-003-beta-mean-mode. I skip here the theory behind measuring\ncentral tendency and variability.\n\n### Tuning the Beta prior {#sec-003-tuning-beta-prior}\n\nWith a sense for how the $\\text{Beta}(\\alpha, \\beta)$ model works, let’s\ntune the shape hyperparameters $\\alpha$ and $\\beta$ to reflect our prior\ninformation about Michelle’s election support $\\pi$. We saw in Figure\n@fig-003-intro-poll-results (left) that across 30 previous polls,\nMichelle’s average support was around 45 percentage points, though she\nroughly polled as low as 25 and as high as 55[^003-beta-binomial-1]\npercentage points. Our $\\text{Beta}(\\alpha, \\beta)$ prior should have\nsimilar patterns. For example, we want to pick $\\alpha$ and $\\beta$ for\nwhich $\\pi$ tends to be around 0.45,\n$E(\\pi) = \\alpha /(\\alpha + \\beta) ≈ 0.45$. Or, after some rearranging,\n\n[^003-beta-binomial-1]: and not 65 as printed in the book\n\n$$\\alpha \\approx \\frac{9}{11}\\beta$$\n\n:::::: my-procedure\n:::: my-procedure-header\n::: {#prp-003-rearrange-formula-for-beta-prior}\n: Rearrange formula $E(\\pi) = \\alpha /(\\alpha + \\beta) ≈ 0.45$\n:::\n::::\n\n::: my-procedure-container\n1.  **Start with the equation**:\\\n    $\\frac{\\alpha}{\\alpha + \\beta} \\approx 0.45$\n\n2.  **Multiply both sides by** $(\\alpha + \\beta)$ to eliminate the\n    denominator:\\\n    $\\alpha \\approx 0.45(\\alpha + \\beta)$\n\n3.  **Expand the right-hand side**:\\\n    $\\alpha \\approx 0.45\\alpha + 0.45\\beta$\n\n4.  **Subtract** $0.45\\alpha$ from both sides to collect terms with\n    $\\alpha$ on one side:\\\n    $\\alpha - 0.45\\alpha \\approx 0.45\\beta$\\\n    $0.55\\alpha \\approx 0.45\\beta$\n\n5.  **Divide both sides by 0.55** to isolate $\\alpha$:\\\n    $\\alpha \\approx \\frac{0.45}{0.55}\\beta$\n\n6.  **Simplify the fraction**:\\\n    $\\alpha \\approx \\frac{9}{11}\\beta$ (since\n    $0.45/0.55 = 45/55 = 9/11$)\n\n7.  **Final Result**:\\\n    $$\\alpha \\approx \\frac{9}{11}\\beta$$\n:::\n::::::\n\nIn a trial and error process, we use `bayesrules::plot_beta()` to plot\nBeta models with $\\alpha$ and $\\beta$ pairs that meet this constraint\n(e.g., Beta(9,11), Beta(18,22), Beta(27,33), Beta(36,44), Beta(45,55),\nBeta(54,66)). Among these, we find that the Beta(45,55) closely captures\nthe typical outcomes and variability in the old polls:\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-trial-and-error-beta-prior}\n: Trial and error to get similar plot as in the right part of\n@fig-003-intro-poll-results\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-trial-and-error-beta-prior .r .cell-code  lst-cap=\"Six different Beta probability density functions (pdf).\"}\nlibrary(patchwork)\n\np1 <- my_plot_beta(9, 11)\np2 <- my_plot_beta(18, 22)\np3 <- my_plot_beta(27, 33)\np4 <- my_plot_beta(36, 44)\np5 <- my_plot_beta(45, 55)\np6 <- my_plot_beta(54, 66)\n\n(\n    combined_plot <- p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol = 3)\n)\n```\n\n::: {.cell-output-display}\n![Beta probability density function. From left to right, vom top to down: Beta(9,11), Beta(18, 22), Beta(27, 33), Beta(36, 44), Beta(45, 55), Beta(54, 66).](003-beta-binomial_files/figure-html/fig-003-trial-and-error-beta-prior-1.png){#fig-003-trial-and-error-beta-prior width=672}\n:::\n:::\n\n:::\n::::::\n\n::: {#cau-003- .callout-caution}\n###### Difficult to find the appropriate model\n\nIt is difficult in @fig-003-trial-and-error-beta-prior to decide which\nis the appropriate Beta model. Even producing bigger graphs displaying\nthe measures of central tendency numerically and the x-coordinate with a\nmouse click in a Shiny app is not helpful. An exact visual estimate of\nthe 95% (or 99%) confidence interval (= +/- 2 or +/- 3 standard\ndeviations respectively) is not possible. One would have to show these\nlimits in the graph itself.\n\nI have added the 95% credible interval values into\n@fig-003-plot-beta-trial. Now we can see the details and it is now easy\nto decide which Beta values have the nearest 95% credible interval to\nthe previous polls for Michelle’s support.\n:::\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-plot-beta-trials}\n: Plot Beta trials with showing a 95% <a class='glossary' title='In Bayesian statistics, a credible interval is an interval within which an unobserved parameter value falls with a particular probability. It is an interval in the domain of a posterior probability distribution or a predictive distribution. The generalisation to multivariate problems is the credible region. Credible intervals are a Bayesian analog to confidence intervals in frequentist statistics. The interval is fixed (after data is observed), the parameter is random. Interpretation: ‘There is a 95% probability that the parameter lies within this interval, given the data.’ (Wikipedia and Brave-AI)'>credible interval</a>\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-plot-beta-trial .r .cell-code  lst-cap=\"Plot several Beta trials showing a 95% credible interval\"}\nmy_plot_beta_trial <- function(alpha, beta) {\n\n    library(bayesrules)\n    library(ggplot2)\n    \n    # Calculate the quantiles\n    ci_95_lower <- qbeta(0.025, alpha, beta)\n    ci_95_upper <- qbeta(0.975, alpha, beta)\n    \n    # Create the Beta(alpha, beta) plot\n    p <- my_plot_beta(alpha, beta)\n    \n    # Add confidence interval lines and labels\n    p + \n      geom_vline(xintercept = ci_95_lower, col = \"blue\", lty = 2, linewidth = 0.8) +\n      geom_vline(xintercept = ci_95_upper, col = \"blue\", lty = 2, linewidth = 0.8) +\n      \n      # Add text labels for 95% CI\n      annotate(\"text\", x = ci_95_lower, y = 0.5, \n               label = round(ci_95_lower, 3), \n               color = \"blue\", hjust = 1.1, size = 3.5) +\n      annotate(\"text\", x = ci_95_upper, y = 0.5, \n               label = round(ci_95_upper, 3), \n               color = \"blue\", hjust = -0.1, size = 3.5)\n\n}\n\np1 <- my_plot_beta_trial(9, 11)\np2 <- my_plot_beta_trial(18, 22)\np3 <- my_plot_beta_trial(27, 33)\np4 <- my_plot_beta_trial(36, 44)\np5 <- my_plot_beta_trial(45, 55)\np6 <- my_plot_beta_trial(54, 66)\n\n\nlibrary(patchwork)\n(\n    combined_plot <- p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol = 3)\n)\n```\n\n::: {.cell-output-display}\n![Beta trials showing a 95% credible interval](003-beta-binomial_files/figure-html/fig-003-plot-beta-trial-1.png){#fig-003-plot-beta-trial width=672}\n:::\n:::\n\n:::\n::::::\n\nThus, a reasonable prior model for Michelle’s election support is\\\n$$\\pi \\sim\\text{Beta}(45, 55)$$.\n\nwith **prior pdf** $f (\\pi)$ following from plugging 45 and 55 into\n(@eq-003-beta-model),\n\n$$\nf (\\pi) =  \\frac{\\Gamma(100)}{\\Gamma(45)\\Gamma(55)}\\pi^{44}(1 - \\pi)^{54} \\text{ for } \\pi \\in [0, 1]\n$$ {#eq-003-prior-pdf-45-55}\n\nBy (@eq-003-beta-mean-mode), this model specifies that Michelle’s\nelection support is most likely around 45 percentage points, with\n**prior mean** and **prior mode**\n\n$$\nE(\\pi) = \\frac{45}{45 + 55} = 0.4500 \\text{ and } Mode(\\pi) = \\frac{45 - 1}{45 + 55 -2} = 0.4490.\n$$ {#eq-prior-mean-and-mode-45-55}\n\nFurther, by (@eq-003-beta-variance), the potential variability in $\\pi$\nis described by a **prior standard deviation** of 0.05. That is, our\nother prior assumptions about Michelle’s possible election support tend\nto deviate by 5 percentage points from the prior mean of 45%:\n\n$$\n\\begin{align*}\nVar(\\pi) &= \\frac{45 \\cdot 55}{(45 + 55)^2{45 + 55 +1}} = 0.0025 \\\\\nSD(\\pi) &= \\sqrt{0-0025} = 0.05\n\\end{align*}\n$$ {#eq-003-sd-45-55}\n\n## Binomial data model & likelihood function\n\nIn the second step of our Bayesian analysis of Michelle’s election\nsupport $\\pi$, you’re ready to collect some data. You plan to conduct a\nnew poll of $n = 50$ Minnesotans and record $Y$ , the number that\nsupport Michelle. The results depend upon, and thus will provide insight\ninto, $\\pi$ – the greater Michelle’s actual support, the greater $Y$\nwill tend to be.\n\nTo *model* the dependence of $Y$ on $\\pi$, we can make the following\nassumptions about the poll:\n\n(1) voters answer the poll *independently* of one another; and\n(2) the *probability* that any polled voter supports your candidate\n    Michelle is $\\pi$.\n\nIt follows from our work in\n@sec-002-conditional-probabilities-chess-game that, conditional on\n$\\pi$, $Y$ is Binomial. Specifically,\n\n$$Y ∣π ∼ Bin(50, π)$$ with conditional pmf $f (y \\mid \\pi)$ defined for\n$y \\in \\{0, 1, 2, ..., 50\\}$,\n\n$$\nf (y \\mid \\pi) = P (Y = y \\mid \\pi) = \\binom{50}{y} \\pi^y(1 − π)^{50−y}.\n$$ {#eq-003-binomial-data-model}\n\nThe <a class='glossary' title='The Conditional Probability Mass Function (PMF) is the probability distribution of a discrete random variable X given that another discrete random variable Y has taken a specific value y. It is defined using the joint probability mass function and the marginal probability mass function.'>conditional pmf</a> $f (y \\mid \\pi)$ provides\nanswers to a *hypothetical* question: *if* Michelle’s support were some\ngiven value of $\\pi$, then how many of the 50 polled voters $Y = y$\nmight we expect to support her? @fig-003-binomial-data-model plots this\npmf under a range of possible $\\pi$. These plots formalize our\nunderstanding that *if* Michelle’s support $\\pi$ were low (top row), the\npolling result $Y$ is also likely to be low. *If* her support were high\n(bottom row), $Y$ is also likely to be high.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-binomial-data-model}\n: Conditional Binomial Model of Michelle’s support in 50 polls\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-binomial-data-model .r .cell-code  lst-cap=\"Bin(50, π) pmf f (y∣π) is plotted for values of π ∈ {0.1, 0.2, . . . , 0.9}.\"}\n# Create data frame with all combinations\ndf <- base::expand.grid(\n  x = 0:50,  # number of successes (0 to size)\n  prob = base::seq(\n      from = 0.1, \n      to = 0.9, \n      by = 0.1)\n) |> \n  dplyr::mutate(\n    probability = stats::dbinom(x, size = 50, prob = prob),\n    prob_label = base::paste(\"Bin(50, \", prob, \")\"),\n    highlight = dplyr::if_else(x == 30, \"black\", \"gray\")\n  )\n\n# Create lollipop chart with facets\nggplot2::ggplot(df, ggplot2::aes(x = x, y = probability)) +\n  ggplot2::geom_linerange(\n    ggplot2::aes(\n      x = x, \n      ymin = 0, \n      ymax = probability, \n      color = highlight), \n    linewidth = 0.8) +\n  ggplot2::geom_point(ggplot2::aes(color = highlight), size = 1) +\n  ggplot2::scale_color_identity() +\n  ggplot2::facet_wrap(~ prob_label, ncol = 3) +\n  ggplot2::scale_x_continuous(breaks = base::seq(0, 50, by = 5)) +\n  ggplot2::scale_y_continuous(breaks = base::seq(0, 0.20, by = 0.05)) +\n  ggplot2::labs(\n    x = \"y\",\n    y = base::expression(\"f (y|π)\"),\n    title = \"Binomial Distribution (n = 50)\"\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(\n    panel.grid.minor.y = ggplot2::element_line(color = \"gray90\", linewidth = 0.2),\n    panel.grid.major.y = ggplot2::element_line(color = \"gray60\", linewidth = 0.5),\n    panel.grid.major.x = ggplot2::element_line(color = \"gray80\", linewidth = 0.3),\n    panel.spacing = grid::unit(1.5, \"lines\"),\n    strip.background = ggplot2::element_rect(fill = \"gray90\", color = \"gray50\"),\n    strip.text = ggplot2::element_text(face = \"bold\", size = 11)\n  )\n```\n\n::: {.cell-output-display}\n![The Bin(50, π) pmf f (y∣π) is plotted for values of π ∈ {0.1, 0.2, . . . , 0.9}. The pmfs at the observed value of polling data Y = 30 are highlighted in black.](003-beta-binomial_files/figure-html/fig-003-binomial-data-model-1.png){#fig-003-binomial-data-model width=672}\n:::\n:::\n\n:::\n::::::\n\nIn *reality*, we ultimately observe that the poll was a huge success: Y\n= 30 of n = 50 (60%) polled voters support Michelle! This result is\nhighlighted by the black lines among the pmfs in\n@fig-003-binomial-data-model. To focus on just these results that match\nthe observed polling data, we extract and compare these black lines in a\nsingle plot (Figure 3.5). These represent the *likelihoods* of the\nobserved polling data, $Y = 30$, at each potential level of Michelle’s\nsupport $\\pi$ in $\\{0.1, 0.2, . . . , 0.9\\}$. In fact, this discrete set\nof scenarios represents a small handful of points along the complete\ncontinuous <a class='glossary' title='The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI)'>likelihood function</a>\n$L(\\pi \\mid y = 30)$ defined for any $\\pi$ between $0$ and $1$ (black\ncurve).\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-likelihood-function}\n: Conditional likelihood function for Michelle’s support in 50 polls\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-likelihood-function .r .cell-code  lst-cap=\"Likelihood function of Michelle’s election support\"}\n# Define the likelihood function\nlikelihood <- function(pi, y = 30, n = 50) {\n  base::choose(n, y) * pi^y * (1 - pi)^(n - y)\n}\n\n# Create data for continuous curve\ndf_continuous <- tibble::tibble(\n  pi = base::seq(0, 1, length.out = 1000)\n    ) |> \n  dplyr::mutate(L = purrr::map_dbl(pi, likelihood))\n\n# Create data for discrete vertical lines\ndf_discrete <- tibble::tibble(\n  pi = base::seq(0.1, 0.9, by = 0.1)\n    )  |> \n  dplyr::mutate(L = purrr::map_dbl(pi, likelihood))\n\n# Create the plot\nggplot2::ggplot() +\n  # Continuous likelihood curve\n  ggplot2::geom_line(data = df_continuous, \n            ggplot2::aes(x = pi, y = L), \n            linewidth = 1.2) +\n  # Vertical lines at discrete values\n  ggplot2::geom_segment(data = df_discrete, \n               ggplot2::aes(x = pi, xend = pi, y = 0, yend = L),\n               linewidth = 0.8) +\n  # Points at discrete values\n  ggplot2::geom_point(data = df_discrete, \n             ggplot2::aes(x = pi, y = L), \n             size = 3) +\n  # Labels and formatting\n  ggplot2::labs(\n    x = base::expression(pi),\n    y = base::expression(L(pi*\"|\"*y==30)),\n    title = base::expression(\"Likelihood Function: \"*L(pi*\"|\"*y==30))\n  ) +\n  ggplot2::scale_x_continuous(breaks = seq(0, 1, 0.1)) +\n  ggplot2::theme_minimal(base_size = 14) +\n  ggplot2::theme(\n    plot.title = ggplot2::element_text(hjust = 0.5, size = 16),\n    axis.title = ggplot2::element_text(size = 14)\n  )\n\n# Print key values using tidyverse approach\ntibble::tibble(\n  pi = c(0.4, 0.5, 0.6, 0.7, 0.8),\n  likelihood = purrr::map_dbl(pi, likelihood)\n    )  |> \n  dplyr::mutate(label = stringr::str_glue(\n      \"L(π={pi}|y=30) = {round(likelihood, 6)}\")) |> \n  dplyr::pull(label) |> \n  purrr::walk(cat, \"\\n\")\n\n# Find maximum\ndf_continuous  |> \n  dplyr::slice_max(L, n = 1)  |> \n  dplyr::mutate(label = stringr::str_glue(\n      \"Maximum likelihood occurs at π = {round(pi, 3)}\"))  |> \n  dplyr::pull(label)  |> \n  base::cat(\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> L(π=0.4|y=30) = 0.001987 \n#> L(π=0.5|y=30) = 0.041859 \n#> L(π=0.6|y=30) = 0.114559 \n#> L(π=0.7|y=30) = 0.037039 \n#> L(π=0.8|y=30) = 0.000612 \n#> Maximum likelihood occurs at π = 0.6\n```\n\n\n:::\n\n::: {.cell-output-display}\n![The likelihood function, $L(\\pi \\mid y = 30)$, of Michelle’s election support π given the observed poll in which $Y = 30$ of $n = 50$ polled Minnesotans supported her. The vertical lines represent the likelihood evaluated at $\\pi$ in $\\{0.1, 0.2, . . . , 0.9\\}$.](003-beta-binomial_files/figure-html/fig-003-likelihood-function-1.png){#fig-003-likelihood-function width=480}\n:::\n:::\n\n:::\n::::::\n\nRecall that the likelihood function is defined by turning the Binomial\npmf on its head. Treating $Y = 30$ as observed data and $\\pi$ as\nunknown, matching the reality of our situation, the Binomial likelihood\nfunction of $\\pi$ follows from plugging $y = 30$ into the Binomial pmf\n(@eq-003-binomial-data-model):\n\n$$\nL(\\pi \\mid y = 30) = \\binom{50}{30} \\pi^{30}(1 − \\pi)^{20} \\text{ for } \\pi \\in [0, 1].\n$$ {#eq-003-binomial-likelihood-function-michelle}\n\nFor example, matching what we see in @fig-003-likelihood-function, the\nchance that $Y = 30$ of $50$ polled voters would support Michelle is\n$0.115$ if her underlying support were $\\pi = 0.6$:\n\n$$\nL(\\pi = 0.6 \\mid y = 30) = \\binom{50}{30} 0.6^{30}(1 − 0.6)^{20} \\approx 0.115\n$$\n\nbut only$0.042$ if her underlying support were $\\pi = 0.5$:\n\n$$\nL(\\pi = 0.5 \\mid y = 30) = \\binom{50}{30} 0.5^{30}(1 − 0.5)^{20} \\approx 0.042\n$$\n\nThe likelihood function is calculated in R with the following code\nsnippet:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlikelihood <- function(pi, y, n) {\n  base::choose(n, y) * pi^y * (1 - pi)^(n - y)\n}\n```\n:::\n\n\nFor instance: From $n = 50$ people that were asked (= polls), $Y = 30$\npeople voted for Michelle ($= 60\\% = 0.6 \\pi$), so the result of\n`likelihood(pi = 0.6, y = 30, n = 50)` =\n0.1145586.\n\nIt’s also important to remember here that $L(\\pi \\mid y = 30)$ is a\nfunction of \\pi that provides insight into the relative compatibility of\nthe observed polling data $Y = 30$ with different $\\pi \\in [0, 1]$. The\nfact that $L(\\pi \\mid y = 30)$ is maximized when $\\pi = 0.6$ suggests\nthat the 60% support for Michelle among polled voters is most likely\nwhen her underlying support is also at 60%. This makes sense! The\nfarther that a hypothetical $\\pi$ value is from $0.6$, the less likely\nwe would be to observe our poll result – $L(\\pi \\mid y = 30)$\neffectively drops to 0 for $\\pi$ values under $0.3$ and above $0.9$.\nThus, it’s extremely unlikely that we would’ve observed a 60% support\nrate in the new poll if, in fact, Michelle’s underlying support were as\nlow as 30% or as high as 90%.\n\n## The Beta posterior model\n\nWe now have two pieces of our Bayesian model in place:\n\n1.  the <a class='glossary' title='The Beta Probability Model is a continuous probability distribution defined on the interval [0, 1], widely used to model probabilities, proportions, or fractions.  It is particularly valuable in Bayesian inference as the conjugate prior for binomial, Bernoulli, and geometric distributions.'>Beta</a>\n    <a class='glossary' title='In Bayesian statistics, a prior model refers to the specification of a prior distribution, which represents the initial beliefs or assumptions about the parameters of a statistical model before observing any data. This prior distribution quantifies the uncertainty about the parameters.'>prior model</a> for Michelle’s support\n    $\\pi$ and\n2.  the <a class='glossary' title='Binomial model is a foundational framework in Bayesian statistics used to estimate the probability of success in a series of independent trials. It combines a binomial likelihood — which models the number of successes in a fixed number of trials—with a beta prior on the success probability, resulting in a beta posterior. This conjugate relationship allows for analytical computation and intuitive interpretation. (Brave-AI)'>Binomial model</a> for the dependence of polling\n    data Y on $\\pi$:\n\n$$\n\\begin{align*}\nY \\mid \\pi &\\sim Bin(50, \\pi)\\\\\n\\pi &\\sim Beta(45, 55)\n\\end{align*}\n$$\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-prior-scaled-likelihood}\n: Prior model with scaled likelihood function\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-prior-scaled-likelihood .r .cell-code  lst-cap=\"Prior model with scaled likelihood function\"}\n# Parameters\nn <- 50  # number of trials\nalpha_prior <- 45\nbeta_prior <- 55\ny_observed <- 30  # 30 successes out of 50 trials\n\n# Create a range of pi values\npi_values <- base::seq(0, 1, length.out = 1000)\n\n# Prior: Beta(45, 55)\nprior <- stats::dbeta(pi_values, alpha_prior, beta_prior)\n\n# Likelihood: Bin(50, pi) for observed y\nlikelihood <- stats::dbinom(y_observed, n, pi_values)\n\n# Scale the likelihood to integrate to 1 (normalize to a probability density)\n# The integral of a binomial likelihood over pi is 1, so we just need to normalize\n# We can approximate the integral using the trapezoidal rule\n# integral_likelihood <- base::sum(base::diff(pi_values) * (likelihood[-1] + likelihood[-length(likelihood)])) / 2\n# scaled_likelihood <- likelihood / integral_likelihood\n\n# Normalize the likelihood to integrate to 1\nscaled_likelihood <- likelihood / sum(likelihood * (pi_values[2] - pi_values[1]))\n\n# Create data frame for plotting\nplot_data <- tibble::tibble(\n  pi = pi_values,\n  Prior = prior,\n  `Scaled Likelihood` = scaled_likelihood\n)\n\n# Reshape data for ggplot2\nplot_data_long <- tidyr::pivot_longer(plot_data, \n                                cols = c(Prior, `Scaled Likelihood`),\n                                names_to = \"Distribution\",\n                                values_to = \"Density\")\n\n# Plot with filled areas\nggplot2::ggplot(plot_data_long, \n      ggplot2::aes(x = pi, y = Density, fill = Distribution)) +\n  ggplot2::geom_area(alpha = 0.6, position = \"identity\") +\n  ggplot2::geom_line(aes(color = Distribution), linewidth = 1, color = \"black\") +\n  ggplot2::scale_fill_manual(values = c(\"Prior\" = \"#f0e442\", \n                                \"Scaled Likelihood\" = \"#0071b2\")) +\n  ggplot2::labs(x = base::expression(pi), \n       y = \"Density\",\n       title = \"Prior and Scaled Likelihood\") +\n  ggplot2::theme_minimal() +\n  theme(legend.position = \"right\",\n        legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![The prior model of $\\pi$ along with the (scaled) likelihood function of $\\pi$ given the new poll results in which $Y = 30$ of $n = 50$ polled Minnesotans support Michelle.](003-beta-binomial_files/figure-html/fig-003-prior-scaled-likelihood-1.png){#fig-003-prior-scaled-likelihood width=672}\n:::\n:::\n\n:::\n::::::\n\nIn @fig-003-prior-scaled-likelihood I have used my own code using\n{**ggplot2**} because the function\n`bayesrules::plot_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50)`\ndisplays also the posterior model. But for educational reasons it is\nimportant at this stage only to compare prior model and (scaled)\nlikelihood function.\n\n::: {#nte-003-prior-scaled-likelihood .callout-note}\n###### Scaling likelihood to integrate to 1\n\nI didn't know how to compute the area under the likelihood function to\nintegrate it to 1. I used <a class='glossary' title='Claude Sonnet 4.5 is Anthropic’s most advanced model in the Sonnet series, released on September 29, 2025, and is now the default model for general use across Anthropic’s platforms, including claude.ai, mobile apps, and the API.'>Claude Sonnet 4.5</a> which\nproposed the likelihood scaling. The procedure consists of several\nsteps, where two steps were completely new for me:\n\n**1. Trapezoidal rule**:\n\nThe trapezoidal rule is a numerical integration method used to\napproximate the area under a curve. It approximates the area under a\ncurve by dividing it into trapezoids rather than rectangles (as in the\nRiemann sum).\n\nFor a function f(x) over interval \\[a,b\\]:\n\n-   Divide the interval into n subintervals\n-   Approximate the area under the curve in each subinterval as a\n    trapezoid\n-   Sum the areas of all trapezoids\n\nThe trapezoidal rule is more accurate than the rectangle rule because:\n\n-   It accounts for the slope of the function between points\n-   It's exact for linear functions\n-   It's more accurate for smooth functions than the rectangle rule\n\nIn the context of our R code:\n\n-   `pi_values` represents the x-axis values ($\\pi$ values from 0 to 1)\n-   `likelihood` represents the y-axis values (the likelihood function)\n-   `diff(pi_values)` gives the width of each subinterval\n\nThe trapezoidal rule approximates the integral as:\n`sum(diff(pi_values) * (likelihood[-1] + likelihood[-length(likelihood)])) / 2`\n\n**2. `diff()` function**\n\nThe `diff()` function in R calculates the **differences between\nconsecutive elements** of a vector. In the context of our code,\n`diff(pi_values)` is used to compute the spacing between adjacent values\nin the `pi_values` vector.\n\nHere's a breakdown of what it does:\n\n-   **Input**: `pi_values` is a sequence of values from 0 to 1, created\n    with `seq(0, 1, length.out = 1000)`. This creates 1000 evenly spaced\n    values between 0 and 1.\n-   **What diff() does**: If `pi_values` has values `[a, b, c, d, ...]`,\n    then `diff(pi_values)` returns `[b-a, c-b, d-c, ...]`. Since\n    `pi_values` is evenly spaced, all differences will be equal to\n    `1/999` (approximately 0.001001)\n\n3.  **Why we use it**: In the integral calculation, we use the\n    trapezoidal rule:\n    `sum(diff(pi_values) * (likelihood[-1] + likelihood[-length(likelihood)])) / 2`.\n    In this formula `diff(pi_values)` gives us the width of each\n    interval between consecutive $\\pi$ values. This width is multiplied\n    by the average height of the likelihood function at the two\n    endpoints to approximate the area under the curve.\n\n4.  **Example**:\n\n    ``` r\n    pi_values <- seq(0, 1, length.out = 5)\n    # pi_values = [0, 0.25, 0.5, 0.75, 1]\n    diff(pi_values)  # Returns [0.25, 0.25, 0.25, 0.25]\n    ```\n\n**Summary** - The trapezoidal rule is particularly useful in our case\nbecause it provides a good approximation of the integral of the\nlikelihood function, which we need to normalize it to a probability\ndensity. - The `diff()` function is essential for numerical integration\nbecause it provides the width of each interval in the trapezoidal rule,\nwhich is used to approximate the area under the curve.\n\nTo see the differences the following code uses a much coarser grid (20\npoints instead of 1000) to make the difference between the two methods\nclearly visible. The rectangle rule uses bars, while the trapezoidal\nrule connects consecutive points with straight lines to form trapezoids.\n\n\n::: {.cell}\n\n```{#lst-003-integral-difference .r .cell-code  lst-cap=\"Rectangle rule vs trapezoidal integration rule\"}\n# Create a coarser grid to make the difference visible\npi_values <- base::seq(0, 1, length.out = 20)\n\n# Calculate binomial likelihood for y=30, n=50\nlikelihood <- stats::dbinom(30, size = 50, prob = pi_values)\n\n# Rectangle rule (left endpoint)\ndx <- base::diff(pi_values)[1]\nintegral_rectangle <- base::sum(likelihood[-base::length(likelihood)] * dx)\n\n# Trapezoidal rule\nintegral_trapezoid <- base::sum((likelihood[-1] + likelihood[-base::length(likelihood)]) / 2 * dx)\n\n# Create visualization data\nrect_df <- base::data.frame(\n  x = pi_values[-base::length(pi_values)],\n  xend = pi_values[-1],\n  y = 0,\n  yend = likelihood[-base::length(likelihood)]\n)\n\ntrap_df <- base::data.frame(\n  x = pi_values,\n  y = likelihood\n)\n\n# Visualize both methods\nggplot2::ggplot() +\n  # Rectangle rule\n  ggplot2::geom_rect(data = rect_df, \n            ggplot2::aes(xmin = x, xmax = xend, ymin = y, ymax = yend),\n            fill = \"blue\", alpha = 0.3, color = \"blue\") +\n  # Trapezoidal rule\n  ggplot2::geom_polygon(data = base::data.frame(\n    x = base::c(pi_values, base::rev(pi_values)),\n    y = base::c(likelihood, rep(0, base::length(likelihood)))\n      ), \n      ggplot2::aes(x = x, y = y), fill = \"red\", alpha = 0.3) +\n  # Actual function\n  ggplot2::geom_line(data = trap_df, ggplot2::aes(x = x, y = y), \n            color = \"black\", linewidth = 1) +\n  ggplot2::geom_point(data = trap_df, ggplot2::aes(x = x, y = y), \n             color = \"black\", size = 2) +\n  ggplot2::labs(x = \"π\", y = \"Likelihood\",\n       title = \"Rectangle Rule (blue) vs Trapezoidal Rule (red)\") +\n  ggplot2::theme_minimal()\n\n# Print numerical comparison\nbase::cat(\"Rectangle rule integral:\", integral_rectangle, \"\\n\")\nbase::cat(\"Trapezoidal rule integral:\", integral_trapezoid, \"\\n\")\nbase::cat(\"Difference:\", abs(integral_rectangle - integral_trapezoid), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rectangle rule integral: 0.01960784 \n#> Trapezoidal rule integral: 0.01960784 \n#> Difference: 3.469447e-18\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Compare the rectangle rule (bars) with the trapezoidal integration rule](003-beta-binomial_files/figure-html/fig-003-integral-difference-1.png){#fig-003-integral-difference width=672}\n:::\n:::\n\n\nIn fact the numerical difference between these two methods is extremely\nsmall: The difference is in the 18th decimal place!\n:::\n\nThese pieces of the puzzle are shown together in\n@fig-003-prior-scaled-likelihood where, *only for the purposes of visual\ncomparison to the prior*, the likelihood function is scaled to integrate\nto 1. (The scaled likelihood function is calculated by\n$L(\\pi \\mid  y)/ \\int_{0}^{1} L(\\pi \\mid y)d\\pi$.) The prior and data,\nas captured by the likelihood, don’t completely agree. Constructed from\nold polls, the prior is a bit more pessimistic about Michelle’s election\nsupport than the data obtained from the latest poll. Yet both insights\nare valuable to our analysis. Just as much as we shouldn’t ignore the\nnew poll in favor of the old, we also shouldn’t throw out our bank of\nprior information in favor of the newest thing (also great life advice).\nThinking like Bayesians, we can construct a\n<a class='glossary' title='The posterior model represents the updated beliefs about the parameters of a model after incorporating observed data. It is derived by combining the prior distribution, which reflects initial beliefs about the parameter, with the likelihood of the observed data using Bayes’ theorem.'>posterior model</a> of $\\pi$ which\ncombines the information from the prior with that from the data.\n\n:::::: my-assessment\n:::: my-assessment-header\n::: {#cor-003-quiz-combine-posterior}\n: Which plot reflects the correct posterior model of Michelle’s election\nsupport $\\pi$?\n:::\n::::\n\n::: my-assessment-container\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::include_graphics(\"img/003-quiz-combine-posterior.png\")\n```\n\n::: {.cell-output-display}\n![](img/003-quiz-combine-posterior.png){width=100%}\n:::\n:::\n\n\n<center>**Which plot reflects the correct posterior model of Michelle’s\nelection support** $\\pi$?: <select class='webex-select'><option value='blank'></option><option value=''>a</option><option value='answer'>b</option><option value=''>c</option></select></center>\n\n<br />\n:::\n::::::\n\nThe plot of the above solution is the only plot in which the posterior\nmodel of $\\pi$ strikes a balance between the relative pessimism of the\nprior and optimism of the data. You can reproduce this correct posterior\nusing the `bayesrules::plot_beta_binomial()` function, plugging in the\nprior <a class='glossary' title='In the Bayesian framework, a hyperparameter is a parameter of a prior distribution.  It defines the shape, location, or scale of the prior belief about a model parameter before observing any data. Unlike model parameters, which are estimated from the data (e.g., the mean or variance of a distribution), hyperparameters are set before the analysis begins and are not directly learned from the data. (Brave-AI)'>hyperparameters</a>\n($\\alpha = 45, \\beta = 55$) and data ($y = 30$ of $n = 50$ polled voters\nsupport Michelle):\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-plot-beta-binomial-michelle}\n: Plot Beta-Binomial for Michelle support $\\pi$\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-plot-beta-binomial-michelle .r .cell-code  lst-cap=\"Plot beta-binomial for Michelle support\"}\nbayesrules::plot_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50) \n```\n\n::: {.cell-output-display}\n![The prior pdf, scaled likelihood function, and posterior pdf of Michelle’s election support $\\pi$](003-beta-binomial_files/figure-html/fig-003-plot-beta-binomial-michelle-1.png){#fig-003-plot-beta-binomial-michelle width=672}\n:::\n:::\n\n:::\n::::::\n\nIn its balancing act, the posterior here is slightly “closer” to the\nprior than to the likelihood. (We’ll gain intuition for why this is the\ncase in @sec-chap-004.) The posterior being *centered* at $\\pi = 0.5$\nsuggests that Michelle’s support is equally likely to be above or below\nthe 50% threshold required to win Minnesota. Further, combining\ninformation from the prior and data, the *range* of posterior plausible\nvalues has narrowed: we can be fairly certain that Michelle’s support is\nsomewhere between 35% and 65%. You might also recognize something new:\nlike the prior, the posterior model of $\\pi$ is continuous and lives on\n\\[0,1\\]. That is, like the prior, the posterior appears to be a\n$\\text{Beta}(\\alpha, \\beta)$ model where the shape parameters have been\n*updated* to combine information from the prior and data. This is indeed\nthe case. Conditioned on the observed poll results ($Y = 30$), the\nposterior model of Michelle’s election support is $\\text{}(75, 75)$:\n\n$$\\pi \\mid (Y = 30) \\sim \\text{Beta}(75, 75)$$ with a corresponding pdf\nwhich follows from @eq-003-beta-model\n\n$$\nf (\\pi \\mid y) =  \\frac{\\Gamma(150)}{\\Gamma(75)\\Gamma(75)} \\pi^{74}(1- \\pi)^{74} \\text{ for } \\pi \\in [0, 1]\n$$ {#eq-posterior-model-michelle}\n\nBefore backing up this claim with some math, let’s examine the evolution\nin your understanding of Michelle’s election support $\\pi$. The\n`bayesrules::summarize_beta_binomial()` function summarizes the typical\nvalues and variability in the prior and posterior models of $\\pi$. These\ncalculations follow directly from applying the prior and posterior Beta\nparameters into @eq-003-beta-mean-mode and @eq-003-beta-variance:\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-summarize-beta-binomial-michelle}\n: Typical values and variability in the prior and posterior models of\n$\\pi$ support for Michelle\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-003-summarize-beta-binomial-michelle .cell tbl-cap='Summarizing the typical values and variability in the prior and posterior models of $\\pi$ support for Michelle'}\n\n```{#lst-003-summarize-beta-binomial-michelle .r .cell-code  lst-cap=\"Typical values and variability in the prior and posterior models of Michelle's support\"}\nbayesrules::summarize_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50) |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta| mean|      mode|       var|        sd|\n|:---------|-----:|----:|----:|---------:|---------:|---------:|\n|prior     |    45|   55| 0.45| 0.4489796| 0.0024505| 0.0495025|\n|posterior |    75|   75| 0.50| 0.5000000| 0.0016556| 0.0406894|\n\n\n:::\n:::\n\n:::\n::::::\n\nA comparison illuminates the polling data’s influence on the posterior\nmodel. Mainly, after observing the poll in which 30 of 50 people\nsupported Michelle, the expected value of her underlying support $\\pi$\nnudged up from approximately 45% to 50%. On the other hand, the\nvariability within the model decreased, indicating a narrower range of\nposterior plausible $\\pi$ values in light of the polling data:\n\n::::::: {#nte-003-calculate-prior-mean-sd-beta-michelle .callout-note}\n###### Prior mean E ($\\mu$) and standard deviation SD ($\\sigma$) for Beta(45, 55)\n\nFor the calculation to compare prior with posterior values I am going to\nuse for the prior part the already explained formula for the prior mean\nand stadard deviation of the beta model $\\text{Beta}(45, 55)$ in\n@eq-003-beta-mean-mode resp. @eq-003-beta-variance.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-003-calculate-prior-mean-sd-beta-michelle}\n: Calculate prior mean and standard deviation for the Beta model\n$\\text{Beta}(45, 55)$ for Michelle support $\\pi$\n:::\n::::\n\n::: my-theorem-container\n**Prior Mean = E of Beta(45, 55)**\n\n$$\n\\begin{align*}\nE(\\pi) = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{45}{45+55} = \\frac{45}{100} = \\bf{0.45}\n\\end{align*}\n$$ {#eq-003-calculate-mean-prior-beta-michelle}\n\n**Prior Standard Deviation = SD of Beta(45, 55)\\$**\n\n$$\n\\begin{align*}\nSD(\\pi) &= \\sqrt{\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 \\cdot (α + β + 1)}} \n=  \\sqrt{\\frac{45 \\cdot 55}{(45 + 55)^2 \\cdot (45 + 55 + 1)}} \\\\\n&= \\sqrt{\\frac{2475}{(100)^2 \\cdot (101)}} \n= \\sqrt{\\frac{2475}{1010000}} \n= \\sqrt(0.002450495) \\\\\n&= \\bf{0.04950247} \\\\\n\\end{align*}\n$$ {#eq-003-calculate-standard-deviation-prior-beta-michelle}\n:::\n::::::\n:::::::\n\n::::::::: {#nte-003-calculate-posterior-mean-sd-beta-michelle .callout-note}\n###### Posterior mean E ($\\mu$) and standard deviation SD ($\\sigma$) for Beta(75, 75) = Beta-Binomial Beta(45, 55), y = 30, n = 50\n\nThere are two possible calculations for the posterior mean and standard\ndeviation:\n\n1.  Using again the formulae from @eq-003-beta-mean-mode resp.\n    @eq-003-beta-variance, but this time updated to combine information\n    from the prior and data resulting in the beta model\n    $\\text{Beta}(75, 75)$.\n2.  Using the integrated formulae\n    @eq-003-beta-binomial-mean-mode-variance for the Beta-Binomial model\n    which are specified later in @sec-003-beta-binomial-model.\n\n:::::::: my-theorem\n:::: my-theorem-header\n::: {#thm-003-calculate-posterior-mean-sd-beta-michelle}\n: Calculate posterior mean and standard deviation for the Beta model\n$\\text{Beta}(75, 75)$ = Beta-Binomial Beta(45, 55), y = 30, n = 50 for\nMichelle support $\\pi$\n:::\n::::\n\n::::: my-theorem-container\n::: {#big-text style=\"font-size: 150%\"}\n**Posterior Mean**\n:::\n\n**Posterior Mean = E of `Beta(75, 75)`: Version 1**\n\nThere are two steps:\n\n1.  At first I have to calculate the Beta-Binomial model using\n    @eq-003-beta-binomial-formula (presented in the next section) to get\n    the value for the posterior Beta.\n\n$$\n\\begin{align*}\n\\pi \\mid (Y = y) &\\sim \\text{Beta}(\\alpha + y, \\beta + n − y) \\\\\n&\\sim \\text{Beta}(45 + 30, 55 + 50 − 30) \\\\\n&\\sim \\bf{\\text{Beta}(75, 75)}\n\\end{align*}\n$$ {#eq-calculate-posterior-beta-michelle}\n\n2.  Then I can calculate with the posterior Beta the measures for the\n    central tendency.\n\n$$\n\\begin{align*}\nE(\\pi) = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{75}{75+75} = \\frac{75}{150} = \\bf{0.5}\n\\end{align*}\n$$ {#eq-003-calculate-mean-posterior-beta-michelle-1}\n\n**Posterior Mean = E of Beta-Binomial `Beta(45, 55), y = 30, n = 50`:\nVersion 2**\n\n$$\n\\begin{align*}\nE(\\pi) = \\frac{\\alpha + y}{\\alpha + \\beta + n} &= \\frac{45 + 30}{45 + 55 + 50} = \\frac{75}{150} = \\bf{0.5}\n\\end{align*}\n$$ {#eq-003-calculate-mean-posterior-beta-michelle-2}\n\n::: {style=\"font-size: 150%\"}\n**Posterior Standard Deviation**\n:::\n\n**Posterior Standard Deviation = SD of `Beta(75, 75)`: Version 1**\n\n$$\n\\begin{align*}\nSD(\\pi) &= \\sqrt{\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 \\cdot (α + β + 1)}} \n=  \\sqrt{\\frac{75 \\cdot 75}{(75 + 75)^2 \\cdot (75 + 75 + 1)}} \\\\\n&= \\sqrt{\\frac{5625}{(150)^2 \\cdot (151)}} \n= \\sqrt{\\frac{5625}{22500 \\cdot 151}}\n= \\sqrt{\\frac{5625}{3397500}} \\\\\n&= \\sqrt{0.001655629} = \\bf{0.04068942} \\\\\n\\end{align*}\n$$ {#eq-003-calculate-standard-deviation-posterior-beta-michelle-1}\n\n**Posterior Standard Deviation = SD of `Beta(45, 55), y = 30, n = 50`:\nVersion 2**\n\n$$\n\\begin{align*}\nSD(\\pi \\mid Y = y) &=  \\sqrt{\\frac{(\\alpha + y)(\\beta + n − y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n + 1)}} \\\\\n&= \\sqrt{\\frac{(45 + 30)(55 + 50 − 30)}{(45 + 55 + 50)^2(45 + 55 + 50 + 1)}} \\\\\n&= \\sqrt{\\frac{(75)(75)}{(150)^2(151)}} = \\sqrt{\\frac{5625}{22500 \\cdot 151}} \\\\\n&= \\sqrt{\\frac{5625}{3397500}} = \\sqrt{0.001655629} \\\\\n&= \\bf{0.04068942}\n\\end{align*}\n$$ {#eq-003-calculate-standard-deviation-posterior-beta-michelle-2}\n:::::\n::::::::\n\n<center>**Compare these manually calculated values with the computed\nresults in @tbl-003-summarize-beta-binomial-michelle using\n`bayesrules::summarize_beta_binomial()`.**</center>\n:::::::::\n\nIf you’re happy taking our word that the posterior model of $\\pi$ is\n`Beta(75,75)`, you can skip to @sec-003-beta-binomial-model and still be\nprepared for the next material in the book. However, we strongly\nrecommend that you consider the magic from which the posterior is built.\nGoing through the process can help you further develop intuition for\nBayesian modeling.\n\nAs with our previous Bayesian models, the posterior conditional pdf of\n$\\pi$ strikes a balance between the prior pdf $f (\\pi)$ and the\nlikelihood function $L(\\pi \\mid y = 30)$ via Bayes’ Rule\n@eq-002-posterior-pmf-for-variable:\n\n$$\nf (\\pi \\mid y = 30) =  \\frac{f (\\pi)L(\\pi \\mid y = 30)}{f (y = 30)}\n$$\n\nRecall from @sec-002-posterior-shortcut that $f (y = 30)$ is a\n**normalizing constant**, i.e., a constant across $\\pi$ which scales the\nposterior pdf $f (\\pi \\mid y = 30)$ to integrate to 1. **We don’t need\nto calculate the normalizing constant in order to construct the\nposterior model**. Rather, we can simplify the posterior construction by\nutilizing the fact that the posterior pdf is proportional to the product\nof the prior pdf (@eq-003-prior-pdf-45-55) and likelihood function\n(@eq-003-binomial-likelihood-function-michelle):\n\n$$\n\\begin{align*}\nf (\\pi \\mid y = 30) \\propto f (\\pi)L(\\pi \\mid y = 30)\\\\  \n&=  \\frac{\\Gamma(100)}{\\Gamma(45)\\Gamma(55)}  \\pi^{44}(1 − π)^{54} \\cdot \\binom{50}{30} \\pi^{30}(1 − \\pi)^{20}  \\\\\n&=\\left[\\frac{\\Gamma(100)}{\\Gamma(45)\\Gamma(55)}\\right] \\cdot \\pi^{74}(1 − π)^{74}  \\\\\n&\\propto \\pi^{74}(1 − \\pi)^{74}\n\\end{align*}\n$$\n\nIn the third line of our calculation, we combined the constants and the\nelements that depend upon $\\pi$ into two different pieces. In the final\nline, we made a big simplification: we dropped all constants that don’t\ndepend upon $\\pi$. We don’t need these. Rather, it’s the dependence of f\n($\\pi$ \\mid y = 30) on π that we care about:\n\n$$f ( \\pi \\mid y = 30) = c \\pi^{74}(1 − \\pi)^{74} \\propto \\pi^{74}(1 − \\pi)^{74}$$\n\nWe *could* complete the definition of this posterior pdf by calculating\nthe normalizing constant $c$ for which the pdf integrates to 1:\n\n$$\n1 = \\int f (\\pi \\mid y = 30)d\\pi = \\int c \\cdot \\pi^{74}(1 − \\pi)^{74}d\\pi ⇒ c =  \\frac{1}{\\int \\pi^{74}(1 − \\pi)^{74}d\\pi}\n$$\n\nBut again, we don’t need to do this calculation. The pdf of $\\pi$ is\ndefined by its structural dependence on $\\pi$, that is, the\n<a class='glossary' title='The basic idea of a kernel densitiy estimate is to have a set of data to predict future results. The kernel idea is to create a similarity function (called a kernel function) between any two sets of inputs. For any new set of inputs, we predict by taking a weighted average of past results, weighted by the similarity of the past inputs to the current inputs. So in a essence a kernel is a function used to measure similarity between data points in a transformed feature space. Intuitively, you can think of a kernel as a way to compute the “distance” or “similarity” between points without explicitly mapping them into that higher-dimensional space.'>kernel</a> of the pdf. Notice here that\n$f (\\pi \\mid y = 30)$ has the same kernel as the normalized\n$\\text{Beta}(75,75)$ pdf in @eq-posterior-model-michelle:\n\n$$\nf (\\pi \\mid y = 30) =  \\frac{\\Gamma(150)}{\\Gamma(75)\\Gamma(75)}  \\pi^{74}(1 − \\pi)^{74} \\propto \\pi^{74}(1 − \\pi)^{74}\n$$\n\nThe fact that the unnormalized posterior pdf $f (\\pi \\mid y = 30)$\nmatches an unnormalized $\\text{Beta}(75,75)$ pdf verifies our claim that\n$\\pi \\mid (Y = 30) \\sim \\text{Beta}(75, 75)$. Magic. For extra practice\nin identifying the posterior model of π from an unnormalized posterior\npdf or kernel, take the following quiz:\n\n:::::: my-assessment\n:::: my-assessment-header\n::: {#cor-003-quiz-identify-posterior-model}\n: Identify the posterior model of $\\pi$ from an unnormalized posterior\npdf or kernel\n:::\n::::\n\n::: my-assessment-container\nFor each scenario below, identify the correct Beta posterior model of\n$\\pi\\in [0, 1]$ from its unnormalized pdf:\n\na.  f $(\\pi \\mid y) \\propto \\pi^{3−1}(1 − \\pi)^{12−1}$\n    <select class='webex-select'><option value='blank'></option><option value=''>Beta(12,3)</option><option value='answer'>Beta(3,12)</option><option value=''>Beta(2,11)</option><option value=''>Beta(11,2)</option></select>\nb.  $f (\\pi \\mid y) \\propto \\pi^{11}(1 − \\pi)^{2}$\n    <select class='webex-select'><option value='blank'></option><option value='answer'>Beta(12,3)</option><option value=''>Beta(3,12)</option><option value=''>Beta(2,11)</option><option value=''>Beta(11,2)</option></select>\nc.  $f (\\pi \\mid y) \\propto 1$\n    <select class='webex-select'><option value='blank'></option><option value='answer'>Beta(0,0)</option><option value=''>Beta(0,1)</option><option value=''>Beta(1,0)</option><option value='answer'>Beta(1,1)</option></select>\n:::\n::::::\n\nNow, instead of identifying a model from a kernel, practice identifying\nthe kernels of models.\n\n:::::: my-assessment\n:::: my-assessment-header\n::: {#cor-003-quiz-indentify-model-kernels}\n: Identify the kernels of models\n:::\n::::\n\n::: my-assessment-container\nIdentify the kernels of each pdf below.\n\n1.  $f (\\pi \\mid y) = ye^{−\\pi y} \\text{ for } \\pi > 0$\n    <select class='webex-select'><option value='blank'></option><option value=''>a</option><option value=''>b</option><option value=''>c</option><option value='answer'>d</option></select>\n\n<!-- -->\n\na.  y\nb.  $e^{−\\pi}$\nc.  $ye^{−\\pi}$\nd.  $e^{-\\pi y}$\n\n<!-- -->\n\n2.  $f (\\pi \\mid y) = \\frac{2y}{ (y−1)!} \\pi^{y−1}e^{−2\\pi} \\text{ for } \\pi > 0$\n    <select class='webex-select'><option value='blank'></option><option value='answer'>a</option><option value=''>b</option><option value=''>c</option><option value=''>d</option></select>\n\n<!-- -->\n\na.  y\nb.  $e^{−\\pi}$\nc.  \\$ye\\^{−\\pi}\nd.  e\\^{−\\pi y}\n\n<!-- -->\n\n3.  $f (\\pi) = 3\\pi^2 \\text{ for } \\pi \\in [0, 1]$\n    <select class='webex-select'><option value='blank'></option><option value=''>a</option><option value='answer'>b</option><option value=''>c</option><option value=''>d</option></select>\n\n<!-- -->\n\na.  $\\pi$\nb.  $\\pi^2$\nc.  $3\\pi^2$\nd.  3\n:::\n::::::\n\n## Beta-Binomial Model {#sec-003-beta-binomial-model}\n\nIn the previous section we developed the fundamental Beta-Binomial model\nfor Michelle’s election support $\\pi$. In doing so, we assumed a\nspecific $\\text{Beta}(45,55)$ prior and a specific polling result (Y =\n30 of n = 50 polled voters supported your candidate) within a specific\ncontext. This was a special case of the more general\n<a class='glossary' title='The Beta-Binomial Bayesian Model is a fundamental framework in Bayesian statistics used to estimate a proportion π (e.g., success probability) when data are binary outcomes (success/failure) across n independent trials. The Beta-Binomial Bayesian Model combines a Beta prior with a Binomial likelihood, resulting in a Beta posterior due to conjugacy.  This makes it ideal for estimating unknown proportions (e.g., success rates) from binary data.'>Beta-Binomial model</a>:\n\n$$\n\\begin{align*}\nY \\mid \\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta)\n\\end{align*}\n$$ {#eq-003-beta-binomial-model}\n\nThis general model has vast applications, applying to *any* setting\nhaving a parameter of interest $\\pi$ that lives on $[0,1]$ with *any*\ntuning of a $\\text{Beta}$ prior and *any* data $Y$ which is the number\nof “successes” in $n$ fixed, independent trials, each having probability\nof success $\\pi$.\n\nFor example, $\\pi$ might be a coin’s tendency toward Heads and data $Y$\nrecords the number of Heads observed in a series of $n$ coin flips. Or\n$\\pi$ might be the proportion of adults that use social media and we\nlearn about $\\pi$ by sampling $n$ adults and recording the number $Y$\nthat use social media.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-003-calculation-beta-binomial-model}\n: Calculation of the Beta-Binomial model / the posterior Beta model\n:::\n::::\n\n::: my-theorem-container\nNo matter the setting, upon observing $Y = y$ successes in $n$ trials,\nthe posterior of $\\pi$ can be described by a $\\text{Beta}$ model which\nreveals the influence of the prior (through $\\alpha$ and $beta$) and\ndata (through $y$ and $n$):\n\n$$\n\\begin{align*}\n\\pi \\mid (Y = y) \\sim \\text{Beta}(\\alpha + y, \\beta + n − y)\n\\end{align*}\n$$ {#eq-003-beta-binomial-formula}\n\nMeasures of posterior central tendency and variability follow from\n@eq-003-beta-mean-mode and @eq-003-beta-variance:\n\n$$\n\\begin{align*}\nE(\\pi \\mid Y = y) &=  \\frac{\\alpha + y}{\\alpha + \\beta + n}  \\\\\nMode(\\pi \\mid Y = y) &=  \\frac{\\alpha + y − 1}{\\alpha + \\beta + n − 2}  \\\\\nVar(\\pi \\mid Y = y) &=  \\frac{(\\alpha + y)(\\beta + n − y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n + 1)}\n\\end{align*}\n$$ {#eq-003-beta-binomial-mean-mode-variance}\n:::\n::::::\n\nImportantly, notice that the posterior follows a different\nparameterization of the same probability model as the prior – both the\nprior and posterior are Beta models with different tunings. In this\ncase, we say that the $\\text{Beta}(\\alpha, \\beta)$ model is a\n<a class='glossary' title='If the [posterior distribution] is in the same probability distribution family as the prior probability distribution the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. A conjugate prior is an algebraic convenience; otherwise, numerical integration may be necessary. (Wikipedia)'>conjugate prior</a> for the corresponding\n$\\text{Bin}(n, \\pi)$ data model. Our work below will highlight that\nconjugacy simplifies the construction of the posterior, and thus can be\na desirable property in Bayesian modeling.\n\n:::::{.my-experiment}\n:::{.my-experiment-header}\n:::::: {#def-003-conjugate-prior}\n: Conjugate Prior\n::::::\n:::\n::::{.my-experiment-container}\nWe say that $f (\\pi)$ is a conjugate prior for $L(\\pi \\mid y)$ if the\nposterior, $f (\\pi \\mid y) \\propto f (\\pi)L(\\pi \\mid y)$, is from the\nsame model family as the prior.\n::::\n:::::\n\n\nThe posterior construction for the general Beta-Binomial model is very\nsimilar to that of the election-specific model. First, the Beta prior\npdf $f (\\pi)$ is defined by (3.1) and the likelihood function\n$L(\\pi \\mid y)$ by (2.7), the conditional pmf of the\n$\\text{Bin}(n, \\pi)$ model upon observing data $Y = y$. For\n$\\pi \\in [0, 1]$,\n\n$$\nf (π) =  \\frac{Γ(α + β)}{Γ(α)Γ(β)}\\pi^{\\alpha−1}(1 − \\pi)^{\\beta−1} \\text{ and } L(\\pi \\mid y) = \\binom{n}{y} \\pi^y(1 − \\pi)^{n−y}\n$$ {eq-003-beta-binomial-posterior-construction}\n\nPutting these two pieces together, the posterior pdf follows from Bayes’\nRule:\n\n$$\n\\begin{align*}\nf (\\pi \\mid y) &\\propto f (\\pi)L(\\pi \\mid y) \\\\\n&=  \\frac{\\Gamma(α + β)}{\\Gamma(α)Γ(β)} \\pi^{\\alpha−1}(1 − π)^{β−1} ⋅ \\binom{n}{y} \\pi^y(1 − \\pi)^{n−y} \\\\\n&\\propto \\pi^{(\\alpha+y)−1}(1 − \\pi)^{(\\beta+n−y)−1}\n\\end{align*}\n$$\n\nAgain, we’ve dropped normalizing constants which don’t depend upon $\\pi$\nand are left with the *unnormalized* posterior pdf. Note that this\nshares the same structure as the normalized\n$\\text{Beta}(\\alpha + y, \\beta + n − y)$ pdf,\n\n$$\nf (\\pi \\mid y) =  \\frac{\\Gamma(\\alpha + \\beta + n)}{\\Gamma(\\alpha + y)\\Gamma(\\beta + n − y)}  \\pi^{(\\alpha+y)−1}(1 − \\pi)^{(\\beta+n−y)−1}\n$$\n\n## Simulating the Beta-Binomial\n\nUsing @sec-002-posterior-simulation-fake-news as a guide, let’s simulate\nthe posterior model of Michelle’s support $\\pi$.\n\n(1) We begin by simulating 10,000 values of $\\pi$ from the\n    $\\text{Beta}(45,55)$ prior using `stats::rbeta()` and, subsequently,\n    a potential $\\text{Bin}(50,\\pi)$ poll result $Y$ from each $\\pi$\n    using `stats::rbinom()`.\n(2) The resulting 10,000 pairs of $\\pi$ and $y$ values are shown in\n    @fig-003-simulate-beta-binomial-scatterplot-michelle. In general,\n    the greater Michelle’s support, the better her poll results tend to\n    be. Further, the highlighted pairs illustrate that the eventual\n    observed poll result, $Y = 30$ of $50$ polled voters supported\n    Michelle, would most likely arise if her underlying support $\\pi$\n    were somewhere in the range from $0.4$ to $0.6$.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-simulate-beta-binomial-scatterplot-michelle}\n: Scatterplot of 10,000 simulated pairs of the Beta-Binomial\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-simulate-beta-binomial-scatterplot-michelle .r .cell-code  lst-cap=\"Scatterplot of 10,000 simulated pairs of the Beta-Binomial\"}\nbase::set.seed(84735)\nmichelle_sim <- tibble::tibble(pi = stats::rbeta(10000, 45, 55)) |> \n  dplyr::mutate(y = stats::rbinom(10000, size = 50, prob = pi))\n  \nggplot2::ggplot(michelle_sim, ggplot2::aes(x = pi, y = y)) + \n  ggplot2::geom_point(ggplot2::aes(color = (y == 30)), size = 0.1)\n```\n\n::: {.cell-output-display}\n![Scatterplot of 10,000 simulated pairs of Michelle’s support $\\pi$ and polling outcome $y$](003-beta-binomial_files/figure-html/fig-003-simulate-beta-binomial-scatterplot-michelle-1.png){#fig-003-simulate-beta-binomial-scatterplot-michelle width=672}\n:::\n:::\n\n:::\n::::::\n\nWhen we zoom in closer on just those pairs that match our $Y = 30$ poll\nresults, the behavior across the remaining set of $\\pi$ values well\napproximates the $\\text{Beta}(75,75)$ posterior model of $\\pi$:\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-simulate-beta-binomial-density-michelle}\n: Density plot of simulated $\\pi$ values\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-simulate-beta-binomial-density-michelle .r .cell-code  lst-cap=\"Density plot of simulated $\\pi$ values\"}\n# Keep only the simulated pairs that match our data\nmichelle_posterior <- michelle_sim  |>  \n  dplyr::filter(y == 30)\n  \n# Plot the remaining pi values\nggplot2::ggplot(michelle_posterior, ggplot2::aes(x = pi)) + \n  ggplot2::geom_density()\n```\n\n::: {.cell-output-display}\n![A density plot of simulated $\\pi$ values that produced polling outcomes in which $Y = 30$ voters supported Michelle.](003-beta-binomial_files/figure-html/fig-003-simulate-beta-binomial-density-michelle-1.png){#fig-003-simulate-beta-binomial-density-michelle width=672}\n:::\n:::\n\n:::\n::::::\n\nAs such, we can also use our simulated sample to *approximate* posterior\nfeatures, such as the mean and standard deviation in Michelle’s support.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-summarize-posterior-features-michelle}\n: Posterior mean and standard deviation in Michelle’s support with\n10.000 simulated pairs\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-003-summarize-posterior-features-michelle .cell tbl-cap='Posterior mean and standard deviation in Michelle’s support with 10.000 simulated pairs'}\n\n```{#lst-003-summarize-posterior-features-michelle .r .cell-code  lst-cap=\"Posterior mean and standard deviation with 10.000 simulated pairs\"}\nmichelle_posterior |> \n  dplyr::summarize(Mean = base::mean(pi), SD = stats::sd(pi)) |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|      Mean|       SD|\n|---------:|--------:|\n| 0.5054909| 0.037321|\n\n\n:::\n:::\n\n:::\n::::::\n\nAs such, we can also use our simulated sample to approximate posterior\nfeatures, such as the mean and standard deviation in Michelle’s support.\nThe results are quite similar to the theoretical values calculated\nabove, $E(\\pi \\mid Y = 30) = 0.5$ and $SD(\\pi \\mid Y = 30) = 0.0407$.\n\nIn interpreting these simulation results, “approximate” is a key word.\nSince only `base::nrow(michelle_posterior)` =\n211 of our 10,000 simulations matched our\nobserved $Y = 30$ data, this approximation might be improved by upping\nour original simulations from 10,000 to, say, 50,000:\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-compare-simulate-10000-50000-beta-binomial-michelle}\n: Compare posterior mean and standard deviation in Michelle’s support\nwith 10,000 and 50,000 simulated pairs\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-003-simulate-beta-binomial-50000-michelle .cell tbl-cap='Compare approximation of posterior mean and standard deviation between 10,000 and 50,000 simulated pairs'}\n\n```{#lst-003-simulate-beta-binomial-50000-michelle .r .cell-code  lst-cap=\"Compare approximation of posterior mean and standard deviation with different numbers of simulated pairs\"}\n## Simulate 50,000 pairs\nbase::set.seed(84735)\nmichelle_sim2 <- tibble::tibble(pi = stats::rbeta(50000, 45, 55)) |> \n  dplyr::mutate(y = stats::rbinom(50000, size = 50, prob = pi))\n  \n# Keep only the simulated pairs that match our data\nmichelle_posterior2 <- michelle_sim2  |>  \n  dplyr::filter(y == 30) \n\n# Create table to compare the approximation of different simulations\ncompare_michelle_simuation <- tibble::tibble(\n  n = c(\"Theory\", \"10,000\", \"50,000\"),\n  Mean = c(0.5, \n        michelle_posterior |> dplyr::summarize(base::mean(pi)), \n        michelle_posterior2 |> dplyr::summarize(base::mean(pi))\n        ),\n  Mead_Diff = c(0.0000,\n                0.5 - michelle_posterior |> dplyr::summarize(base::mean(pi)),\n                0.5 - michelle_posterior2 |> dplyr::summarize(base::mean(pi))\n                ),\n  SD = c(0.04068942,\n        michelle_posterior |> dplyr::summarize(stats::sd(pi)), \n        michelle_posterior2 |> dplyr::summarize(stats::sd(pi))),\n  SD_Diff = c(0.0000,\n                0.04068942 - michelle_posterior |> dplyr::summarize(stats::sd(pi)),\n                0.04068942 - michelle_posterior2 |> dplyr::summarize(stats::sd(pi))\n                )\n  ) \n\ncompare_michelle_simuation |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|n      |Mean      |Mead_Diff    |SD         |SD_Diff      |\n|:------|:---------|:------------|:----------|:------------|\n|Theory |0.5       |0            |0.04068942 |0            |\n|10,000 |0.5054909 |-0.005490905 |0.03732097 |0.003368448  |\n|50,000 |0.4987574 |0.001242608  |0.03998315 |0.0007062704 |\n\n\n:::\n:::\n\n:::\n::::::\n\nInstead of `base::nrow(michelle_posterior)` =\n**211** of our 10,000 simulations we have\nnow **995** rows that match our observed\ndata $Y = 30$ with the result that the approximation has improved quite\nextensively.\n\n## Example: Milgram’s behavioral study of obedience {#sec-003-milgram-experiment}\n\nIn a 1963 issue of The *Journal of Abnormal and Social Psychology*,\nStanley Milgram described a study in which he investigated the\npropensity of people to obey orders from authority figures, even when\nthose orders may harm other people [@milgram-1963]. In the paper,\nMilgram describes the study as:\n\n> consist\\[ing\\] of ordering a naive subject to administer electric\n> shock to a victim. A simulated shock generator is used, with 30\n> clearly marked voltage levels that range from IS to 450 volts. The\n> instrument bears verbal designations that range from *Slight Shock* to\n> *Danger: Severe Shock*. The responses of the victim, who is a trained\n> confederate of the experimenter, are standardized. The orders to\n> administer shocks are given to the naive subject in the context of a\n> ‘learning experiment’ ostensibly set up to study the effects of\n> punishment on memory. As the experiment proceeds the naive subject is\n> commanded to administer increasingly more intense shocks to the\n> victim, even to the point of reaching the level marked *Danger: Severe\n> Shock*.\n\nIn other words, study participants were given the task of testing\nanother participant (who was in truth a trained actor) on their ability\nto memorize facts. If the actor *didn’t* remember a fact, the\nparticipant was ordered to administer a shock on the actor and to\nincrease the shock level with every subsequent failure. Unbeknownst to\nthe participant, the shocks were fake and the actor was only\n*pretending* to register pain from the shock.\n\n### A Bayesian analysis\n\nWe can translate Milgram’s study into the Beta-Binomial framework. The\nparameter of interest here is $\\pi$, the chance that a person would obey\nauthority (in this case, administering the most severe shock), even if\nit meant bringing harm to others. Since Milgram passed away in 1984, we\ndon’t have the opportunity to ask him about his understanding of $\\pi$\nprior to conducting the study. Thus, we’ll diverge from the actual study\nhere, and suppose that another psychologist helped carry out this work.\nPrior to collecting data, they indicated that a $\\text{Beta}(1,10)$\nmodel accurately reflected their understanding about $\\pi$, developed\nthrough previous work.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-milgram-beta-prior}\n: A $\\text{Beta}(1,10)$ prior model of $\\pi$\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n# Beta(1,10) prior \nbayesrules::plot_beta(alpha = 1, beta = 10)\n```\n\n::: {.cell-output-display}\n![Assuming a $\\text{Beta}(1,10)$ prior model of $\\pi$](003-beta-binomial_files/figure-html/fig-003-milgram-beta-prior-1.png){#fig-003-milgram-beta-prior width=480}\n:::\n:::\n\n:::\n::::::\n\nNext, let Y be the number of the 40 study participants that would\ninflict the most severe shock. Assuming that each participant behaves\nindependently of the others, we can model the dependence of $Y$ on $\\pi$\nusing the Binomial. In summary, we have the following Beta-Binomial\nBayesian model:\n\n$$\n\\begin{align*}\nY \\mid \\pi &∼ \\text{Bin}(40, \\pi)\\\\  \n\\pi &∼ \\text{Beta}(1, 10)\n\\end{align*}\n$$ {#eq-cross-ref-ID}\n\n:::::: my-assessment\n:::: my-assessment-header\n::: {#cor-003-milgram-beta-prior}\n: Examine the psychologist’s prior model for the Milgram experiment\n:::\n::::\n\n::: my-assessment-container\nWhat does the $\\text{Beta}(1,10)$ prior model in\n@fig-003-milgram-beta-prior reveal about the psychologist’s prior\nunderstanding of $\\pi$?\n\n\n::: {.cell}\n\n:::\n\n\n<div class='webex-radiogroup' id='radio_LVGTYNDNZD'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LVGTYNDNZD\" value=\"\"></input> <span>They don’t have an informed opinion.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LVGTYNDNZD\" value=\"\"></input> <span>They’re fairly certain that a large proportion of people will do what authority tells them.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_LVGTYNDNZD\" value=\"answer\"></input> <span>They’re fairly certain that only a small proportion of people will do what authority tells them.</span></label></div>\n\n:::\n::::::\n\nThe psychologist’s prior is that $\\pi$ typically takes on values near 0\nwith low variability. Thus, the psychologist is fairly certain that very\nfew people will just do whatever authority tells them. Of course, the\npsychologist’s understanding will evolve upon seeing the results of\nMilgram’s study.\n\n:::::: my-assessment\n:::: my-assessment-header\n::: {#cor-003-quiz-posterior-beta-milgram}\n: Calculate the posterior beta model for the Milgram experiment\n:::\n::::\n\n::: my-assessment-container\nIn the end, 26 of the 40 study participants inflicted what they\nunderstood to be the maximum shock. In light of this data, what’s the\npsychologist’s posterior model of $\\pi$:\n<select class='webex-select'><option value='blank'></option><option value=''>Beta(26, 10)</option><option value=''>Beta(26, 27)</option><option value='answer'>Beta(27, 24)</option><option value=''>Beta(27, 10)</option></select>\n\n$$\\pi \\mid (Y = 26) \\sim \\text{Beta}(???, ???)$$\n:::\n::::::\n\nPlugging the prior <a class='glossary' title='In the Bayesian framework, a hyperparameter is a parameter of a prior distribution.  It defines the shape, location, or scale of the prior belief about a model parameter before observing any data. Unlike model parameters, which are estimated from the data (e.g., the mean or variance of a distribution), hyperparameters are set before the analysis begins and are not directly learned from the data. (Brave-AI)'>hyperparameters</a>\n($\\alpha = 1, \\beta = 10$) and data ($y = 26, n = 40$) into\n@eq-003-beta-binomial-formula establishes the psychologist’s posterior\nmodel of $\\pi$:\n\n$$\n\\begin{align*}\n\\pi \\mid (Y = y) &\\sim \\text{Beta}(\\alpha + y, \\beta + n − y) \\\\\n\\pi \\mid (Y = 26) &\\sim \\text{Beta}(1 + 26, 10 + 40 − 26) \\\\\n\\pi \\mid (Y = 26) &\\sim \\text{Beta}(27, 24)\n\\end{align*}\n$$ {#eq-calculate-posterior-beta-milgram}\n\nThis posterior is summarized and plotted below, contrasted with the\nprior pdf and scaled likelihood function. Note that the psychologist’s\nunderstanding evolved quite a bit from their prior to their posterior.\nThough they started out with an understanding that fewer than \\~25% of\npeople would inflict the most severe shock, given the strong\ncounterevidence in the study data, they now understand this figure to be\nsomewhere between \\~30% and \\~70%.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-summarize-beta-binomial-milgram}\n: Summarize the Beta-Binomial of the Milgram experiment\n:::\n::::\n\n::: my-r-code-container\n\n::: {#tbl-003-summarize-beta-binomial-milgram .cell tbl-cap='Summary of the Beta-Binomial model for the Milgram experiment'}\n\n```{#lst-003-summarize-beta-binomial-milgram .r .cell-code  lst-cap=\"Summarie Milgram’s Beta-Binomial model\"}\nbayesrules::summarize_beta_binomial(alpha = 1, beta = 10, y = 26, n = 40) |> \n  knitr::kable(digits = 5)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|    mean|    mode|     var|      sd|\n|:---------|-----:|----:|-------:|-------:|-------:|-------:|\n|prior     |     1|   10| 0.09091| 0.00000| 0.00689| 0.08299|\n|posterior |    27|   24| 0.52941| 0.53061| 0.00479| 0.06922|\n\n\n:::\n:::\n\n:::\n::::::\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-003-result-beta-binomial-milgram}\n: Result of the Bayesian analys of the Milgram experiment\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{#lst-003-result-beta-binomial-milgram .r .cell-code  lst-cap=\"Bayesian result of the Milgram experiment\"}\nbayesrules::plot_beta_binomial(alpha = 1, beta = 10, y = 26, n = 40)\n```\n\n::: {.cell-output-display}\n![The Beta prior pdf, scaled Binomial likelihood function, and Beta posterior pdf for $\\pi$, the proportion of subjects that would follow the given instructions.](003-beta-binomial_files/figure-html/fig-003-result-beta-binomial-milgram-1.png){#fig-003-result-beta-binomial-milgram width=672}\n:::\n:::\n\n:::\n::::::\n\n### The role of ethics in statistics and data science\n\nIn working through the previous example, we hope you were a bit\ndistracted by your inner voice – this experiment seems ethically\ndubious. You wouldn’t be alone in this thinking. Stanley Milgram is a\ncontroversial historical figure.\n\nEvery data collection, visualization, analysis, and communication\nengenders both harms and benefits to individuals and groups, both direct\nand indirect. As statisticians and data scientists, it is critical to\nalways consider these harms and benefits. We encourage you to ask\nyourself the following questions each time you work with data:\n\n::::::: num-check\n::::: num-check-header\n::: num-check-icon\n:::\n\n::: {#tdo-003-ethical-issues}\n:::\n\n: Ask yourself the following questions each time you work with data\n:::::\n\n::: num-check-body\n-   [ ] What are the study’s potential **benefits** to society? To\n    participants?\n-   [ ] What are the study’s potential **risks** to society? To\n    participants?\n-   [ ] What **ethical issues** might arise when generalizing\n    observations on the study participants to a larger population?\n-   [ ] Who is **included** and **excluded** in this study? What are the\n    corresponding risks and benefits? Are individuals in groups that\n    have been historically (and currently) marginalized put at greater\n    risk?\n-   [ ] Were the people who might be affected by your study **involved**\n    in the study? If not, you may not be qualified to evaluate these\n    questions.\n-   [ ] What’s the personal story or **experience** of each subject\n    represented by a row of data?\n:::\n:::::::\n\nThe importance of considering the context and implications for your\nstatistical and data science work cannot be overstated. As statisticians\nand data scientists, we are responsible for considering these issues so\nas not to harm individuals and communities of people. Fortunately, there\nare many resources available to learn more.\n\n:::::: my-resource\n:::: my-resource-header\n::: {#lem-003-ethical-issuesin-data-science}\n: Books and articles on ethical issues in data science\n:::\n::::\n\n::: my-resource-container\n-   📰 Integrating Data Science Ethics into an Undergraduate Major:\n    [@baumer-2018]\n-   📖 Data Feminism: [@dignazio-2023]\n-   📖 Counting Feminicide: Data Feminism in Action: [@dignazio-2024]\n-   📰 Automating Bias: [@eubanks-2018]\n-   📖 Automating Inequality: How High-Tech Tools Profile, Police, and\n    Punish the Poor: [@eubanks-2019]\n-   📰 Datasheets for Datasets: [@gebru-2021]\n-   📰 Model Cards for Model Reporting: [@mitchell-2019]\n-   📖 Algorithms of Oppression: How Search Engines Reinforce Racism:\n    [@noble-2018]\n-   📰 Closing the AI Accountability Gap: Defining an End-to-End\n    Framework for Internal Algorithmic Auditing: [@raji-2020]\n:::\n::::::\n\n## Summary\n\nIn this chapter, we built the foundational\n<a class='glossary' title='The Beta-Binomial Bayesian Model is a fundamental framework in Bayesian statistics used to estimate a proportion π (e.g., success probability) when data are binary outcomes (success/failure) across n independent trials. The Beta-Binomial Bayesian Model combines a Beta prior with a Binomial likelihood, resulting in a Beta posterior due to conjugacy.  This makes it ideal for estimating unknown proportions (e.g., success rates) from binary data.'>Beta-Binomial model</a> for $\\pi$, an\nunknown **proportion** that can take any value between $0$ and $1$:\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-003-beta-binomial-model-summary}\n: Beta-Binomial model\n:::\n::::\n\n::: my-theorem-container\n$$\n\\begin{align*}\nY \\mid \\pi \\sim \\text{Bin}(n, \\pi)& \\\\\n&⇒ π∣(Y = y) ∼ Beta(α + y, β + n − y) \\\\\n\\pi \\sim \\text{Beta}(\\alpha, \\beta)&\n\\end{align*}\n$$ {#eq-003-beta-binomial-model-summary}\n\nCompare with @eq-003-beta-binomial-model.\n:::\n::::::\n\nIn constructing this model we followed the four pieces common to every\nBayesian analysis (compare with my personal summary in\n@prp-002-bayesian-analysis-steps):\n\n:::::: my-procedure\n:::: my-procedure-header\n::: {#prp-003-beta-binomial-bayesian-analysis-steps}\n: Steps of the Beta-Binomial model analysis\n:::\n::::\n\n::: my-procedure-container\n**1. Prior model**\n\nThe <a class='glossary' title='The Beta Probability Model is a continuous probability distribution defined on the interval [0, 1], widely used to model probabilities, proportions, or fractions.  It is particularly valuable in Bayesian inference as the conjugate prior for binomial, Bernoulli, and geometric distributions.'>Beta prior model</a> for $\\pi$\ncan be tuned to reflect the relative prior plausibility of each\n$\\pi \\in [0, 1]$.\n\n$$\nf (\\pi) =  \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\n$$ {#eq-003-beta-prior-model-summary}\n\n**2. Data model**\n\nTo learn about $\\pi$, we collect data $Y$ , the number of successes in\n$n$ independent trials, each having probability of success $\\pi$. The\ndependence of $Y$ on $\\pi$ is summarized by the\n<a class='glossary' title='Binomial model is a foundational framework in Bayesian statistics used to estimate the probability of success in a series of independent trials. It combines a binomial likelihood — which models the number of successes in a fixed number of trials—with a beta prior on the success probability, resulting in a beta posterior. This conjugate relationship allows for analytical computation and intuitive interpretation. (Brave-AI)'>Binomial model</a> $\\text{Bin}(n, \\pi)$.\n\n**3. Likelihood function**\n\nUpon observing data $Y = y$ where $y \\in \\{0, 1, . . . , n\\}$, the\n<a class='glossary' title='The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI)'>likelihood function</a> of $\\pi$,\nobtained by plugging $y$ into the Binomial <a class='glossary' title='A probability mass function (PMF) is a mathematical function that calculates the probability a discrete random variable will be a specific value. PMFs also describe the probability distribution for the full range of values for a discrete variable. Probability mass functions find the LIKELIHOOD of a particular outcome. Using a PMF to calculate the likelihoods for all possible values of the discrete variable produces its PROBABILITY DISTRIBUTION.(Statistics By Jim)'>pmf</a>,\nprovides a mechanism by which to compare the compatibility of the data\nwith different $\\pi$:\n\n$$\nL(\\pi \\mid y) = \\binom{n}{y}\\pi^{y}(1 − \\pi)^{n−y} \\text{ for } \\pi \\in [0, 1]\n$$ {#eq-003-binomial-likelihood-function-summary}\n\n**4. Posterior model**\n\nVia <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ Rule</a>, the\n<a class='glossary' title=''>conjugate</a> Beta prior combined with the Binomial data\nmodel produce a Beta posterior model for $\\pi$. The updated Beta\nposterior parameters $(\\alpha + y, \\beta + n − y)$ reflect the influence\nof the prior (via $\\alpha$ and $\\beta$) and the observed data (via $y$\nand $n$).\n\n$$\nf (\\pi \\mid y) \\propto f (\\pi)L(\\pi \\mid y) \\propto \\pi^{(\\alpha+y)−1}(1 − \\pi)^{(\\beta+n−y)−1}\n$$\n:::\n::::::\n\n## Exercises\n\n::: {#wrn-003-exercises .callout-warning}\n###### Take my solutions with caution!\n\nKeep in mind, that I am not an expert but a learner. This means that the\npresented solutions may have errors or are completely wrong.\n\n**\"With a Little Help from My Friends\"[^003-beta-binomial-5] <a class='glossary' title='Claude Haiku is Anthropic’s smallest, fastest, and most cost-effective model in the Claude family, designed for high-speed, low-latency tasks while maintaining strong performance on coding, computer use, and agent workflows.'>Claude Haiku</a> and <a class='glossary' title='Claude Sonnet 4.5 is Anthropic’s most advanced model in the Sonnet series, released on September 29, 2025, and is now the default model for general use across Anthropic’s platforms, including claude.ai, mobile apps, and the API.'>Claude Sonnet 4.5</a>**\n\n[^003-beta-binomial-5]: Song by The Beatles, written by John Lennon and Paul McCartney 1967; transformed by Joe Cocker 1968. \n\nFor some of the exercises I have used <a class='glossary' title='Large Language Models (LLMs) are advanced artificial intelligence systems trained on massive amounts of text data to understand, generate, and manipulate human language. LLMs function as statistical prediction machines, learning patterns in language by predicting the next word in a sequence. This allows them to perform diverse tasks such as answering questions, writing essays, translating languages, summarizing documents, generating code, and even engaging in conversation. (Brave-AI)'>LLMs</a>, mostly the free Claude Haiku:\n\n- @exr-003-calculus-challenge for confirming formulae\n- @exr-003-summary-beta-binomial-1 and @exr-003-summary-beta-binomial-2 for recovering the data from the results of `bayesrules::summary_beta_binomial()`\n- @exr-003-plot-beta-binomial-1 (Part c) to get the code for reproducing the plot with `bayesrules::plot_beta_binomial()` and\n- @exr-003-plot-beta-binomial-2 (Part c) for the more detailed answer.\n\nIf you find errors please report to me at the [GitHub\nrepo](https://github.com/petzi53/bayes-rules) of this website.\n:::\n\n### Practice: Beta prior models\n\nI am going to use the [Shiny\napp](https://ajohns24.github.io/454_Fall_21/the-beta-binomial-model.html#shiny-apps)\nwith the addition to show a 95% <a class='glossary' title='In Bayesian statistics, a credible interval is an interval within which an unobserved parameter value falls with a particular probability. It is an interval in the domain of a posterior probability distribution or a predictive distribution. The generalisation to multivariate problems is the credible region. Credible intervals are a Bayesian analog to confidence intervals in frequentist statistics. The interval is fixed (after data is observed), the parameter is random. Interpretation: ‘There is a 95% probability that the parameter lies within this interval, given the data.’ (Wikipedia and Brave-AI)'>credible interval</a> (CI).\n\nIn each situation below, tune a $\\text{Beta}(\\alpha, \\beta)$ model that\naccurately reflects the given prior information. In many cases, there’s\nno single “right” answer, but rather multiple “reasonable” answers.\n\n#### Tune Beta prior: Take I\n\n::::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-tune-beta-prior-1}\n: Tune your Beta prior: Take I\n:::\n::::\n\n:::: my-exercise-container\n::: panel-tabset\n###### (a)\n\nYour friend applied to a job and tells you: “I think I have a 40% chance\nof getting the job, but I’m pretty unsure.” When pressed further, they\nput their chances between 20% and 60%.\n\n------------------------------------------------------------------------\n\n**My Solution**\n\n$$\\text{Beta}(9, 13)$$ with a mode of 0.400 and a 95% CI of \\[0.2182,\n0.6156\\].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(9, 13)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-1-1.png){width=288}\n:::\n:::\n\n\n###### (b)\n\nA scientist has created a new test for a rare disease. They expect that\nthe test is accurate 80% of the time with a variance of 0.05.\n\n------------------------------------------------------------------------\n\n**My Solution**\n\n$$\\text{Beta}(25, 7)$$ with a mode of 0.800 and a variance of 0.0052. It\nhas a 95 CI of \\[0.6253, 0.9041\\].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(25, 7)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-2-1.png){width=288}\n:::\n:::\n\n\n###### (c)\n\nYour Aunt Jo is a successful mushroom hunter. She boasts: “I expect to\nfind enough mushrooms to feed myself and my co-workers at the\nauto-repair shop 90% of the time, but if I had to give you a likely\nrange it would be between 85% and 100% of the time.”\n\n------------------------------------------------------------------------\n\n**My Solution**\n\n$$\\text{Beta}(21, 3)$$ with a mode of 0.9091 and a 95% CI of \\[0.7196,\n0.9722\\]. This is a left skewed distribution as it has to be. But it is\nnot very near the desired interval \\[0.85, 1.00\\]. I could'nt do it\nbetter. In different combinations with the left side of the CI nearer to\n0.8 the mode goes 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(21, 3)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-3-1.png){width=288}\n:::\n:::\n\n\n###### (d)\n\nSal (who is a touch hyperbolic) just interviewed for a job, and doesn’t\nknow how to describe their chances of getting an offer. They say, “I\ncouldn’t read my interviewer’s expression! I either really impressed\nthem and they are absolutely going to hire me, or I made a terrible\nimpression and they are burning my resumé as we speak.”\n\n------------------------------------------------------------------------\n\n**My Solution**\n\n$$\\text{Beta}(1, 1)$$\n\n$\\text{Beta}(1, 1)$ is also known as the uniform distribution on \\[0,\n1\\], often denoted as $\\text{Uniform}(0, 1)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(1, 1)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-4-1.png){width=288}\n:::\n:::\n\n\nThis distribution is flat because it assigns equal probability density\nto all values of the parameter between 0 and 1, reflecting a lack of\nprior preference for any specific value. It is also referred to as a\nnoninformative prior or flat prior in Bayesian statistics.\n:::\n::::\n:::::::\n\n#### Tune Beta prior: Take II\n\nAs in @exr-003-tune-beta-prior-1, tune an appropriate\n$\\text{Beta}(\\alpha, \\beta)$ prior model for each situation below.\n\n::::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-tune-beta-prior-2}\n: Tune your Beta prior: Take II\n:::\n::::\n\n:::: my-exercise-container\n::: panel-tabset\n###### (a)\n\nYour friend tells you “I think that I have a 80% chance of getting a\nfull night of sleep tonight, and I am pretty certain.” When pressed\nfurther, they put their chances between 70% and 90%.\n\n------------------------------------------------------------------------\n\n**My Solution**\n\n$$\\text{Beta}(38, 9)$$ with a mode of 0.8222and a 95% CI of \\[0.6858,\n0.9064\\].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(38, 9)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-5-1.png){width=288}\n:::\n:::\n\n\n###### (b)\n\nA scientist has created a new test for a rare disease. They expect that\nit’s accurate 90% of the time with a variance of 0.08.\n\n------------------------------------------------------------------------\n\n**My Solution**\n\n$$\\text{Beta}(97, 11)$$ with a mode of 0.9057 and a variance of 0.0008.\nIt has a 95 CI of \\[0.8348, 0.9476\\]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(97, 11)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-6-1.png){width=288}\n:::\n:::\n\n\n###### (c)\n\nMax loves to play the video game *Animal Crossing*. They tell you: “The\nprobability that I play *Animal Crossing* in the morning is somewhere\nbetween 75% and 95%, but most likely around 85%.”\n\n------------------------------------------------------------------------\n\n**My Solution**\n\n$$\\text{Beta}(56, 8)$$ with a mode of 0.8871 and a 95% CI of \\[0.7844,\n0.9435\\].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(56, 8)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-7-1.png){width=288}\n:::\n:::\n\n\n###### (d)\n\nThe bakery in Easthampton, Massachusetts often runs out of croissants on\nSundays. Ben guesses that by 10 a.m., there is a 30% chance they have\nrun out, but is pretty unsure about that guess.\n\n------------------------------------------------------------------------\n\n**My Solution**\n\n$$\\text{Beta}(8, 18)$$\n\nwith a mode of 0.2917 and a 95% CI of \\[0.1495, 0.4939\\].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(8, 18)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-8-1.png){width=288}\n:::\n:::\n\n:::\n::::\n:::::::\n\n#### Specify a non-informative Beta prior\n\nYou want to specify a Beta prior for a situation in which you have no\nidea about some parameter $\\pi$. You think $\\pi$ is equally likely to be\nanywhere between 0 and 1.\n\n::::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-beta-prior-between-0-1}\n: Specify a Beta prior for a situation in which you have no idea\n:::\n::::\n\n:::: my-exercise-container\n::: panel-tabset\n###### (a)\n\nPlot a Beta prior for a situation in which you have no idea.\n\n**My Solution**\n\n$$\\text{Beta}(1, 1)$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(1, 1)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-9-1.png){width=288}\n:::\n:::\n\n\n###### (b)\n\nWhat is the mean of the Beta prior that you specified? Explain why that\ndoes or does not align with having no clue.\n\n------------------------------------------------------------------------\n\n**My Solution**\n\nThe mean is 0.5.\n\n------------------------------------------------------------------------\n\nThe following explanation comes from the Brave AI, using Claude Haiku:\n\nThe mean is 0.5 because of the symmetry of the Beta(1,1) distribution.\n\nThe mean of a Beta distribution is given by\n$\\frac{\\alpha}{\\alpha + \\beta}$. For $\\alpha = 1$ and $\\beta = 1$, the\nmean is $\\frac{1}{1 + 1} = 0.5$.\n\nBut here's the deeper reason: When $\\alpha = \\beta$, the Beta\ndistribution is perfectly symmetric around 0.5. This means the\nprobability density is mirrored on both sides of 0.5—the likelihood of\ngetting a value at 0.3 is the same as getting 0.7, for example.\n\nBecause the distribution is balanced symmetrically around the midpoint\nof the interval (0,1), the mean naturally falls at that midpoint: 0.5.\n\nSo the flat prior doesn't just reflect \"no preference\"—the symmetry of\nequal parameters ($\\alpha = \\beta = 1$) ensures that preference is\ncentered exactly at 0.5, making it the average value of the\ndistribution.\n\n###### (c)\n\nWhat is the standard deviation of the Beta prior that you specified?\n\n------------------------------------------------------------------------\n\n**My Solution**\n\nCompare with @eq-003-beta-variance.\n\nFrom the Shiny app:\n\nThe <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> (SD): $\\sigma = 0.2887$ taken\n\nFrom the AI <a class='glossary' title='Claude Haiku is Anthropic’s smallest, fastest, and most cost-effective model in the Claude family, designed for high-speed, low-latency tasks while maintaining strong performance on coding, computer use, and agent workflows.'>Claude Haiku</a>:\n\nThe standard deviation of the Beta(1,1) distribution is **√(1/12) ≈\n0.289**.\n\nHere's how it's calculated:\n\nThe <a class='glossary' title='Variance is the squared deviation from the mean of a random variable. The variance is also often defined as the square of the standard deviation. Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. It is the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by σ, σ^2, VAR(x), var(x) or V(x). (Wikipedia)'>variance</a> of a Beta distribution is\ngiven by:\n\n$$\\text{Variance} = \\frac{(\\alpha \\cdot \\beta)}{ ((\\alpha + \\beta)^2 \\cdot (\\alpha + \\beta + 1))}$$\n\nFor Beta(1,1):\n\n$$\n\\begin{align*}\n\\text{Variance} &= \\frac{(1 \\cdot 1)}{ ((1 + 1)^2 \\cdot (1 + 1 + 1))} \\\\\n&= \\frac{1}{4 · 3} = \\frac{1}{12} \\approx 0.0833\n\\end{align*}\n$$\n\nThe standard deviation is the square root of variance:\n\n$$SD = \\sqrt{\\frac{1}{12}} \\approx 0.289$$\n\nThis relatively high standard deviation (compared to the mean of 0.5)\nreflects the fact that \\$\\text{Beta}(1,1) is a **uniform\ndistribution**—all values between 0 and 1 are equally likely, so there's\nsubstantial spread across the entire interval. This is why it's called a\n\"flat\" or \"vague\" prior—it expresses genuine uncertainty about the\nparameter across the full range.\n\n###### (d)\n\nSpecify and plot an example of a Beta prior that has a smaller standard\ndeviation than the one you specified.\n\n------------------------------------------------------------------------\n\n**My Solution**\n\nThere are countless solution. I have chosen a solution with the same\nmean (.5) and a very vage prior:\n\n$$\\text{Beta}(2, 2)$$\n\nIt has a (slightly) smaller standard deviation of 0.2236 instead of\n0.2887.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(2, 2)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-10-1.png){width=288}\n:::\n:::\n\n\n###### (e)\n\nSpecify and plot an example of a Beta prior that has a larger standard\ndeviation than the one you specified.\n\n------------------------------------------------------------------------\n\n**My Solution**\n\n$$\\text{Beta}(0.5, 0.5)$$\n\nA good example is\n\n$$\\text{Beta}(0.5, 0.5)$$\n\nIt has a standard deviation of approximately **0.354** (compared to\nBeta(1,1)'s 0.289).\n\nHere's why:\n\nFor Beta(0.5, 0.5):\n\n$$\n\\begin{align*}\n\\text{Variance} &= \\frac{(0.5 \\cdot 0.5)}{ ((0.5 + 0.5)^2 \\cdot (0.5 + 0.5 + 1))}  = \\frac{0.25}{1 \\cdot 2} = \\frac{1}{12} = 0.125 \\\\\n\\text{SD} &= \\sqrt{0.125} \\approx 0.354\n\\end{align*}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(0.5, 0.5)\n```\n\n::: {.cell-output-display}\n![](003-beta-binomial_files/figure-html/unnamed-chunk-11-1.png){width=288}\n:::\n:::\n\n\nWith the help of <a class='glossary' title='Large Language Models (LLMs) are advanced artificial intelligence systems trained on massive amounts of text data to understand, generate, and manipulate human language. LLMs function as statistical prediction machines, learning patterns in language by predicting the next word in a sequence. This allows them to perform diverse tasks such as answering questions, writing essays, translating languages, summarizing documents, generating code, and even engaging in conversation. (Brave-AI)'>LLM</a>\n<a class='glossary' title='Claude Haiku is Anthropic’s smallest, fastest, and most cost-effective model in the Claude family, designed for high-speed, low-latency tasks while maintaining strong performance on coding, computer use, and agent workflows.'>Claude Haiku</a>\n\nWhile $\\text{Beta}(1,1)$ is uniform (flat), $\\text{Beta}(0.5, 0.5)$ is\n**U-shaped**. It assigns higher probability density to extreme values\n(near 0 and near 1) and *lower* probability density to middle values\nlike 0.5. This creates more variability—the distribution is more spread\nout toward the extremes.\n\nThis is a \"super-vague\" prior that expresses even stronger uncertainty,\nreflecting a belief that the true parameter is likely to be extreme\nrather than moderate. It's useful in situations where you want to be\nmaximally non-committal about where the parameter lies.\n:::\n::::\n:::::::\n\n#### Which Beta? Take I\n\n:::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-which-beta-1}\n: Which Beta? Take I\n:::\n::::\n\n::: my-exercise-container\nSix Beta pdfs are plotted below. Match each to one of the following\nmodels: Beta(0.5,0.5), Beta(1,1), Beta(2,2), Beta(6,6), Beta(6,2),\nBeta(0.5,6).\n\n![6 different Beta\nmodels](img/003-which-beta-1.png){#fig-003-which-beta-1\nfig-alt=\"6 different beta models: a = u-shape, b = arc (open below), c = left-skewed, d = horizontal line, e = L-shaped, f = symmetrical bell curve\"\nfig-align=\"center\" width=\"70%\"}\n\n-   \n\n    (a) $\\text{Beta}(0.5, 0.5)$\n\n-   \n\n    (b) $\\text{Beta}(2, 2)$\n\n-   \n\n    (c) $\\text{Beta}(6, 2)$\n\n-   \n\n    (d) $\\text{Beta}(1, 1)$\n\n-   \n\n    (e) $\\text{Beta}(0.5, 6)$\n\n-   \n\n    (f) $\\text{Beta}(6, 6)$\n:::\n::::::\n\n#### Which Beta? Take II\n\n:::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-which-beta-2}\n: Which Beta? Take II\n:::\n::::\n\n::: my-exercise-container\nSix Beta pdfs are plotted below. Match each to one of the following\nmodels: Beta(1,0.3), Beta(2,1), Beta(3,3), Beta(6,3), Beta(4,2),\nBeta(5,6).\n\n![Another 6 different Beta\nmodels](img/003-which-beta-2.png){#fig-003-which-beta-2\nfig-alt=\"6 different beta models: a = j-shape, b = symmetrical bell-curve, c = left-skewed with big tail, d = 45 degree line, e = very lightly left-skewed, f = left-skewed with small tail\"\nfig-align=\"center\" width=\"70%\"}\n\n(a) $\\text{Beta}(1, 0.3)$\n(b) $\\text{Beta}(3, 3)$\n(c) $\\text{Beta}(4, 2)$\n(d) $\\text{Beta}(2, 1)$\n(e) $\\text{Beta}(5, 6)$\n(f) $\\text{Beta}(6, 3)$\n:::\n::::::\n\n#### Beta properties\n\n::::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-beta-properties}\n: Examine the properties of the Beta models in @fig-003-which-beta-1\n:::\n::::\n\n:::: my-exercise-container\n::: panel-tabset\n###### Summary\n\nThe following table is a summary of all the different requested values\ntaken from the a slightly modifies Shiny app as presented in [STAT 454:\nBayesian Statistics in section\n3.3](https://ajohns24.github.io/454_Fall_21/the-beta-binomial-model.html#shiny-apps)\n\n| \\#  | Model          |   Mean |    Mode |   SD   |\n|-----|:---------------|-------:|--------:|:------:|\n| a   | Beta(0.5, 0.5) |    0.5 | 0 and 1 | 0.3536 |\n| b   | Beta(2, 2)     |    0.5 |     0.5 | 0.2236 |\n| c   | Beta(6, 2)     |   0.75 |  0.8333 | 0.1443 |\n| d   | Beta(1, 1)     |    0.5 |  ?????? | 0.2887 |\n| e   | Beta(0.5, 6)   | 0.0769 |       0 | 0.0973 |\n| f   | Beta(6, 6)     |    0.5 |     0.5 | 0.1387 |\n\n: Summary table of model properties {#tbl-003-exercise-model-summary}\n{.striped}\n\n| Statistic | Formula | $\\alpha = \\beta = 0.5$ | $\\alpha = \\beta = 1$ | $\\alpha < 1$ and $\\beta > 1$ | $\\alpha > 1$ and $\\beta < 1$ |\n|-----------|:---------:|:-----------------:|:---------:|:---------:|:---------:|\n| **Mean** | $\\frac{\\alpha}{\\alpha + \\beta}$ | **0.5** | **0.5** | **value dependent** | **value dependent** |\n| **Mode** | $\\frac{\\alpha - 1}{\\alpha + \\beta - 2}$ (only if $\\alpha > 1, \\beta > 1$) | **undefined**<br>(bimodal at 0 and 1) | **undefined**<br>(every point is a mode) | **0** | **1** |\n| **SD** | $\\sqrt{ \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)} }$ | **\\~0.3536** | **\\~0.2887** | **value dependent** | **value dependent** |\n\n###### (a) Mean\n\n-   Which Beta model has the *smallest* mean? Beta(0.5, 6) = 0.0769.\n-   Which Beta model has the *biggest* mean? Beta(6, 2) = 0.75.\n\n**Provide visual evidence:**\n\n\n::: {#fig-003-exercise-beta-means .cell layout-ncol=\"2\"}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-003-exercise-beta-means\n#| fig-height: 2.5\n#| fig-width: 3.5\n#| fig-cap: \"Beta models with the smallest and biggest mean\"\n#| fig-subcap: \n#|    - \"Smallest mean: Beta(0.5, 6)\"\n#|    - \"Biggest mean: Beta(6, 2)\"\n#| layout-ncol: 2\n\nmy_plot_beta(0.5, 6, TRUE)\nmy_plot_beta(6, 2, TRUE)\n```\n````\n\n::: {.cell-output-display}\n![Smallest mean: Beta(0.5, 6)](003-beta-binomial_files/figure-html/fig-003-exercise-beta-means-1.png){#fig-003-exercise-beta-means-1 width=336}\n:::\n\n::: {.cell-output-display}\n![Biggest mean: Beta(6, 2)](003-beta-binomial_files/figure-html/fig-003-exercise-beta-means-2.png){#fig-003-exercise-beta-means-2 width=336}\n:::\n\nBeta models with the smallest and biggest mean\n:::\n\n\n**Calculate the corresponding means:**\n\nTo calculate it manually use $\\frac{\\alpha}{\\alpha + \\beta}$. Confer\nwith the formula in @eq-003-beta-mean-mode. To calculate it with R you\ncan apply the above formula or (more convenient) use the\n`bayesrules::summarize_beta()` function:\n\n-   The mean of $\\text{Beta}(0.5, 6)$ is\n    `bayesrules::summarize_beta(0.5, 6)$mean` =\n    0.0769231.\n-   The mean of $\\text{Beta}(6, 2)$ is\n    `bayesrules::summarize_beta(6, 2)$mean` =\n    0.75.\n\n###### (b) Mode\n\n-   Which Beta model has the *smallest* mode? $\\text{Beta}(0.5, 0.5)$ is\n    a <a class='glossary' title='A bimodal distribution is a type of probability distribution that exhibits two distinct peaks or modes, indicating the presence of two separate groups, processes, or subpopulations within a single dataset.  These peaks represent the most frequently occurring values or ranges in the data, separated by a visible valley or trough.'>bimodal distribution</a> and has modes\n    at 0 and 1. Another model with mode of 0 is $\\text{Beta}(0.5, 6)$.\n-   Which Beta model has the *biggest* mode? This is (again)\n    $\\text{Beta}(0.5, 0.5)$, because it second mode = 1.\n\n**Provide visual evidence:**\n\n\n::: {#fig-003-exercise-beta-modes .cell layout-ncol=\"2\"}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-003-exercise-beta-modes\n#| fig-height: 2.5\n#| fig-width: 3.5\n#| fig-cap: \"Beta models with the smallest and biggest mode.\"\n#| fig-subcap: \n#|    - \"Smallest mode: Beta(0.5, 6). Mode = 0, line therefore not visible.\"\n#|    - \"Biggest mode: Beta(0.5, 0.5). Two modes with 1, line not drawn.\"\n#| layout-ncol: 2\n\nmy_plot_beta(0.5, 6, FALSE, TRUE)\nmy_plot_beta(0.5, 0.5, FALSE, TRUE)\n```\n````\n\n::: {.cell-output-display}\n![Smallest mode: Beta(0.5, 6). Mode = 0, line therefore not visible.](003-beta-binomial_files/figure-html/fig-003-exercise-beta-modes-1.png){#fig-003-exercise-beta-modes-1 width=336}\n:::\n\n::: {.cell-output-display}\n![Biggest mode: Beta(0.5, 0.5). Two modes with 1, line not drawn.](003-beta-binomial_files/figure-html/fig-003-exercise-beta-modes-2.png){#fig-003-exercise-beta-modes-2 width=336}\n:::\n\nBeta models with the smallest and biggest mode.\n:::\n\n\n**Calculate the corresponding mode:**\n\nThe mode for Beta models where $\\alpha$ and/or $\\beta$ \\< 1 is not\ncovered by the formula in @eq-003-beta-mean-mode.\n\nThe mode depends on the values of $\\alpha$ and $\\beta$:\n\nIf $\\alpha > 1$ and $\\beta > 1$:\n\n$$\\text{Mode for Beta models} = \\frac{α+β−2}{α−1}$$\n\nIf $\\alpha \\leq 1$ or $\\beta \\leq 1$, the mode is not unique or lies at\nthe boundary:\n\n-   If $\\alpha < 1$ and $\\beta > 1$: mode = 0\n-   If $\\alpha > 1$ and $\\beta < 1$: mode = 1\n-   If $\\alpha = \\beta = 1$: <a class='glossary' title='The standard uniform distribution is a continuous probability distribution where all values in the interval [0, 1] are equally likely.  It is a special case of the continuous uniform distribution with parameters a = 0 and b = 1, often denoted as U(0, 1). (Brave-AI)'>uniform distribution</a>, the\n    mode is undefined (every point is a mode).\n-   If $\\alpha = \\beta = 0.5$: the distribution is U-shaped (arcsine\n    distribution), and the mode is not unique—it peaks at both ends (0\n    and 1), but the density is infinite at the boundaries. The mode is\n    not defined in the interior; the density tends to infinity at\n    $x = 0$ and $x = 1$, so the distribution has bimodal behavior at the\n    endpoints.\n\nTo calculate the mode manually if $\\alpha \\text{ and } \\beta > 1$ use\n@eq-003-beta-mean-mode and @eq-003-beta-sd.\n\nTo calculate it with R you can apply the above formula or (more\nconvenient) use the `bayesrules::summarize_beta()` function, which has\nthe advantage that you can also use values below 1 for\n$\\alpha \\text{ or/and } \\beta$.\n\n-   The mode of $\\text{Beta}(0.5, 6)$ is\n    `bayesrules::summarize_beta(0.5, 6)$mode` =\n    0.\n-   The mode of $\\text{Beta}(0.5, 0.5)$ is\n    `bayesrules::summarize_beta(0.5, 0.5)$mode` =\n    0 and 1.\n\n###### (c) SD\n\n-   Which Beta model has the *smallest* standard deviation? SD of\n    $\\text{Beta}(0.5, 6) \\approx 0.0973$.\n-   Which Beta model has the *biggest* standard deviation? SD of\n    $\\text{Beta}(0.5, 0.5) \\approx 0.3536$\n\n**Provide visual evidence:**\n\n\n::: {#fig-003-exercise-beta-standard-deviations .cell layout-ncol=\"2\"}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-003-exercise-beta-standard-deviations\n#| fig-height: 2.5\n#| fig-width: 3.5\n#| fig-cap: \"Beta models with the smallest and biggest standard deviations\"\n#| fig-subcap: \n#|    - \"Smallest SD: Beta(0.5, 6).\"\n#|    - \"Biggest SD: Beta(0.5, 0.5).\"\n#| layout-ncol: 2\n\nmy_plot_beta(0.5, 6)\nmy_plot_beta(0.5, 0.5)\n```\n````\n\n::: {.cell-output-display}\n![Smallest SD: Beta(0.5, 6).](003-beta-binomial_files/figure-html/fig-003-exercise-beta-standard-deviations-1.png){#fig-003-exercise-beta-standard-deviations-1 width=336}\n:::\n\n::: {.cell-output-display}\n![Biggest SD: Beta(0.5, 0.5).](003-beta-binomial_files/figure-html/fig-003-exercise-beta-standard-deviations-2.png){#fig-003-exercise-beta-standard-deviations-2 width=336}\n:::\n\nBeta models with the smallest and biggest standard deviations\n:::\n\n\nThe width and height of the curves visually reflect the standard\ndeviation — wider curves indicate greater spread.\n\n**Calculate the corresponding standard deviation (SD):**\n\nTo calculate it manually use @eq-003-beta-variance and @eq-003-beta-sd.\n\nTo calculate it with R you can apply the above formula or (more\nconvenient) use the `bayesrules::summarize_beta()` function:\n\n-   The standard deviation of $\\text{Beta}(0.5, 6)$ is\n    `bayesrules::summarize_beta(0.5, 6)$sd` =\n    0.0973009.\n-   The standard deviation of $\\text{Beta}(0.5, 0.5)$ is\n    `bayesrules::summarize_beta(0.5, 0.5)$sd` =\n    0.3535534.\n\nNote that the Beta models for smallest and largest mode and SD are the\nsame!\n:::\n::::\n:::::::\n\n#### Using R for Beta\n\n:::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-using-r-for-beta}\n: Using R for Beta\n:::\n::::\n\n::: my-exercise-container\na)  Use `bayesrules::plot_beta()` to plot the six Beta models in\n    @fig-003-which-beta-1.\n\n\n::: {.cell}\n\n```{#lst-003-plot-six-beta-models .r .cell-code  lst-cap=\"Plot six Beta models\"}\np1 <- bayesrules::plot_beta(0.5, 0.5)\np2 <- bayesrules::plot_beta(2, 2)\np3 <- bayesrules::plot_beta(6, 2)\np4 <- bayesrules::plot_beta(1, 1)\np5 <- bayesrules::plot_beta(0.5, 6)\np6 <- bayesrules::plot_beta(6, 6)\n\nlibrary(patchwork)\n\np1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol = 3)\n```\n\n::: {.cell-output-display}\n![Six Beta models replicated from @fig-003-which-beta-1](003-beta-binomial_files/figure-html/fig-003-plot-six-beta-models-1.png){#fig-003-plot-six-beta-models width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nb)  Use `bayesrules::summarize_beta()` to confirm your answers to\n    @exr-003-beta-properties.\n\n\n::: {#tbl-003-summarize-six-beta-models .cell tbl-cap='Summary of six Beta models to confirm answers from @exr-003-beta-properties'}\n\n```{#lst-003-summarize-six-beta-models .r .cell-code  lst-cap=\"Summarize six Beta models\"}\ndf1 <- bayesrules::summarize_beta(0.5, 0.5)\ndf2 <- bayesrules::summarize_beta(2, 2) |> \n  dplyr::mutate(mode = as.character(mode))\ndf3 <- bayesrules::summarize_beta(6, 2) |> \n  dplyr::mutate(mode = stringr::str_trunc(\n    base::as.character(mode), 6, ellipsis = ''))\ndf4 <- bayesrules::summarize_beta(1, 1) |> \n  dplyr::mutate(mode = as.character(mode))\ndf5 <- bayesrules::summarize_beta(0.5, 6) |> \n  dplyr::mutate(mode = as.character(mode))\ndf6 <- bayesrules::summarize_beta(6, 6) |> \n  dplyr::mutate(mode = as.character(mode))\n\ndf_combined <- dplyr::full_join(df1, df2,\n  by = dplyr::join_by(mean, mode, var, sd)\n  ) |> \n  dplyr::full_join(df3, by = dplyr::join_by(mean, mode, var, sd)) |> \n  dplyr::full_join(df4, by = dplyr::join_by(mean, mode, var, sd)) |> \n  dplyr::full_join(df5, by = dplyr::join_by(mean, mode, var, sd)) |> \n  dplyr::full_join(df6, by = dplyr::join_by(mean, mode, var, sd))\n\nmodel = c(\"Beta(0.5, 0.5)\",\n          \"Beta(2, 2)\",\n          \"Beta(6, 2)\",\n          \"Beta(1, 1)\",\n          \"Beta(0.5, 6)\",\n          \"Beta(6, 6)\"\n          )\n\ndf_combined <- tibble::tibble(model) |>\n  dplyr::bind_cols(df_combined) |> \n  knitr::kable(digits = 4)\n\ndf_combined\n```\n\n::: {.cell-output-display}\n\n\n|model          |   mean|mode    |    var|     sd|\n|:--------------|------:|:-------|------:|------:|\n|Beta(0.5, 0.5) | 0.5000|0 and 1 | 0.1250| 0.3536|\n|Beta(2, 2)     | 0.5000|0.5     | 0.0500| 0.2236|\n|Beta(6, 2)     | 0.7500|0.8333  | 0.0208| 0.1443|\n|Beta(1, 1)     | 0.5000|NaN     | 0.0833| 0.2887|\n|Beta(0.5, 6)   | 0.0769|0       | 0.0095| 0.0973|\n|Beta(6, 6)     | 0.5000|0.5     | 0.0192| 0.1387|\n\n\n:::\n:::\n\n:::\n::::::\n\n#### Confirm formulas for Beta models\n\n::::::::::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-calculus-challenge}\n: Confirm formulas for Beta models\n:::\n::::\n\n:::::::::: my-exercise-container\nConfirm the mean, mode, and variance formulas for a\n$\\text{Beta}(\\alpha, \\alpha)$ distribution by applying the definitions\ndirectly to the Beta pdf.\n\nLet $\\pi$ follow a $\\text{Beta}(\\alpha, \\beta)$ model. Formulas for the\nmean, mode, and variance of $\\pi$ are given by @eq-003-beta-mean-mode\nand @eq-003-beta-variance. Confirm these properties by applying the\nfollowing definitions of mean, mode, and variance directly to the Beta\npdf f ($\\pi$), @eq-003-beta-model:\n\n$$\n\\begin{align*}\nE(\\pi) &= \\int \\pi (\\pi)\\partial\\pi  \\\\\n\\text{Mode}(\\pi) &= \\text{argmax}_{\\pi}f (\\pi)  \\\\\n\\text{Var}(\\pi) &= E [(\\pi − E(\\pi))^2] = E(\\pi^2) − [E(\\pi)]^2\n\\end{align*}\n$$\n\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\n\nI have not much experience with calculus. I prepared the solution with\n<a class='glossary' title='Claude Haiku is Anthropic’s smallest, fastest, and most cost-effective model in the Claude family, designed for high-speed, low-latency tasks while maintaining strong performance on coding, computer use, and agent workflows.'>Claude Haiku</a>.\n\nThe Beta pdf is:\n$$f(\\pi) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha - 1} (1 - \\pi)^{\\beta - 1}, \\quad \\pi \\in [0, 1]$$\n\n------------------------------------------------------------------------\n\n::: {style=\"font-size: 150%\"}\n**1. Mean**\n:::\n\n$$E(\\pi) = \\int_0^1 \\pi f(\\pi) \\, d\\pi$$\n\n$$E(\\pi) = \\int_0^1 \\pi \\cdot \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha - 1} (1 - \\pi)^{\\beta - 1} \\, d\\pi$$\n\n$$= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^1 \\pi^{\\alpha} (1 - \\pi)^{\\beta - 1} \\, d\\pi$$\n\nThe integral equals the Beta function:\n$$B(\\alpha + 1, \\beta) = \\frac{\\Gamma(\\alpha + 1)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta + 1)}$$\n\nUsing the property $\\Gamma(\\alpha + 1) = \\alpha\\Gamma(\\alpha)$ and\n$\\Gamma(\\alpha + \\beta + 1) = (\\alpha + \\beta)\\Gamma(\\alpha + \\beta)$:\n\n$$E(\\pi) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\cdot \\frac{\\alpha\\Gamma(\\alpha)\\Gamma(\\beta)}{(\\alpha + \\beta)\\Gamma(\\alpha + \\beta)}$$\n\n$$= \\frac{\\alpha\\Gamma(\\alpha + \\beta)}{(\\alpha + \\beta)\\Gamma(\\alpha + \\beta)} = \\frac{\\alpha}{\\alpha + \\beta}$$\n\n✅ **Confirmed**: $E(\\pi) = \\frac{\\alpha}{\\alpha + \\beta}$\n\n------------------------------------------------------------------------\n\n::: {style=\"font-size: 150%\"}\n**2. Mode**\n:::\n\n$$\\text{Mode}(\\pi) = \\arg\\max_{\\pi} f(\\pi)$$\n\nTo find the maximum, take the derivative of $\\log f(\\pi)$ and set it\nequal to zero:\n\n$$\\log f(\\pi) = \\log\\Gamma(\\alpha + \\beta) - \\log\\Gamma(\\alpha) - \\log\\Gamma(\\beta) + (\\alpha - 1)\\log \\pi + (\\beta - 1)\\log(1 - \\pi)$$\n\n$$\\frac{d}{d\\pi} \\log f(\\pi) = \\frac{\\alpha - 1}{\\pi} - \\frac{\\beta - 1}{1 - \\pi}$$\n\nSetting equal to zero:\n$$\\frac{\\alpha - 1}{\\pi} = \\frac{\\beta - 1}{1 - \\pi}$$\n\n$$(\\alpha - 1)(1 - \\pi) = (\\beta - 1)\\pi$$\n\n$$\\alpha - 1 - (\\alpha - 1)\\pi = (\\beta - 1)\\pi$$\n\n$$\\alpha - 1 = (\\alpha + \\beta - 2)\\pi$$\n\n$$\\pi = \\frac{\\alpha - 1}{\\alpha + \\beta - 2} \\quad \\text{(for } \\alpha > 1, \\beta > 1\\text{)}$$\n\n✅ **Confirmed**:\n$\\text{Mode}(\\pi) = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}$\n\n------------------------------------------------------------------------\n\n::: {style=\"font-size: 150%\"}\n**3. Variance**\n:::\n\n$$\\text{Var}(\\pi) = E(\\pi^2) - [E(\\pi)]^2$$\n\n::: {style=\"font-size: 120%\"}\n**Step 1: Compute** $E(\\pi^2)$\n:::\n\n$$E(\\pi^2) = \\int_0^1 \\pi^2 \\cdot \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha - 1} (1 - \\pi)^{\\beta - 1} \\, d\\pi$$\n\n$$= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^1 \\pi^{\\alpha + 1} (1 - \\pi)^{\\beta - 1} \\, d\\pi$$\n\nThe integral is the Beta function $B(\\alpha + 2, \\beta)$:\n\n$$B(\\alpha + 2, \\beta) = \\frac{\\Gamma(\\alpha + 2)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta + 2)}$$\n\nTherefore:\n\n$$E(\\pi^2) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\cdot \\frac{\\Gamma(\\alpha + 2)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta + 2)}$$\n\nUsing the property $\\Gamma(z + 1) = z\\Gamma(z)$: -\n$\\Gamma(\\alpha + 2) = (\\alpha + 1)\\alpha\\Gamma(\\alpha)$ -\n$\\Gamma(\\alpha + \\beta + 2) = (\\alpha + \\beta + 1)(\\alpha + \\beta)\\Gamma(\\alpha + \\beta)$\n\nSubstituting:\n\n$$E(\\pi^2) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\cdot \\frac{(\\alpha + 1)\\alpha\\Gamma(\\alpha)\\Gamma(\\beta)}{(\\alpha + \\beta + 1)(\\alpha + \\beta)\\Gamma(\\alpha + \\beta)}$$\n\n$$= \\frac{(\\alpha + 1)\\alpha}{(\\alpha + \\beta + 1)(\\alpha + \\beta)}$$\n\n------------------------------------------------------------------------\n\n::: {style=\"font-size: 120%\"}\n**Step 2: Compute** $[E(\\pi)]^2$\n:::\n\nFrom the previous confirmation:\n\n$$[E(\\pi)]^2 = \\left(\\frac{\\alpha}{\\alpha + \\beta}\\right)^2 = \\frac{\\alpha^2}{(\\alpha + \\beta)^2}$$\n\n------------------------------------------------------------------------\n\n::: {style=\"font-size: 120%\"}\n**Step 3: Calculate** $\\text{Var}(\\pi) = E(\\pi^2) - [E(\\pi)]^2$\n:::\n\n$$\\text{Var}(\\pi) = \\frac{\\alpha(\\alpha + 1)}{(\\alpha + \\beta)(\\alpha + \\beta + 1)} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2}$$\n\nFind a common denominator: $(\\alpha + \\beta)^2(\\alpha + \\beta + 1)$\n\n$$\\text{Var}(\\pi) = \\frac{\\alpha(\\alpha + 1)(\\alpha + \\beta) - \\alpha^2(\\alpha + \\beta + 1)}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}$$\n\nExpand the numerator:\n\n$$\\alpha(\\alpha + 1)(\\alpha + \\beta) = \\alpha^2(\\alpha + \\beta) + \\alpha(\\alpha + \\beta) = \\alpha^3 + \\alpha^2\\beta + \\alpha^2 + \\alpha\\beta$$\n\n$$\\alpha^2(\\alpha + \\beta + 1) = \\alpha^3 + \\alpha^2\\beta + \\alpha^2$$\n\nSubtract:\n\n$$\\alpha^3 + \\alpha^2\\beta + \\alpha^2 + \\alpha\\beta - (\\alpha^3 + \\alpha^2\\beta + \\alpha^2) = \\alpha\\beta$$\n\nTherefore:\n\n$$\\text{Var}(\\pi) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}$$\n\n✅ **Confirmed**:\n$$\\text{Var}(\\pi) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}$$\n\n------------------------------------------------------------------------\n\n::: {style=\"font-size: 150%\"}\n**Final Results**\n:::\n\n-   **Mean**: $E(\\pi) = \\frac{\\alpha}{\\alpha + \\beta}$\n-   **Mode**: $\\text{Mode}(\\pi) = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}$\n    (for $\\alpha > 1, \\beta > 1$)\n-   **Variance**:\n    $\\text{Var}(\\pi) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}$\n\nThese match the standard formulas for the Beta distribution.\n\n✅ **All formulas confirmed**.\n::::::::::\n:::::::::::::\n\n#### Interpreting prior\n\n:::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-interpreting-priors}\n: Interpreting priors\n:::\n::::\n\n::: my-exercise-container\nWhat do you call a sweet carbonated drink: pop, soda, coke, or something\nelse? Let $\\pi$ be the proportion of U.S. residents that prefer the term\n“pop.” Two different beverage salespeople from different regions of the\ncountry have different priors for $\\pi$. The first salesperson works in\nNorth Dakota and specifies a $\\text{Beta}(8,2)$ prior. The second works\nin Louisiana and specifies a $\\text{Beta}(1,20)$ prior.\n\n**a) Calculate the prior mean, mode, standard deviation of** $\\pi$ for\nboth salespeople\n\n\n::: {#tbl-003-summarizing-salespeople .cell tbl-cap='Prior mean, mode, standard deviation of $\\pi$ for both salespeople.'}\n\n```{.r .cell-code}\nsales_person1 <- bayesrules::summarize_beta(8, 2)\nsales_person2 <- bayesrules::summarize_beta(1, 20)\n\ncombined <- dplyr::full_join(\n  sales_person1, \n  sales_person2,\n  by = dplyr::join_by(mean, mode, var, sd)\n  ) \n\nmodel = c(\"Beta(8, 2)\",\n          \"Beta(1, 20)\"\n          )\n\n\ndf_combined <- tibble::tibble(model) |>\n  dplyr::bind_cols(combined) |> \n  knitr::kable(digits = 4)\n\ndf_combined\n```\n\n::: {.cell-output-display}\n\n\n|model       |   mean|  mode|    var|     sd|\n|:-----------|------:|-----:|------:|------:|\n|Beta(8, 2)  | 0.8000| 0.875| 0.0145| 0.1206|\n|Beta(1, 20) | 0.0476| 0.000| 0.0021| 0.0454|\n\n\n:::\n:::\n\n\n**b) Plot the prior pdfs for both salespeople**\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- bayesrules::plot_beta(8, 2)\np2 <- bayesrules::plot_beta(1, 20)\n\nlibrary(patchwork)\np1 + p2 + plot_layout(ncol = 2)\n```\n\n::: {.cell-output-display}\n![Prior for both salespeople](003-beta-binomial_files/figure-html/fig-plot-priors-salespeople-1.png){#fig-plot-priors-salespeople width=672}\n:::\n:::\n\n\n**c) Compare, in words, the salespeople’s prior understandings about the\nproportion of U.S. residents that say “pop”**\n\n-   The salesperson from North Dakota is convinced that about 80% or\n    more of all people prefer the term \"pop\". But he agrees that there\n    is a tiny chance that he is wrong and the proportion of people using\n    \"pop\" is only 25%. His assumption reflects his regional familiarity\n    with the term.\n-   In contrast the salesperson from Louisiana believes that no or very\n    few persons (about only 5%) uses the term \"pop\". He is secure that\n    not even 25% of the population uses \"\"pop\". This believe aligns with\n    the Southern preference for \"soda\" or \"coke.\"\n\nThe two different priors reflect the regional linguistic preferences and\nthe salespeople's confidence in their local terminology.\n:::\n::::::\n\n### Practice: Beta-Binomial models\n\n#### Different priors, different posteriors\n\nContinuing @exr-003-interpreting-priors, we poll 50 U.S. residents and\n12 (24%) prefer the term “pop.”\n\n:::::::::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-diferent-priors-different-posteriors}\n: Different priors, different posteriors\n:::\n::::\n\n::::::::: my-exercise-container\n::: {style=\"font-size: 150%\"}\na)  Specify the unique posterior model of $\\pi$ for both salespeople\n:::\n\n(We encourage you to construct these posteriors from scratch.)\n\nI am going to use the formula from @eq-003-beta-binomial-formula. For my\nunderstanding in @nte-003-calculate-posterior-mean-sd-beta-michelle I\nhave already calculated an example in two versions. One from scratch,\ntaking the beta-binomial-formula, the other one starting with the\ncalculation of Beta posterior from the Beta prior and then constructing\nthe Beta-Binomial from there.\n\n::: {style=\"font-size: 130%\"}\n**Version 1**\n:::\n\nAt first I have to calculate the Beta-Binomial model using\n@eq-003-beta-binomial-formula to get the value for the posterior Beta.\n\n**Salesperson from North Dakota**\n\n$$\n\\begin{align*}\n\\pi \\mid (Y = y) &\\sim \\text{Beta}(\\alpha + y, \\beta + n − y) \\\\\n&\\sim \\text{Beta}(8 + 12, 2 + 50 − 12) \\\\\n&\\sim \\bf{\\text{Beta}(20, 40)}\n\\end{align*}\n$$\n\nThen I can calculate with the posterior Beta the measures for the\ncentral tendency using @eq-003-beta-mean-mode, @eq-003-beta-variance and\n@eq-003-beta-sd:\n\n$$\n\\begin{align*}\n\\text{E}(\\pi) &= \\frac{\\alpha}{\\alpha + \\beta} = \\frac{20}{20+40} = \\frac{20}{60} \\approx \\bf{0.3333} \\\\\n\\text{Mode} &= \\frac{\\alpha - 1}{\\alpha + \\beta - 2} = \\frac{20 - 1}{20 + 40 - 2} = \\frac{19}{58} = \\bf{0.3275862}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n\\text{SD}(\\pi) &= \\sqrt{\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 \\cdot (α + β + 1)}} \n=  \\sqrt{\\frac{20 \\cdot 40}{(20 + 40)^2 \\cdot (20 + 40 + 1)}} \\\\\n&= \\sqrt{\\frac{800}{(60)^2 \\cdot (61)}} \n= \\sqrt{\\frac{800}{3600 \\cdot 61}}\n= \\sqrt{\\frac{800}{219600}} \\\\\n&= \\sqrt{0.003642987} = \\bf{0.06035716} \\\\\n\\end{align*}\n$$\n\n**Salesperson from Luisiana**\n\n$$\n\\begin{align*}\n\\pi \\mid (Y = y) &\\sim \\text{Beta}(\\alpha + y, \\beta + n − y) \\\\\n&\\sim \\text{Beta}(1 + 12, 20 + 50 − 12) \\\\\n&\\sim \\bf{\\text{Beta}(13, 58)}\n\\end{align*}\n$$\n\nThen I can calculate with the posterior Beta the measures for the\ncentral tendency using @eq-003-beta-mean-mode, @eq-003-beta-variance and\n@eq-003-beta-sd:\n\n$$\n\\begin{align*}\n\\text{E}(\\pi) &= \\frac{\\alpha}{\\alpha + \\beta} = \\frac{13}{13+58} = \\frac{13}{71} \\approx \\bf{0.1831} \\\\\n\\text{Mode} &= \\frac{\\alpha - 1}{\\alpha + \\beta - 2} = \\frac{13 - 1}{13 + 58 - 2} = \\frac{12}{69} = \\bf{0.173913}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\nSD(\\pi) &= \\sqrt{\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 \\cdot (α + β + 1)}} \n=  \\sqrt{\\frac{13 \\cdot 58}{(13 + 58)^2 \\cdot (13 + 58 + 1)}} \\\\\n&= \\sqrt{\\frac{754}{(71)^2 \\cdot (72)}} \n= \\sqrt{\\frac{754}{5041 \\cdot 72}}\n= \\sqrt{\\frac{754}{362952}} \\\\\n&= \\sqrt{0.00207741} = \\bf{0.04557861} \\\\\n\\end{align*}\n$$\n\n::: {style=\"font-size: 130%\"}\n**Version 2**\n:::\n\nUsing the integrated formulae @eq-003-beta-binomial-mean-mode-variance\nfor the Beta-Binomial model which are specified in\n@sec-003-beta-binomial-model.\n\n**Salesperson from North Dakota**\n\n$$\n\\begin{align*}\n\\text{E}(\\pi \\mid Y = y) &=  \\frac{\\alpha + y}{\\alpha + \\beta + n} = \\frac{8 + 12}{8 + 2 + 50} = \\frac{20}{60} \\approx \\bf{0.3333} \\\\\n\\text{Mode}(\\pi \\mid Y = y) &=  \\frac{\\alpha + y − 1}{\\alpha + \\beta + n − 2} = \\frac{8 + 12 -1}{8 + 2+ 50 - 2} = \\frac{19}{58} = \\bf{0.3275862}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n\\text{SD}(\\pi \\mid Y = y) &=  \\sqrt{\\frac{(\\alpha + y)(\\beta + n − y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n + 1)}} \\\\\n&= \\sqrt{\\frac{(8 + 12)(2 + 50 − 12)}{(8 + 2 + 50)^2(8 + 2 + 50 + 1)}} \\\\\n&= \\sqrt{\\frac{20 \\cdot 40}{3600 \\cdot 61}} = \\sqrt{\\frac{800}{219600}} = \\sqrt{0.003642987} \\\\\n&= \\bf{0.06035716}\n\\end{align*}\n$$\n\n**Salesperson from Luisiana**\n\n$$\n\\begin{align*}\n\\text{E}(\\pi \\mid Y = y) &=  \\frac{\\alpha + y}{\\alpha + \\beta + n} = \\frac{1 + 12}{1 + 20 + 50} = \\frac{13}{71} = \\bf{0.1830986} \\\\\n&=   \\\\\n\\text{Mode}(\\pi \\mid Y = y) &=  \\frac{\\alpha + y − 1}{\\alpha + \\beta + n − 2} = \\frac{1 + 12 - 1}{1 + 20 + 50 − 2} = \\frac{12}{69} = \\bf{0.173913}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n\\text{SD}(\\pi \\mid Y = y) &=  \\sqrt{\\frac{(\\alpha + y)(\\beta + n − y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n + 1)}} \\\\\n&=  \\sqrt{\\frac{(1 + 12)(20 + 50 − 12)}{(1 + 20 + 50)^2(1 + 20 + 50 + 1)}} \\\\\n&=  \\sqrt{\\frac{13 \\cdot 58}{(71)^2(72)}} = \\sqrt{\\frac{754}{5041 \\cdot 72}} = \\sqrt{\\frac{754}{362952}} \\\\\n&= \\sqrt{0.00207741} = \\bf{0.04557861}\n\\end{align*}\n$$\n\n::: {style=\"font-size: 130%\"}\n**Version 3**\n:::\n\nUsing the `bayesrules::summarize_beta_binomial()` function:\n\n**Salesperson from North Dakota**\n\n\n::: {#tbl-003-exercise-beta-binomial-north-dakota .cell tbl-cap='Prior and posterior model for salesperson from North Dakota'}\n\n```{#lst-003-exercise-beta-binomial-north-dakota .r .cell-code  lst-cap=\"Prior and posterior model (North Dakota)\"}\nbayesrules::summarize_beta_binomial(\n  alpha = 8, \n  beta = 2, \n  y = 12, \n  n = 50) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|   mean|   mode|    var|     sd|\n|:---------|-----:|----:|------:|------:|------:|------:|\n|prior     |     8|    2| 0.8000| 0.8750| 0.0145| 0.1206|\n|posterior |    20|   40| 0.3333| 0.3276| 0.0036| 0.0604|\n\n\n:::\n:::\n\n\n**Salesperson from Luisiana**\n\n\n::: {#tbl-003-exercise-beta-binomial-luisiana .cell tbl-cap='Prior and posterior model for salesperson from Luisiana'}\n\n```{#lst-003-exercise-beta-binomial-luisiana .r .cell-code  lst-cap=\"Prior and posterior model (Luisiana)\"}\nbayesrules::summarize_beta_binomial(\n  alpha = 1, \n  beta = 20, \n  y = 12, \n  n = 50) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|   mean|   mode|    var|     sd|\n|:---------|-----:|----:|------:|------:|------:|------:|\n|prior     |     1|   20| 0.0476| 0.0000| 0.0021| 0.0454|\n|posterior |    13|   58| 0.1831| 0.1739| 0.0021| 0.0456|\n\n\n:::\n:::\n\n\n::: {style=\"font-size: 150%\"}\nb)  Plot the prior pdf, likelihood function, and posterior pdf for both\n    salespeople\n:::\n\n\n::: {#fig-003-exercise-dakota-luisiana .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nbayesrules::plot_beta_binomial(\n  alpha = 8,\n  beta = 2,\n  y = 12,\n  n = 50\n)\n\nbayesrules::plot_beta_binomial(\n  alpha = 1,\n  beta = 20,\n  y = 12,\n  n = 50\n)\n```\n\n::: {.cell-output-display}\n![Salesperson from North Dakota](003-beta-binomial_files/figure-html/fig-003-exercise-dakota-luisiana-1.png){#fig-003-exercise-dakota-luisiana-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Salesperson from Luisiana](003-beta-binomial_files/figure-html/fig-003-exercise-dakota-luisiana-2.png){#fig-003-exercise-dakota-luisiana-2 width=672}\n:::\n\nPrior pdf, likelihood function, and posterior pdf for both salespeople\n:::\n\n\n::: {style=\"font-size: 150%\"}\nc)  Compare the salespeople’s posterior understanding of $\\pi$\n:::\n\nThe assumptions of the salesperson from North Dakota has to decrease\nfrom previously 80% to 33% whereas the salesperson from Luisiana has to\nincrease it from 5% to about 18%. This has an converging effect as their\nwidely separated believes are now much closer than before the data came\nin.\n:::::::::\n::::::::::::\n\n#### Regular bike ridership\n\n:::::::::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-regular-bike-ridership}\n: Regular bike ridership\n:::\n::::\n\n::::::::: my-exercise-container\nA university wants to know what proportion of students are regular bike\nriders, $\\pi$, so that they can install an appropriate number of bike\nracks. Since the university is in sunny Southern California, staff think\nthat $\\pi$ has a mean of 1 in 4 students, and a mode of 5/22.\n\n**a) Specify and plot a Beta model that reflects the staff’s prior ideas\nabout** $\\pi$\n\n$$\n\\text{Beta}(6, 18)\n$$\n\n::::: columns\n::: {.column width=\"40%\"}\n\n::: {#tbl-003-exercise-3-11-a .cell tbl-cap='Summary of $\\text{Beta}(6, 18)$'}\n\n```{.r .cell-code}\nbayesrules::summarize_beta(6, 18) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n| mean|   mode|    var|     sd|\n|----:|------:|------:|------:|\n| 0.25| 0.2273| 0.0075| 0.0866|\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(6,18)\n```\n\n::: {.cell-output-display}\n![Density plot for $\\text{Beta}(6, 18)$](003-beta-binomial_files/figure-html/fig-003-exercise-3-11-a-1.png){#fig-003-exercise-3-11-a width=672}\n:::\n:::\n\n:::\n:::::\n\n**b) Among 50 surveyed students, 15 are regular bike riders. What is the\nposterior model for** $\\pi$?\n\n$$\n\\begin{align*}\n\\alpha = 6 \\\\ \n\\beta = 18\\\\ \ny = 15 \\\\ \nn = 50\n\\end{align*}\n$$ \n\nUsing @eq-003-beta-binomial-formula\n\n$$\n\\begin{align*}\n\\pi \\mid (Y = y) &\\sim \\text{Beta}(\\alpha + y, \\beta + n − y) \\\\\n&\\sim \\text{Beta}(6 + 15, 18 + 50 − 15) \\\\\n&\\sim \\text{Beta}(21, 53)\n\\end{align*}\n$$\n\n**c) What is the mean, mode, and standard deviation of the posterior\nmodel?**\n\n\n::: {#tbl-003-exercise-3-11-c .cell tbl-cap='mean, mode, and standard deviation of posterior model $\\text{Beta}(21, 53)$'}\n\n```{.r .cell-code}\nbayesrules::summarize_beta(21, 53) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|   mean|   mode|    var|     sd|\n|------:|------:|------:|------:|\n| 0.2838| 0.2778| 0.0027| 0.0521|\n\n\n:::\n:::\n\n\n**d) Does the posterior model more closely reflect the prior information\nor the data? Explain your reasoning.**\n\n::::: columns\n::: {.column width=\"50%\"}\n\n::: {#tbl-003-exercise-3-11-d .cell tbl-cap='Summary of the Beta-Binomial model for $\\alpha = 6, \\beta = 18, y = 15, n = 50)$'}\n\n```{.r .cell-code}\nbayesrules::summarize_beta_binomial(6, 18, y = 15, n = 50) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|   mean|   mode|    var|     sd|\n|:---------|-----:|----:|------:|------:|------:|------:|\n|prior     |     6|   18| 0.2500| 0.2273| 0.0075| 0.0866|\n|posterior |    21|   53| 0.2838| 0.2778| 0.0027| 0.0521|\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta_binomial(6, 18, y = 15, n = 50)\n```\n\n::: {.cell-output-display}\n![Density plot for the Beta-Binomial model for $\\alpha = 6, \\beta = 18, y = 15, n = 50)$](003-beta-binomial_files/figure-html/fig-003-exercise-3-11-d-1.png){#fig-003-exercise-3-11-d width=672}\n:::\n:::\n\n:::\n:::::\n\nSeen from @fig-003-exercise-3-11-d the posterior curve is more in line\nwith the data (scaled likelihood curve) than with the prior curve.\n\nThe prior assumption was that one of 4 students are regular bikers (=\n25%). But the data has shown that 15 of 50 students use their bike\nregularly (= 30%) . Inspecting @tbl-003-exercise-3-11-d shows that the\nmean has changed from 0.25 to 0.2838 and it is therefore nearer to the\ndata proportion of regular riders. (The middle mean between prior and\nbeta would be 0.275.)\n:::::::::\n::::::::::::\n\n#### Same-sex marriage\n\n:::::: my-exercise\n:::: my-exercise-header\n::: {#exr-003-same-sex-marriage}\n: Same sex marriage\n:::\n::::\n\n::: my-exercise-container\nA 2017 Gallup[^003-beta-binomial-2] survey found that 10.2% of LGBT\nadults in the U.S. were married to a same-sex spouse [@jones2022]. Now it’s the\n2020s, and Bayard guesses that $\\pi$, the percent of LGBT adults in the U.S.\nwho are married to a same-sex spouse, has most likely increased to about\n15% but could reasonably range from 10% to 25%.\n\n[^003-beta-binomial-2]: and not a Pew Research survey, as stated in the\n    book\n\n::: {.callout-note #nte-003-same-sex-marriage}\n\n###### New data show that same-sex marriage has declined\n\nA as matter of fact, a 2025 Gallup survey reports that same-sex marriage has decreased to about 8%. Only \"[o]ne in 12 LGBTQ+ Adults Married to Same-Sex Spouse\" [@jones2025] ($\\approx 8.33\\%$). Read more about the reasons of this decline and other details in [@sears-2025][^003-beta-binomial-3].\n\n[^003-beta-binomial-3]: Keep in mind that the figure of 59 of this report refers to the share of *cohabiting* same-sex couples who are married — that is, among LGBTQ+ individuals already in a same-sex relationship and living together, 59% are married. In contrast the \"one in 12\" (~8.3%) figure refers to the share of *all LGBTQ+ adults* who are married to a same-sex spouse — this includes all LGBTQ+ individuals, including those who are single, in opposite-sex relationships, or not cohabiting. Many LGBTQ+ adults are in opposite-sex relationships (22.8%) or single (51.1%), which dilutes the overall percentage married to a same-sex spouse across the entire LGBTQ+ population.\n\n:::\n\n\n**a) Identify and plot a Beta model that reflects Bayard’s prior ideas about $\\pi$**\n\nI experimented with the Shiny app. The nearest I found was \n\n$$\\text{Beta}(18 / 99)$$\n\nIt has a <a class='glossary' title='In Bayesian statistics, a credible interval is an interval within which an unobserved parameter value falls with a particular probability. It is an interval in the domain of a posterior probability distribution or a predictive distribution. The generalisation to multivariate problems is the credible region. Credible intervals are a Bayesian analog to confidence intervals in frequentist statistics. The interval is fixed (after data is observed), the parameter is random. Interpretation: ‘There is a 95% probability that the parameter lies within this interval, given the data.’ (Wikipedia and Brave-AI)'>CI</a> of [0.0946, 0.2242] with a mean of `bayesrules::summarize_beta(18, 99)$mean` = 0.1538462 and a mode of `bayesrules::summarize_beta(18, 99)$mode` = 0.1478261.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(18, 99)\n```\n\n::: {.cell-output-display}\n![Beta(18, 99)](003-beta-binomial_files/figure-html/fig-003-exercise-12a-1.png){#fig-003-exercise-12a width=50%}\n:::\n:::\n\n\n\n**b) Bayard wants to update his prior, so he randomly selects 90 US LGBT adults and 30 of them are married to a same-sex partner. What is the posterior model for $\\pi$?**\n\nUsing again @eq-003-beta-binomial-formula\n\n$$\n\\begin{align*}\n\\pi \\mid (Y = y) &\\sim \\text{Beta}(\\alpha + y, \\beta + n − y) \\\\\n&\\sim \\text{Beta}(18 + 30, 99 + 90 − 30) \\\\\n&\\sim \\text{Beta}(48, 159)\n\\end{align*}\n$$\n\n**c) Calculate the posterior mean, mode, and standard deviation of $\\pi$**\n\n\n::: {#tbl-003-exercise-13c .cell tbl-cap='Properties of $\\text{Beta-Binomial}{18, 99, 30, 90}'}\n\n```{.r .cell-code}\nbayesrules::summarize_beta_binomial(18, 99, 30, 90) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|   mean|   mode|    var|     sd|\n|:---------|-----:|----:|------:|------:|------:|------:|\n|prior     |    18|   99| 0.1538| 0.1478| 0.0011| 0.0332|\n|posterior |    48|  159| 0.2319| 0.2293| 0.0009| 0.0293|\n\n\n:::\n:::\n\n\n\n\n\n**d) Does the posterior model more closely reflect the prior information or the data? Explain your reasoning.**\n\nThe posterior model reflects approximately the prior information and data fifty-fifty. The prior assumed 15% and the data showed that in fact there were 33,3% LBGT-people married to the same sex. The mean of the posterior model lies with a mean of 23% roughly in the middle, perhaps a tiny bit more to prior side. This could be confirmed with the \n\n\n\n::: {.cell figh-width='3'}\n\n```{.r .cell-code}\nbayesrules::plot_beta_binomial(18, 99, 30, 90)\n```\n\n::: {.cell-output-display}\n![Prior, scaled likelihood and posterior model of $\\text{Beta-Binomial}(18, 99, 30, 90)$](003-beta-binomial_files/figure-html/fig-003-exercise-13d-1.png){#fig-003-exercise-13d width=672}\n:::\n:::\n\n\n\n::: {.callout-note #nte-003-exercise-new-gallup-research}\n\nI am interested to know how the new Gallup research changes Bayard’s prior. Gallup interviews a minimum of 1,000 U.S. adults aged 18. So we have at least $n = 1000$ and $y = 83$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| fig-height: 3\n#| figh-width: 3\nbayesrules::plot_beta_binomial(18, 99, 83, 1000)\n```\n\n::: {.cell-output-display}\n![Prior, scaled likelihood and posterior model of $\\text{Beta-Binomial}(18, 99, 83, 1000)$, including the newest Gallup research [@jones2025]](003-beta-binomial_files/figure-html/fig-003-exercise-13d-2-1.png){#fig-003-exercise-13d-2 width=672}\n:::\n:::\n\n\n\n::: {#tbl-003-exercise-13c-2 .cell tbl-cap='Properties of $\\text{Beta-Binomial}{18, 99, 83, 1000}'}\n\n```{.r .cell-code}\nbayesrules::summarize_beta_binomial(18, 99, 83, 1000) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|   mean|   mode|    var|     sd|\n|:---------|-----:|----:|------:|------:|------:|------:|\n|prior     |    18|   99| 0.1538| 0.1478| 0.0011| 0.0332|\n|posterior |   101| 1016| 0.0904| 0.0897| 0.0001| 0.0086|\n\n\n:::\n:::\n\n\n\n\nThe high number of respondents results in a very small CI, e.g., we can be pretty sure that the proportion of LGBT adults in the U.S. married to a same-sex spouse is around 9%. Bayard’s optimistic prior has practically no effect. He has to face the fact the the proportion of LGBT adults in the U.S. married to a same-sex spouse not increased but decreased slightly.\n:::\n\n:::\n::::::\n\n\n#### Knowing someone who is transgender\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\n:::::: {#exr-003-knowing-transgender}\n: Knowing someone who is transgender\n::::::\n:::\n::::{.my-exercise-container}\nA September 2016 Pew Research survey found that 30% of U.S. adults are aware that they know someone who is transgender [@mitchell2016][^003-beta-binomial-4] It is now the 2020s, and Sylvia believes that the current percent of people who know someone who is transgender, $\\pi$, has increased to somewhere between 35% and 60%.\n\na) Identify and plot a Beta model that reflects Sylvia’s prior ideas about $\\pi$. b) Sylvia wants to update her prior, so she randomly selects 200 US adults and 80 of them are aware that they know someone who is transgender. Specify and plot the posterior model for $\\pi$.\nc) What is the mean, mode, and standard deviation of the posterior model? \nd) Describe how the prior and posterior Beta models compare.\n\n\n::: {.callout-note #nte-003-exercise-3-13}\n###### I skip this exercise because it is very similar to @exr-003-regular-bike-ridership and @exr-003-same-sex-marriage.\n:::\n\n\n\n[^003-beta-binomial-4]: The URL in the book is not valid.\n\n::::\n:::::\n\n#### Summarizing the Beta-Binomial: Take I\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\n:::::: {#exr-003-summary-beta-binomial-1}\n: Summarizing the Beta-Binomial: Take I\n::::::\n:::\n::::{.my-exercise-container}\n\nWrite the corresponding input code for the `bayesrules::summarize_beta_binomial()` output below.\n\n![Reconstruct code for this table](img/003-exr-14.png){#fig-exr-003-14 \nfig-alt=\"Reconstruct code for this table\" fig-align=\"center\" \nwidth=\"70%\"}\n\n***\n\n**Beta-Binomial Model Posterior Update**  \nGiven a **Beta(2, 3)** prior and a **Beta(11, 24)** posterior in a binomial model, the observed data can be recovered using the conjugate updating rule:\n\n$$\n\\pi \\mid Y = y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\n$$\n\nFrom the prior:  \n- $\\alpha = 2$  \n- $\\beta = 3$\n\nFrom the posterior:  \n- $\\alpha + y = 11$ → $y = 11 - 2 = 9$  \n- $\\beta + n - y = 24$ → $3 + n - 9 = 24$ → $n = 24 + 6 = 30$\n\nThus:  \n- **$y = 9$**  \n- **$n = 30$**\n\n\n::: {#tbl-003-summary-beta-binomial-1 .cell tbl-cap='Solution for Exercise 14'}\n\n```{.r .cell-code}\nbayesrules::summarize_beta_binomial(2, 3, 9, 30) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|   mean|   mode|   var|     sd|\n|:---------|-----:|----:|------:|------:|-----:|------:|\n|prior     |     2|    3| 0.4000| 0.3333| 0.040| 0.2000|\n|posterior |    11|   24| 0.3143| 0.3030| 0.006| 0.0774|\n\n\n:::\n:::\n\n\n\n\n::::\n:::::\n\n#### Summarizing the Beta-Binomial: Take II\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\n:::::: {#exr-003-summary-beta-binomial-2}\n: Summarizing the Beta-Binomial: Take II\n::::::\n:::\n::::{.my-exercise-container}\n\nWrite the corresponding input code for the `bayesrules::summarize_beta_binomial()` output below.\n\n![Reconstruct code for this table](img/003-exr-15.png){#fig-exr-003-15 \nfig-alt=\"Reconstruct code for this table\" fig-align=\"center\" \nwidth=\"70%\"}\n\n***\n\n**Beta-Binomial Model Posterior Update**  \nGiven a **Beta(1, 2)** prior and a **Beta(100, 3)** posterior in a binomial model, the observed data can be recovered using the conjugate updating rule:\n\n$$\n\\pi \\mid Y = y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\n$$\n\nFrom the prior:  \n- $\\alpha = 1$  \n- $\\beta = 2$\n\nFrom the posterior:  \n- $\\alpha + y = 100$ → $y = 100 - 1 = 99$  \n- $\\beta + n - y = 3$ → $2 + n - 99 = 3$ → $n = 3 + 99 -2 = 100$\n\nThus:  \n- **$y = 99$**  \n- **$n = 100$**\n\n\n::: {#tbl-003-summary-beta-binomial-2 .cell tbl-cap='Solution for Exercise 15'}\n\n```{.r .cell-code}\nbayesrules::summarize_beta_binomial(1, 2, 99, 100) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|   mean|   mode|    var|     sd|\n|:---------|-----:|----:|------:|------:|------:|------:|\n|prior     |     1|    2| 0.3333| 0.0000| 0.0556| 0.2357|\n|posterior |   100|    3| 0.9709| 0.9802| 0.0003| 0.0165|\n\n\n:::\n:::\n\n\n\n\n::::\n:::::\n\n#### Plotting the Beta-Binomial: Take I\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\n:::::: {#exr-003-plot-beta-binomial-1}\n: Plotting the Beta-Binomial: Take I\n::::::\n:::\n::::{.my-exercise-container}\nBelow is output from `bayesrules::plot_beta_binomial()` function.\n\n![Beta-Binomial model to explain and reconstruct](img/003-exr-16.png){#fig-003-exr-16 \nfig-alt=\"Density Beta-Binomial plot with a prior mode at 1, a scaled likelihood at around 0.22 and a posterior of 0.75\" fig-align=\"center\" \nwidth=\"70%\"}\n\n***\n\n**a) Describe and compare both the prior model and likelihood function in words** \n\nThe prior reflects a very optimistic assumption, practically believing that every trial shows the event we are looking for. The data that came in, corrected this very informative prior and settled the mean and mode at about 0.22. \n\n**b) Describe the posterior model in words. Does it more closely agree with the data (as reflected by the likelihood function) or the prior?** \n\nThe posterior model is still pretty positive with a mean and mode about 0.75. It is much nearer to the prior than to the data, indicating that not much data came in.\n\n**c) Provide the specific `bayesrules::plot_beta_binomial()` code you would use to produce a similar plot.**\n\n::: {#big-text-step-1 style=\"font-size: 150%\"}\n**Step 1: Prior mode = 1** \n:::\n\n\nThe mode of a Beta(α, β) distribution is:\n\n$$\\text{Mode} = \\frac{\\alpha - 1}{\\alpha + \\beta - 2} \\quad (\\text{for } \\alpha > 1, \\beta > 1)$$\n\nSet mode = 1:\n\n$$\\frac{\\alpha - 1}{\\alpha + \\beta - 2} = 1$$\n\n$$\\alpha - 1 = \\alpha + \\beta - 2$$\n\n$$\\alpha - 1 - \\alpha + 2 = \\beta$$\n\n$$\\beta = 1$$\n\nSo we have $\\bf{\\beta = 1 \\text{ and } \\alpha > 1}$ (to be determined).\n\n::: {#big-text style=\"font-size: 150%\"}\n**Step 2: Likelihood peak = 0.22** \n:::\n\n\nThe binomial likelihood peaks at the observed proportion:\n\n$$\\text{Likelihood peak} = \\frac{y}{n} = 0.22$$\n\nChoose integers that satisfy this ratio. For simplicity:\n\n$$y = 11, \\quad n = 50 \\quad \\Rightarrow \\quad \\frac{11}{50} = 0.22 \\checkmark$$\n\n::: {#big-text style=\"font-size: 150%\"}\n**Step 3: Posterior mode = 0.75** \n:::\n\n\nIn the Beta-Binomial conjugate model, the posterior is:\n\n$$\\text{Posterior} = \\text{Beta}(\\alpha + y, \\beta + n - y)$$\n\nSubstituting our values (with β = 1, y = 11, n = 50):\n\n$$\\text{Posterior} = \\text{Beta}(\\alpha + 11, 1 + 50 - 11) = \\text{Beta}(\\alpha + 11, 40)$$\n\nThe posterior mode is:\n\n$$\\text{Mode}_{\\text{posterior}} = \\frac{(\\alpha + 11) - 1}{(\\alpha + 11) + 40 - 2} = \\frac{\\alpha + 10}{\\alpha + 49}$$\n\nSet this equal to 0.75:\n\n$$\\frac{\\alpha + 10}{\\alpha + 49} = 0.75$$\n\n\n::: {#big-text style=\"font-size: 110%\"}\n**Solve for α** \n:::\n\n$$\\alpha + 10 = 0.75(\\alpha + 49)$$\n\n$$\\alpha + 10 = 0.75\\alpha + 36.75$$\n\n$$0.25\\alpha = 26.75$$\n\n$$\\alpha = 107$$\n\n::: {#big-text style=\"font-size: 150%\"}\n**Final Answer** \n:::\n\n$\\quad\\Rightarrow$\n\n| Parameter | Value |\n|-----------|-------|\n| **Prior** | Beta(107, 1) |\n| **Data** | y = 11 successes, n = 50 trials |\n| **Posterior** | Beta(118, 40) |\n\n::: {style=\"font-size: 125%\"}\n**Verification** \n:::\n\n- Prior mode: $(107 - 1)/(107 + 1 - 2) = 106/106 = 1$ ✓\n- Likelihood peak: $11/50 = 0.22$ ✓\n- Posterior mode: $(118 - 1)/(118 + 40 - 2) = 117/156 = 0.75$ ✓\n\n::: {#big-text style=\"font-size: 125%\"}\n**R code to visualize** \n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta_binomial(alpha = 107, beta = 1, y = 11, n = 50)\n```\n\n::: {.cell-output-display}\n![Reconstructing @fig-003-exr-16 (First try)](003-beta-binomial_files/figure-html/fig-003-plot-beta-binomial-1a-1.png){#fig-003-plot-beta-binomial-1a width=480}\n:::\n:::\n\n\n::: {.callout-note #nte-003-plot-beta-binomial-1}\n###### How about the differences in the y-axis?\n\n@fig-003-plot-beta-binomial-1a correctly shows the shapes and modes of @fig-003-exr-16. But I wonder about the differences in the y-axis. In the original figure the magnitude of the prior is smaller, whereas the peaks of the distributions (scaled likelihood and posterior) are higher than in my replication. \n\nThe heights of the prior, likelihood, and posterior are scaled for visualization and are not directly comparable in magnitude, e.g., they are not interpretable in absolute terms. And they can't be changed independently from the other parameters. To get similar heights of the peaks as in @fig-003-exr-16 I need to change my model parameters slightly. \n\nAfter some trial and error it turned out that `bayesrules::plot_beta_binomial(alpha = 40, beta = 1, y = 3, n = 17)`is a better approximation to @fig-003-exr-16, but still not exactly the same.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta_binomial(alpha = 40, beta = 1, y = 3, n = 17)\n```\n\n::: {.cell-output-display}\n![Reconstructing @fig-003-exr-16 (Second try)](003-beta-binomial_files/figure-html/fig-003-plot-beta-binomial-1b-1.png){#fig-003-plot-beta-binomial-1b width=480}\n:::\n:::\n\n\nBesides my time consuming trial and error: What is the rational to get the prior height much lower and the likelihood and posterior somewhat higher?  \n\nTo systematically adjust the heights in your beta-binomial plot, you need to understand what controls the **density values** (y-axis heights) for each component:\n\n::: {#big-text style=\"font-size: 125%\"}\n**Key Factors Affecting Heights** \n:::\n\n**Prior height** (Beta distribution):\n- Controlled by `alpha` and `beta` parameters\n- Lower `alpha + beta` → flatter, lower peak\n- Higher `alpha + beta` → taller, sharper peak\n- Peak height ≈ proportional to `(alpha + beta)`\n\n**Likelihood height** (scaled):\n- The likelihood is scaled to integrate to 1 for visualization\n- Peak depends on `n` (sample size) and how concentrated the data is\n- Smaller `n` → lower, flatter likelihood\n- The actual peak location is at `y/n`\n\n**Posterior height** (Beta distribution):\n- Parameters are `alpha + y` and `beta + (n - y)`\n- Height follows same rules as prior: sum of parameters matters\n\n::: {#big-text style=\"font-size: 125%\"}\n**Systematic approach** \n:::\n\nTo get **lower prior** and **higher likelihood/posterior**:\n\n1. **Reduce prior strength**: Lower `alpha + beta` while keeping the ratio `alpha/(alpha+beta)` similar to maintain the prior mean\n   - Your example: 107+1=108 → 40+1=41 (much weaker prior)\n\n2. **Adjust sample size**: Smaller `n` with proportionally smaller `y` keeps the same `y/n` ratio but changes relative heights\n   - Your example: 11/50=0.22 → 3/17≈0.18 (similar location)\n\n3. **Calculate the ratio**: If you want prior height to be X% of original, scale `alpha` and `beta` by approximately `sqrt(X)` while adjusting `y` and `n` proportionally\n\n**Formula approach**: If you want to maintain the same peak locations but adjust heights, keep these ratios constant:\n- Prior peak: `alpha/(alpha+beta)` \n- Likelihood/posterior peak: `y/n`\n\nThen adjust the absolute values of `alpha+beta` and `n` to control heights.\n\n\n:::\n\n\n::::\n:::::\n\n\n#### Plotting the Beta-Binomial: Take II\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\n:::::: {#exr-003-plot-beta-binomial-2}\n: Plotting the Beta-Binomial: Take 2\n::::::\n:::\n::::{.my-exercise-container}\nBelow is output from `bayesrules::plot_beta_binomial()` function.\n\n![Beta-Binomial model to explain and reconstruct](img/003-exr-17.png){#fig-003-exr-17 \nfig-alt=\"Density Beta-Binomial plot with a prior mean at 1, a scaled likelihood at around 0.10 and a posterior of approximately 0.13\" fig-align=\"center\" \nwidth=\"70%\"}\n\n***\n\n**a) Describe and compare both the prior model and likelihood function in words** \n\nThe prior reflects a very vague assumption. It is represented by the well-known $\\text{Beta}(2, 2)$ prior (cf. with @fig-003-which-beta-1). The likelihood has its mean/mode approximately at $\\pi = 0.10$ and the CI is pretty small. \n\n**b) Describe the posterior model in words. Does it more closely agree with the data (as reflected by the likelihood function) or the prior?** \n\nBecause of the uninformed prior the posterior model reflects mostly the data. It overlaps big parts of the likelihood curve.\n\n**c) Provide the specific `bayesrules::plot_beta_binomial()` code you would use to produce a similar plot.**\n\nI think the code for this example is much easier to specify: \n\n- We know the prior (= $\\text{Beta}(2, 2)$) \n- We will choose for simplicity for the likelihood peak $\\frac{y}{n} = 0.1 = \\frac{1}{10}$\n- The posterior is \n\n$$\n\\begin{align*}\n\\text{Posterior} &= \\text{Beta}(\\alpha + y, \\beta + n - y) \\\\\n&= \\text{Beta}(2 + 1, 2 + 10 - 1) = \\text{Beta}(3, 11)\n\\end{align*}\n$$ \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta_binomial(2, 2, 1, 10)\n```\n\n::: {.cell-output-display}\n![Reconstructing @fig-003-exr-17 (Version 1)](003-beta-binomial_files/figure-html/fig-003-plot-beta-binomial-2a-1.png){#fig-003-plot-beta-binomial-2a width=480}\n:::\n:::\n\n\n\nIf we compare with @fig-003-exr-17 we can see that our data and posterior distribution is right skewed. We need to increase $\\alpha$ by incrementing the number of successes. So let's try with y = 3. To match the required peak at 0.1 we need 30 trials.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta_binomial(2, 2, 3, 30)\n```\n\n::: {.cell-output-display}\n![Reconstructing @fig-003-exr-17 (Version 2)](003-beta-binomial_files/figure-html/fig-003-plot-beta-binomial-2b-1.png){#fig-003-plot-beta-binomial-2b width=480}\n:::\n:::\n\nThis is exact the required configuration resp. R code.\n::::\n:::::\n\n#### More Beta-Binomial\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\n:::::: {#exr-003-more-beta-binomial}\n: More Beta-Binomial\n::::::\n:::\n::::{.my-exercise-container}\n\n::: {#big-text style=\"font-size: 125%\"}\n**a) Part 1 of the task**\n:::\n\n\n\nPatrick has a Beta(3,3) prior for $\\pi$, the probability that someone in their town attended a protest in June 2020. In their survey of 40 residents, 30 attended a protest. Summarize Patrick’s analysis using `summarize_beta_binomial()` and `plot_beta_binomial()`.  \n\n::::: columns\n::: {.column width=\"50%\"}\n\n::: {#tbl-003-exercise-3-18-a .cell tbl-cap='Summary of the Beta-Binomial model for $\\alpha = 3, \\beta = 3, y = 30, n = 40)$'}\n\n```{.r .cell-code}\nbayesrules::summarize_beta_binomial(3, 3, y = 30, n = 40) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|   mean|   mode|    var|     sd|\n|:---------|-----:|----:|------:|------:|------:|------:|\n|prior     |     3|    3| 0.5000| 0.5000| 0.0357| 0.1890|\n|posterior |    33|   13| 0.7174| 0.7273| 0.0043| 0.0657|\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta_binomial(3, 3, y = 30, n = 40)\n```\n\n::: {.cell-output-display}\n![Density plot for the Beta-Binomial model for $\\alpha = 3, \\beta = 3, y = 30, n = 40)$](003-beta-binomial_files/figure-html/fig-003-exercise-3-18-a-1.png){#fig-003-exercise-3-18-a width=672}\n:::\n:::\n\n:::\n:::::\n\n::: {#big-text style=\"font-size: 125%\"}\n**b) Part 2 of the task**\n:::\n\n\n\nHarold has the same prior as Patrick, but lives in a different town. In their survey, 15 out of 20 people attended a protest. Summarize Harold’s analysis using `summarize_beta_binomial()` and `plot_beta_binomial()`.  \n\n\n\n::::: columns\n::: {.column width=\"50%\"}\n\n::: {#tbl-003-exercise-3-18-b .cell tbl-cap='Summary of the Beta-Binomial model for $\\alpha = 18, \\beta = 8, y = 15, n = 20)$'}\n\n```{.r .cell-code}\nbayesrules::summarize_beta_binomial(3, 3, y = 15, n = 20) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | alpha| beta|   mean|   mode|    var|     sd|\n|:---------|-----:|----:|------:|------:|------:|------:|\n|prior     |     3|    3| 0.5000| 0.5000| 0.0357| 0.1890|\n|posterior |    18|    8| 0.6923| 0.7083| 0.0079| 0.0888|\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta_binomial(3, 3, y = 15, n = 20)\n```\n\n::: {.cell-output-display}\n![Density plot for the Beta-Binomial model for $\\alpha = 18, \\beta = 8, y = 15, n = 20)$](003-beta-binomial_files/figure-html/fig-003-exercise-3-18-b-1.png){#fig-003-exercise-3-18-b width=672}\n:::\n:::\n\n:::\n:::::\n\n::: {#big-text style=\"font-size: 125%\"}\n**c) Part 3 of the task: How do Patrick and Harold’s posterior models compare? Briefly explain what causes these similarities and differences.**\n:::\n\n\n\nPatrick and Harold do not only have the same prior ($\\text{Beta}(3, 3)$) but also the same success rate of $\\frac{y}{n} = 0.75 = \\frac{30}{40} = \\frac{15}{20}$. But in Harold’s analysis the data do not have as much weight as in Patrick’s. This is the reason that -- although both posteriors are centered around 0.75 --, the posterior from the larger sample (40 trials) is narrower and more peaked, reflecting greater certainty in the estimate.\n\n**A more detailed answer**\n\nEven though both have the same proportion (y/n = 0.75) and the same prior Beta(3,3), they produce different posterior distributions because **Bayesian updating depends on the amount of data, not just the proportion**.\n\nHere's why they differ:\n\n::: {#big-text style=\"font-size: 110%\"}\n**Example 1:** \n:::\n\n- Beta(3,3) + 30 successes in 40 trials\n- Posterior using @eq-003-beta-binomial-formula:\n\n\n$$\n\\begin{align*}\n\\pi \\mid (Y = y) &\\sim \\text{Beta}(\\alpha + y, \\beta + n − y) \\\\\n&\\sim \\text{Beta}(3 + 30, 3 + 40 − 30) \\\\\n&\\sim \\text{Beta}(33, 13) \\\\\n\\end{align*}\n$$ \n\n\n\n::: {#big-text style=\"font-size: 110%\"}\n**Example 2::** \n:::\n\n- Beta(3,3) + 15 successes in 20 trials\n- Posterior using @eq-003-beta-binomial-formula:\n\n\n$$\n\\begin{align*}\n\\pi \\mid (Y = y) &\\sim \\text{Beta}(\\alpha + y, \\beta + n − y) \\\\\n&\\sim \\text{Beta}(3 + 15, 3 + 20 − 15) \\\\\n&\\sim \\text{Beta}(18, 8) \\\\\n\\end{align*}\n$$ \n\n\n::: {#big-text style=\"font-size: 110%\"}\n**Key differences:** \n:::\n\n**More data = More certainty:** The first example has twice as much data (40 vs 20 trials), so the posterior is more concentrated around 0.75 with lower variance.\n\n**Effective sample size**: The $\\text{Beta}(\\alpha, \\beta) = \\text{Beta}(3,3)$ prior is mathematically equivalent to having observed $\\alpha$ successes and $\\beta$ failures in a binomial experiment. So a $\\text{Beta}(3,3)$ prior is like having seen 3 successes and 3 failures in 6 hypothetical trials.\n\nIn Bayesian analysis, this effective sample size represents the **weight of the prior information** relative to the actual data. \n\nSo when we have:\n- Example 1: 40 data points + prior equivalent to 6 observations\n- Example 2: 20 data points + prior equivalent to 6 observations\n\nThe prior has a **larger relative influence** in Example 2 because 6 prior observations represent a larger proportion of the total information (6/26 ≈ 23%) compared to Example 1 (6/46 ≈ 13%). This is why the posterior in Example 2 is more influenced by the prior.\n\nIn Example 1, the prior has less influence relative to 40 data points, so the posterior is pulled more strongly toward the observed 0.75. Beta(33,13) will have smaller variance than Beta(18,8), reflecting greater confidence with more observations.\n\nThis illustrates a fundamental Bayesian principle: **the amount of evidence matters, not just the proportion!**\n::::\n:::::\n\n\n## Glossary Entries {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Bayes’ Theorem </td>\n   <td style=\"text-align:left;\"> This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes' Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Beta Probability Model </td>\n   <td style=\"text-align:left;\"> The Beta Probability Model is a continuous probability distribution defined on the interval [0, 1], widely used to model probabilities, proportions, or fractions.  It is particularly valuable in Bayesian inference as the conjugate prior for binomial, Bernoulli, and geometric distributions. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Beta-Binomial-Model </td>\n   <td style=\"text-align:left;\"> The Beta-Binomial Bayesian Model is a fundamental framework in Bayesian statistics used to estimate a proportion π (e.g., success probability) when data are binary outcomes (success/failure) across n independent trials. The Beta-Binomial Bayesian Model combines a Beta prior with a Binomial likelihood, resulting in a Beta posterior due to conjugacy.  This makes it ideal for estimating unknown proportions (e.g., success rates) from binary data. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bimodality </td>\n   <td style=\"text-align:left;\"> A bimodal distribution is a type of probability distribution that exhibits two distinct peaks or modes, indicating the presence of two separate groups, processes, or subpopulations within a single dataset.  These peaks represent the most frequently occurring values or ranges in the data, separated by a visible valley or trough. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Binomial Model </td>\n   <td style=\"text-align:left;\"> Binomial model is a foundational framework in Bayesian statistics used to estimate the probability of success in a series of independent trials. It combines a binomial likelihood — which models the number of successes in a fixed number of trials—with a beta prior on the success probability, resulting in a beta posterior. This conjugate relationship allows for analytical computation and intuitive interpretation. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Claude Haiku </td>\n   <td style=\"text-align:left;\"> Claude Haiku is Anthropic’s smallest, fastest, and most cost-effective model in the Claude family, designed for high-speed, low-latency tasks while maintaining strong performance on coding, computer use, and agent workflows. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Claude Sonnet 4.5 </td>\n   <td style=\"text-align:left;\"> Claude Sonnet 4.5 is Anthropic's most advanced model in the Sonnet series, released on September 29, 2025, and is now the default model for general use across Anthropic’s platforms, including claude.ai, mobile apps, and the API. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Conjugate Prior </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> CPMF </td>\n   <td style=\"text-align:left;\"> The Conditional Probability Mass Function (PMF) is the probability distribution of a discrete random variable X given that another discrete random variable Y has taken a specific value y. It is defined using the joint probability mass function and the marginal probability mass function. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Credible Interval </td>\n   <td style=\"text-align:left;\"> In Bayesian statistics, a credible interval is an interval within which an unobserved parameter value falls with a particular probability. It is an interval in the domain of a posterior probability distribution or a predictive distribution. The generalisation to multivariate problems is the credible region. Credible intervals are a Bayesian analog to confidence intervals in frequentist statistics. The interval is fixed (after data is observed), the parameter is random. Interpretation: 'There is a 95% probability that the parameter lies within this interval, given the data.' (Wikipedia and Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hyperparameter </td>\n   <td style=\"text-align:left;\"> In the Bayesian framework, a hyperparameter is a parameter of a prior distribution.  It defines the shape, location, or scale of the prior belief about a model parameter before observing any data. Unlike model parameters, which are estimated from the data (e.g., the mean or variance of a distribution), hyperparameters are set before the analysis begins and are not directly learned from the data. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Joint_Probability_Density_Function </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Joint_Probability_Mass_Function </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Kernel </td>\n   <td style=\"text-align:left;\"> The basic idea of a kernel densitiy estimate is to have a set of data to predict future results. The kernel idea is to create a similarity function (called a kernel function) between any two sets of inputs. For any new set of inputs, we predict by taking a weighted average of past results, weighted by the similarity of the past inputs to the current inputs. So in a essence a kernel is a function used to measure similarity between data points in a transformed feature space. Intuitively, you can think of a kernel as a way to compute the \"distance\" or \"similarity\" between points without explicitly mapping them into that higher-dimensional space. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Likelihood-Function </td>\n   <td style=\"text-align:left;\"> The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> LLMs </td>\n   <td style=\"text-align:left;\"> Large Language Models (LLMs) are advanced artificial intelligence systems trained on massive amounts of text data to understand, generate, and manipulate human language. LLMs function as statistical prediction machines, learning patterns in language by predicting the next word in a sequence. This allows them to perform diverse tasks such as answering questions, writing essays, translating languages, summarizing documents, generating code, and even engaging in conversation. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Normal-Normal Conjugate Family </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PMFx </td>\n   <td style=\"text-align:left;\"> A probability mass function (PMF) is a mathematical function that calculates the probability a discrete random variable will be a specific value. PMFs also describe the probability distribution for the full range of values for a discrete variable. Probability mass functions find the LIKELIHOOD of a particular outcome. Using a PMF to calculate the likelihoods for all possible values of the discrete variable produces its PROBABILITY DISTRIBUTION.(&lt;a href=\"https://statisticsbyjim.com/probability/probability-mass-function/\"&gt;Statistics By Jim&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Posterior_Model </td>\n   <td style=\"text-align:left;\"> The posterior model represents the updated beliefs about the parameters of a model after incorporating observed data. It is derived by combining the prior distribution, which reflects initial beliefs about the parameter, with the likelihood of the observed data using Bayes' theorem. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Prior_Model </td>\n   <td style=\"text-align:left;\"> In Bayesian statistics, a prior model refers to the specification of a prior distribution, which represents the initial beliefs or assumptions about the parameters of a statistical model before observing any data. This prior distribution quantifies the uncertainty about the parameters. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Probability_Density_Function </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Probability_Mass_Function </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Deviation </td>\n   <td style=\"text-align:left;\"> The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter $\\sigma$ (sigma), for the population standard deviation, or the Latin letter $s$ for the sample standard deviation. ([Wikipedia] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Uniform Distribution </td>\n   <td style=\"text-align:left;\"> The standard uniform distribution is a continuous probability distribution where all values in the interval [0, 1] are equally likely.  It is a special case of the continuous uniform distribution with parameters a = 0 and b = 1, often denoted as U(0, 1). (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Variance var </td>\n   <td style=\"text-align:left;\"> Variance is the squared deviation from the mean of a random variable. The variance is also often defined as the square of the standard deviation. Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. It is the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by `σ`, `σ^2`, VAR(x), var(x) or V(x). ([Wikipedia](https://en.wikipedia.org/wiki/Variance)) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Session Info {.unnumbered}\n\n::::: my-r-code\n::: my-r-code-header\nSession Info\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.5.2 (2025-10-31)\n#>  os       macOS Tahoe 26.2\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2026-02-15\n#>  pandoc   3.9 @ /opt/homebrew/bin/ (via rmarkdown)\n#>  quarto   1.8.27 @ /usr/local/bin/quarto\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  ! package        * version  date (UTC) lib source\n#>  P abind            1.4-8    2024-09-12 [?] RSPM\n#>  P backports        1.5.0    2024-05-23 [?] RSPM\n#>    base64enc        0.1-6    2026-02-02 [1] CRAN (R 4.5.2)\n#>  P bayesplot        1.15.0   2025-12-12 [?] RSPM\n#>    bayesrules     * 0.0.3    2026-01-20 [1] RSPM\n#>  P boot             1.3-32   2025-08-29 [?] CRAN (R 4.5.2)\n#>    checkmate        2.3.4    2026-02-03 [1] RSPM\n#>  P class            7.3-23   2025-01-01 [?] CRAN (R 4.5.2)\n#>  P cli              3.6.5    2025-04-23 [?] CRAN (R 4.5.0)\n#>  P codetools        0.2-20   2024-03-31 [?] CRAN (R 4.5.2)\n#>  P colourpicker     1.3.0    2023-08-21 [?] RSPM\n#>  P commonmark       2.0.0    2025-07-07 [?] RSPM\n#>  P crosstalk        1.2.2    2025-08-26 [?] RSPM\n#>  P curl             7.0.0    2025-08-19 [?] RSPM\n#>  P digest           0.6.39   2025-11-19 [?] CRAN (R 4.5.2)\n#>    distributional   0.6.0    2026-01-14 [1] RSPM\n#>  P dplyr            1.2.0    2026-02-03 [?] RSPM\n#>  P DT               0.34.0   2025-09-02 [?] RSPM\n#>  P dygraphs         1.1.1.6  2018-07-11 [?] RSPM\n#>    e1071            1.7-17   2025-12-18 [1] RSPM\n#>  P evaluate         1.0.5    2025-08-27 [?] CRAN (R 4.5.1)\n#>  P farver           2.1.2    2024-05-13 [?] RSPM\n#>  P fastmap          1.2.0    2024-05-15 [?] CRAN (R 4.5.0)\n#>  P generics         0.1.4    2025-05-09 [?] RSPM\n#>  P ggplot2        * 4.0.2    2026-02-03 [?] RSPM\n#>  P glossary       * 1.0.0    2023-05-30 [?] RSPM\n#>  P glue             1.8.0    2024-09-30 [?] CRAN (R 4.5.0)\n#>  P gridExtra        2.3      2017-09-09 [?] RSPM\n#>  P groupdata2       2.0.5    2024-12-18 [?] RSPM\n#>  P gtable           0.3.6    2024-10-25 [?] RSPM\n#>  P gtools           3.9.5    2023-11-20 [?] RSPM\n#>  P htmltools        0.5.9    2025-12-04 [?] CRAN (R 4.5.2)\n#>  P htmlwidgets      1.6.4    2023-12-06 [?] RSPM\n#>  P httpuv           1.6.16   2025-04-16 [?] RSPM\n#>  P igraph           2.2.1    2025-10-27 [?] RSPM\n#>  P inline           0.3.21   2025-01-09 [?] RSPM\n#>  P janitor          2.2.1    2024-12-22 [?] RSPM\n#>  P jsonlite         2.0.0    2025-03-27 [?] CRAN (R 4.5.0)\n#>  P kableExtra       1.4.0    2024-01-24 [?] RSPM\n#>  P knitr            1.51     2025-12-20 [?] CRAN (R 4.5.2)\n#>  P labeling         0.4.3    2023-08-29 [?] RSPM\n#>    later            1.4.5    2026-01-08 [1] RSPM\n#>    lattice          0.22-9   2026-02-09 [1] CRAN (R 4.5.2)\n#>  P lifecycle        1.0.5    2026-01-08 [?] RSPM\n#>  P litedown         0.9      2025-12-18 [?] RSPM\n#>    lme4             1.1-38   2025-12-02 [1] RSPM\n#>  P loo              2.9.0    2025-12-23 [?] RSPM\n#>    lubridate        1.9.5    2026-02-04 [1] RSPM\n#>    magrittr         2.0.4    2025-09-12 [1] RSPM\n#>  P markdown         2.0      2025-03-23 [?] RSPM\n#>  P MASS             7.3-65   2025-02-28 [?] CRAN (R 4.5.2)\n#>  P Matrix           1.7-4    2025-08-28 [?] CRAN (R 4.5.2)\n#>  P matrixStats      1.5.0    2025-01-07 [?] RSPM\n#>  P mime             0.13     2025-03-17 [?] CRAN (R 4.5.0)\n#>  P miniUI           0.1.2    2025-04-17 [?] RSPM\n#>  P minqa            1.2.8    2024-08-17 [?] RSPM\n#>  P nlme             3.1-168  2025-03-31 [?] CRAN (R 4.5.2)\n#>  P nloptr           2.2.1    2025-03-17 [?] RSPM\n#>  P otel             0.2.0    2025-08-29 [?] RSPM\n#>  P patchwork      * 1.3.2    2025-08-25 [?] RSPM\n#>  P pillar           1.11.1   2025-09-17 [?] RSPM\n#>  P pkgbuild         1.4.8    2025-05-26 [?] RSPM\n#>  P pkgconfig        2.0.3    2019-09-22 [?] RSPM\n#>    plyr             1.8.9    2023-10-02 [1] RSPM\n#>  P posterior        1.6.1    2025-02-27 [?] RSPM\n#>  P promises         1.5.0    2025-11-01 [?] RSPM\n#>    proxy            0.4-29   2025-12-29 [1] CRAN (R 4.5.2)\n#>    purrr            1.2.1    2026-01-09 [1] CRAN (R 4.5.2)\n#>    QuickJSR         1.9.0    2026-01-25 [1] RSPM\n#>  P R6               2.6.1    2025-02-15 [?] CRAN (R 4.5.0)\n#>    rbibutils        2.4.1    2026-01-21 [1] CRAN (R 4.5.2)\n#>  P RColorBrewer     1.1-3    2022-04-03 [?] RSPM\n#>    Rcpp             1.1.1    2026-01-10 [1] RSPM\n#>  P RcppParallel     5.1.11-1 2025-08-27 [?] RSPM\n#>    Rdpack           2.6.6    2026-02-08 [1] CRAN (R 4.5.2)\n#>    reformulas       0.4.4    2026-02-02 [1] RSPM\n#>    renv             1.1.7    2026-01-27 [1] RSPM\n#>  P reshape2         1.4.5    2025-11-12 [?] RSPM\n#>  P rlang            1.1.7    2025-12-20 [?] Github (tidyverse/rlang@7a519a2)\n#>  P rmarkdown        2.30     2025-09-28 [?] CRAN (R 4.5.0)\n#>  P rstan            2.32.7   2025-03-10 [?] RSPM\n#>  P rstanarm         2.32.2   2025-09-30 [?] RSPM\n#>    rstantools       2.6.0    2026-01-10 [1] RSPM\n#>    rstudioapi       0.18.0   2026-01-16 [1] RSPM\n#>    S7               0.2.1    2025-11-14 [1] CRAN (R 4.5.2)\n#>  P scales           1.4.0    2025-04-24 [?] RSPM\n#>  P sessioninfo      1.2.3    2025-02-05 [?] RSPM\n#>  P shiny            1.12.1   2025-12-09 [?] RSPM\n#>    shinyjs          2.1.1    2026-01-15 [1] RSPM\n#>  P shinystan        2.7.0    2025-12-12 [?] RSPM\n#>  P shinythemes      1.2.0    2021-01-25 [?] RSPM\n#>  P snakecase        0.11.1   2023-08-27 [?] RSPM\n#>  P StanHeaders      2.32.10  2024-07-15 [?] RSPM\n#>  P stringi          1.8.7    2025-03-27 [?] RSPM\n#>  P stringr          1.6.0    2025-11-04 [?] RSPM\n#>    survival         3.8-6    2026-01-16 [1] CRAN (R 4.5.2)\n#>  P svglite          2.2.2    2025-10-21 [?] RSPM\n#>  P systemfonts      1.3.1    2025-10-01 [?] RSPM\n#>  P tensorA          0.36.2.1 2023-12-13 [?] RSPM\n#>  P textshaping      1.0.4    2025-10-10 [?] RSPM\n#>  P threejs          0.3.4    2025-04-21 [?] RSPM\n#>    tibble           3.3.1    2026-01-11 [1] RSPM\n#>    tidyr            1.3.2    2025-12-19 [1] RSPM\n#>  P tidyselect       1.2.1    2024-03-11 [?] RSPM\n#>    timechange       0.4.0    2026-01-29 [1] RSPM\n#>  P V8               8.0.1    2025-10-10 [?] RSPM\n#>    vctrs            0.7.1    2026-01-23 [1] RSPM\n#>    viridisLite      0.4.3    2026-02-04 [1] RSPM\n#>  P webexercises   * 1.1.0    2023-05-15 [?] RSPM\n#>  P withr            3.0.2    2024-10-28 [?] RSPM\n#>    xfun             0.56     2026-01-18 [1] RSPM\n#>    xml2             1.5.2    2026-01-17 [1] RSPM\n#>  P xtable           1.8-4    2019-04-21 [?] RSPM\n#>    xts              0.14.1   2024-10-15 [1] RSPM\n#>  P yaml             2.3.12   2025-12-10 [?] CRAN (R 4.5.2)\n#>    zoo              1.8-15   2025-12-15 [1] RSPM\n#> \n#>  [1] /Users/petzi/Documents/Meine-Repos/bayes-rules/renv/library/macos/R-4.5/aarch64-apple-darwin20\n#>  [2] /Users/petzi/Library/Caches/org.R-project.R/R/renv/sandbox/macos/R-4.5/aarch64-apple-darwin20/4cd76b74\n#> \n#>  * ── Packages attached to the search path.\n#>  P ── Loaded and on-disk path mismatch.\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n:::\n:::::\n",
    "supporting": [
      "003-beta-binomial_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}