{
  "hash": "876c067849307d70f0d706873d3880b2",
  "result": {
    "engine": "knitr",
    "markdown": "# Conjugate Families {#sec-chap-005}\n\n\n\n:::::: {#obj-chap-005}\n::::: my-objectives\n::: my-objectives-header\nObjectives\n:::\n\n::: my-objectives-container\n-   **Practice building Bayesian models**. Build Bayesian models by\n    practicing how to recognize kernels and make use of proportionality.\n-   **Familiarize yourself with conjugacy**. Learn about what makes a\n    prior conjugate and why this is a helpful property.\n:::\n:::::\n::::::\n\nTo prepare for this chapter, note that we’ll be using some new Greek\nletters throughout our analysis:\n\n-   $\\lambda$ = lambda,\n-   $\\mu$ = mu or “mew”,\n-   $\\sigma$ = sigma,\n-   $\\tau$ = tau, and\n-   $\\theta$ = theta.\n\n## Revisiting choice of prior\n\nIn @sec-chap-003 and @sec-chap-004 we used the *flexibility* of the Beta\nmodel to reflect our prior understanding of a proportion parameter\n$\\pi \\in [0, 1]$. There are other criteria to consider when choosing a\nprior model:\n\n-   **Computational ease**: Especially if we don’t have access to\n    computing power, it is helpful if the posterior model is easy to\n    build.\n-   **Interpretability**:: We’ve seen that posterior models are a\n    compromise between the data and the prior model. A posterior model\n    is interpretable, and thus more useful, when you can look at its\n    formulation and *identify* the contribution of the data relative to\n    that of the prior.\n\nThe Beta-Binomial has both of these criteria covered. Its calculation is\neasy. Once we know the $\\text{Beta}(\\alpha, \\beta)$ prior\n<a class='glossary' title='In the Bayesian framework, a hyperparameter is a parameter of a prior distribution.  It defines the shape, location, or scale of the prior belief about a model parameter before observing any data. Unlike model parameters, which are estimated from the data (e.g., the mean or variance of a distribution), hyperparameters are set before the analysis begins and are not directly learned from the data. (Brave-AI)'>hyperparameters</a> and the observed data\n$Y = y$ for the $\\text{Bin}(n, \\pi)$ model, the\n$\\text{Beta}(\\alpha + y, \\beta + n − y)$ posterior model follows.\n\nThis posterior reflects the influence of the data, through the values of\n$y$ and $n$, relative to the prior hyperparameters $\\alpha$ and $\\beta$.\nIf $\\alpha$ and $\\beta$ are large relative to the sample size $n$, then\nthe posterior will not budge that much from the prior. However, if the\nsample size $n$ is large relative to $\\alpha$ and $\\beta$, then the data\nwill take over and be more influential in the posterior model. In fact,\nthe <a class='glossary' title='The Beta-Binomial Bayesian Model is a fundamental framework in Bayesian statistics used to estimate a proportion π (e.g., success probability) when data are binary outcomes (success/failure) across n independent trials. The Beta-Binomial Bayesian Model combines a Beta prior with a Binomial likelihood, resulting in a Beta posterior due to conjugacy.  This makes it ideal for estimating unknown proportions (e.g., success rates) from binary data.'>Beta-Binomial</a> belongs to a\nlarger class of prior-data combinations called **conjugate families**\nthat enjoy both computational ease and interpretable posteriors. Recall\na general definition similar to that from @def-003-conjugate-prior.\n\n:::::: my-experiment\n:::: my-experiment-header\n::: {#def-005-conjugate-prior}\n: Conjugate prior\n:::\n::::\n\n::: my-experiment-container\nLet the prior model for parameter $\\theta$ have pdf f ($\\theta$) and the\nmodel of data Y conditioned on $\\theta$ have likelihood function\n$L(\\theta \\mid y)$. If the resulting posterior model with pdf f\n$(\\theta \\mid y) \\propto f (\\theta)L(\\theta \\mid y)$ is of the same\nmodel family as the prior, then we say this is a\n<a class='glossary' title='If the [posterior distribution] is in the same probability distribution family as the prior probability distribution the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. A conjugate prior is an algebraic convenience; otherwise, numerical integration may be necessary. (Wikipedia)'>conjugate prior</a>.\n:::\n::::::\n\nTo emphasize the utility of conjugate priors, it can be helpful to\nconsider a *non*-conjugate prior. Let parameter $\\pi$ be a proportion\nbetween $0$ and $1$ and suppose we plan to collect data $Y$ where,\nconditional on $\\pi$, $Y \\mid \\pi \\sim \\text{Bin}(n, \\pi)$. Instead of\nour conjugate $\\text{Beta}(\\alpha, \\beta)$ prior for $\\pi$, let’s try\nout a non-conjugate prior with pdf f ($\\pi$), plotted in Figure 5.1:\n\n$$\n\\begin{align*}\nf (\\pi) = e − e^π \\text{ for } \\pi \\in [0, 1]\n\\end{align*}\n$$ {#eq-005-non-conjugate-prior}\n\nThough *not* a Beta pdf, f ($\\pi$) is indeed a *valid* pdf since f\n($\\pi$) is non-negative on the support of $\\pi$ and the area under the\npdf is $1$, i.e.,$\\int_{0}^{1} f (\\pi) = 1$.\n\n\n::: {.cell}\n\n```{#lst-005-non-conjugate-prior .r .cell-code  lst-cap=\"Valid non-conjugate prior\"}\n# Create a data frame with pi values and corresponding f(pi) values\npi_values <- base::seq(0, 1, length.out = 1000)\ndf <- tibble::tibble(\n  pi = pi_values,\n  f_pi = base::exp(1) - base::exp(pi_values)\n)\n\n# Create the plot\nggplot2::ggplot(df, ggplot2::aes(x = pi, y = f_pi)) +\n  ggplot2::geom_line(color = \"black\", linewidth = 1) +\n  ggplot2::labs(\n    title = base::expression(f(pi) == e - e^pi),\n    x = base::expression(pi),\n    y = base::expression(f(pi))\n  ) +\n  ggplot2::theme_minimal()\n```\n\n::: {.cell-output-display}\n![A non-conjugate prior for $\\pi$](005-conjugate-families_files/figure-html/fig-005-non-conjugate-prior-1.png){#fig-005-non-conjugate-prior width=384}\n:::\n:::\n\n\nNext, suppose we observe $Y = 10$ successes from $n = 50$ independent\ntrials, all having the same probability of success $\\pi$. The resulting\nBinomial likelihood function of $\\pi$ is:\n\n$$L(\\pi \\mid y = 10) = \\binom{50}{10}\\pi^{10}(1 − \\pi)^{40} \\text{ for } \\pi \\in [0, 1]$$\n\nRecall from @sec-chap-003 that, when we put the\n<a class='glossary' title='A prior represents the initial belief or knowledge about an uncertain quantity, such as a model parameter, before observing any data. It is formally defined as a probability distribution that quantifies the degree of belief in the parameter’s possible values based on previous information, expert opinion, or theoretical considerations.'>prior</a> and the\n<a class='glossary' title='The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E | H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI)'>likelihood</a> together, we are already on\nour path to finding the\n<a class='glossary' title='The posterior model represents the updated beliefs about the parameters of a model after incorporating observed data. It is derived by combining the prior distribution, which reflects initial beliefs about the parameter, with the likelihood of the observed data using Bayes’ theorem.'>posterior model</a> with\n<a class='glossary' title='A probability density function (PDF) describes a probability distribution for a random, continuous variable. Use a probability density function to find the chances that the value of a random variable will occur within a range of values that you specify. More specifically, a PDF is a function where its integral for an interval provides the probability of a value occurring in that interval. (Statistics By Jim)'>pdf</a>\n\n$$f (\\pi \\mid y = 10) \\propto f (\\pi)L(\\pi \\mid y = 10) = (e − e^π) \\cdot \\binom{50}{10}\\pi^{10}(1 − \\pi)^{40}$$\n\nAs we did in @sec-chap-003, we will drop all constants that do not\ndepend on $\\pi$ since we are only specifying $f (\\pi \\mid y)$ up to a\nproportionality constant:\n\n$$f (\\pi \\mid y) \\propto (e − e^\\pi)\\pi^{10}(1 − \\pi)^{40}$$\n\nNotice here that our non-Beta prior didn’t produce a neat and clean\nanswer for the exact posterior model (fully specified and not up to a\nproportionality constant). We cannot squeeze this posterior pdf kernel\ninto a Beta box or any other familiar model for that matter. That is, we\n*cannot* rewrite $(e − e^\\pi)\\pi^{10}(1 − \\pi)^{40}$ so that it shares\nthe same structure as a Beta kernel, $\\pi^{∎−1}(1 − \\pi)^{∎−1}$. Instead\nwe will need to integrate this kernel in order to complete the\nnormalizing constant, and hence posterior specification:\n\n$$\n\\begin{align*}\nf (\\pi \\mid y = 10) =  \\frac{(e − e^{\\pi})\\pi^{10}(1 − \\pi)^{40}}{\\int_{0}^{1}(e − e^{\\pi})\\pi^{10}(1 − \\pi)^{40}dx}\n\\end{align*}\n$$ {#eq-non-conjugate-posterior-specification}\n\nThis is where we really start to feel the pain of not having a conjugate\nprior! Since this is a particularly unpleasant integral to evaluate, and\nwe’ve been trying to avoid doing any integration altogether, we will\nleave ourselves with @eq-non-conjugate-posterior-specification as the\nfinal posterior. Yikes, what a mess. This *is* a valid posterior pdf –\nit’s non-negative and integrates to $1$ across $\\pi \\in [0, 1]$. But it\ndoesn’t have much else going for it. Consider a few characteristics\nabout the posterior model that result from this particular non-conjugate\nprior in @eq-005-non-conjugate-prior:\n\n-   The calculation for this posterior was messy and unpleasant.\n-   It is difficult to derive any intuition about the balance between\n    the prior information and the data we observed from this posterior\n    model.\n-   It would be difficult to specify features such as the posterior\n    mean, mode, and standard deviation, a process which would require\n    even more integration.\n\nThis leaves us with the question: could we use the conjugate Beta prior\nand still capture the broader information of the messy non-conjugate\nprior in @eq-005-non-conjugate-prior? If so, then we solve the problems\nof messy calculations and indecipherable posterior models.\n\n:::::: my-assessment\n:::: my-assessment-header\n::: {#cor-005-which-beta-for-non-jugate-prior}\n: Which Beta model?\n:::\n::::\n\n::: my-assessment-container\nWhich Beta model would most closely approximate the non-conjugate prior\nfor $\\pi$ in @fig-005-non-conjugate-prior?\n<select class='webex-select'><option value='blank'></option><option value=''>Beta(3,1)</option><option value=''>Beta(1,3)</option><option value=''>Beta(2,1)</option><option value='answer'>Beta(1,2)</option></select>\n:::\n::::::\n\nOne way to find the answer is by comparing the non-conjugate prior in\n@fig-005-non-conjugate-prior with plots of the possible Beta prior\nmodels. For example, the non-conjugate prior information is pretty well\ncaptured by the $\\text{Beta}(1, 2)$\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-005-beta-1-2}\n: A conjugate $\\text{Beta}(1, 2)$ prior model for $\\pi$.\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_beta(1, 2)\n```\n\n::: {.cell-output-display}\n![A conjugate $\\text{Beta}(1, 2)$ prior model for $\\pi$](005-conjugate-families_files/figure-html/fig-005-beta-1-2-1.png){#fig-005-beta-1-2 width=384}\n:::\n:::\n\n:::\n::::::\n\n## Gamma-Poisson conjugate family\n\nLast year, one of this book’s authors got fed up with the number of\nfraud risk phone calls they were receiving. They set out with a goal of\nmodeling *rate* $\\lambda$, the typical number of fraud risk calls\nreceived per day. **Prior** to collecting any data, the author’s guess\nwas that this rate was most likely around $5$ calls per day, but could\nalso reasonably range between $2$ and $7$ calls per day. To learn more,\nthey planned to record the number of fraud risk phone calls on each of\n$n$ sampled days, $(Y1, Y2, . . . , Yn)$.\n\nIn moving forward with our investigation of $\\lambda$, it’s important to\nrecognize that it will *not* fit into the familiar Beta-Binomial\nframework.\n\n-   First, $\\lambda$ is *not* a proportion limited to be between $0$ and\n    $1$, but rather a rate parameter that can take on any *positive*\n    value (e.g., we can’t receive -7 calls per day). Thus, a Beta prior\n    for $\\lambda$ won’t work.\n-   Further, each data point $Y_{i}$ is a *count* that can technically\n    take on any non-negative integer in ${0, 1, 2, . . .}$, and thus is\n    *not* limited by some number of trials $n$ as is true for the\n    Binomial. Not to fret. Our study of $\\lambda$ will introduce us to a\n    *new* conjugate family, the\n    <a class='glossary' title='A Gamma-Poisson model is a hierarchical statistical model that combines two probability distributions to handle overdispersed count data. (a) The Poisson distribution: Models the observed counts, with a rate parameter λ (lambda). (b) The Gamma distribution: Models the variability in λ itself, treating it as a random variable rather than a fixed parameter. (Brave-AI)'>Gamma-Poisson</a>.\n\n### The Poisson data model\n\nThe *spirit* of our analysis starts with a prior understanding of\n$\\lambda$, the daily rate of fraud risk phone calls. Yet before choosing\na prior model structure and tuning this to match our prior\nunderstanding, it’s beneficial to identify a model for the dependence of\nour daily phone call *count* data $Y_i$ on the typical daily *rate* of\nsuch calls $\\lambda$. Upon identifying a reasonable data model, we can\nidentify a prior model which can be tuned to match our prior\nunderstanding while *also* mathematically complementing the data model’s\ncorresponding likelihood function. Keeping in mind that each data point\n$Y_i$ is a *random count* that can go from $0$ to a really big number,\n$Y \\in \\{0, 1, 2, . . .\\}$, the\n<a class='glossary' title='The Poisson model is a fundamental statistical model used for count data — that is, data representing the number of times an event occurs within a fixed interval of time or space. (Brave-AI)'>Poisson model</a>, described in its general\nform below, makes a reasonable candidate for modeling this data.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-005-poisson-model}\n: Poisson model\n:::\n::::\n\n::: my-theorem-container\nLet discrete random variable $Y$ be the number of independent events\nthat occur in a fixed amount of time or space, where $\\lambda > 0$ is\nthe rate at which these events occur. Then the dependence of $Y$ on\n<a class='glossary' title='Unobserved variables are usually called Parameters. (SR2, Chap.2) A parameter is an unknown numerical characteristics of a population that must be estimated. (CDS). They are also numbers that govern statistical models (stats.stackexchange). A parameter is also a number that is a defining characteristic of some population or a feature of a population. (SwR, Glossary)'>parameter</a> $\\lambda$ can be modeled by the\nPoisson. In mathematical notation:\n\n$$Y \\mid \\lambda \\sim Pois(\\lambda)$$\n\nCorrespondingly, the Poisson model is specified by pmf\n\n$$\nf (y \\mid \\lambda) =  \\frac{\\lambda^y e^{−\\lambda}}{y!} \\text{ for } y \\in \\{0, 1, 2, . . .\\}\n$$ {#eq-005-poisson-model}\n\nwhere $f (y \\mid \\lambda)$ sums to one across\n$y, \\sum_{y=0}^{\\infty} f (y \\mid \\lambda) = 1$. Further, a Poisson\nrandom variable $Y$ has equal mean and variance,\n\n$$\nE(Y \\mid \\lambda) = Var(Y \\mid \\lambda) = \\lambda\n$$ {#eq-005-poisson-equal-mean-variance}\n:::\n::::::\n\nFigure 5.3 illustrates the Poisson pmf @eq-005-poisson-model under\ndifferent rate parameters $\\lambda$. In general, as the rate of events\n$\\lambda$ increases, the typical number of events increases, the\nvariability increases, and the skew decreases. For example, when events\noccur at a rate of $\\lambda = 1$, the model is heavily skewed toward\nobserving a small number of events – we’re most likely to observe $0$ or\n$1$ events, and rarely more than 3. In contrast, when events occur at a\nhigher rate of $\\lambda = 5$, the model is roughly symmetric and more\nvariable – we’re most likely to observe $4$ or $5$ events, though have a\nreasonable chance of observing anywhere between $1$ and $10$ events.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data frame with all combinations\ndata <- base::expand.grid(\n  x = 0:12,  # number of events\n  lambda = c(1, 3, 5) # rate of events\n) |> \n  dplyr::mutate(\n    probability = stats::dpois(x = x, lambda = lambda),\n    prob_label = base::paste0(\"Pois(\", lambda, \")\")\n  )\n\n# Create lollipop chart with facets\nggplot2::ggplot(data, ggplot2::aes(x = x, y = probability)) +\n  ggplot2::geom_linerange(\n    ggplot2::aes(\n      x = x, \n      ymin = 0, \n      ymax = probability, \n      color = \"black\"), \n    linewidth = 0.8) +\n  ggplot2::geom_point(ggplot2::aes(color = \"black\"), size = 1) +\n  ggplot2::scale_color_identity() +\n  ggplot2::facet_wrap(~ prob_label, ncol = 3) +\n  ggplot2::scale_x_continuous(breaks = base::seq(0, 12, by = 2)) +\n  # ggplot2::scale_y_continuous(breaks = base::seq(0, 0.20, by = 0.05)) +\n  ggplot2::labs(\n    x = \"y\",\n    y = base::expression(\"f (y)\")\n  ) +\n  ggplot2::theme_minimal() +\n    ggplot2::theme(\n    panel.spacing = grid::unit(1.5, \"lines\"),\n    strip.background = ggplot2::element_rect(fill = \"gray90\", color = \"gray50\"),\n    strip.text = ggplot2::element_text(face = \"bold\", size = 11)\n  )\n```\n\n::: {.cell-output-display}\n![Poisson pmfs with different rate parameters.](005-conjugate-families_files/figure-html/fig-005-plot-poisson-models-1.png){#fig-005-plot-poisson-models width=672}\n:::\n:::\n\n\nLet $(Y_1, Y_2, . . . , Y_n)$ denote the number of fraud risk calls we\nobserved on each of the n days in our data collection period. We assume\nthat the daily number of calls might differ from day to day and can be\nindependently modeled by the Poisson. Thus, on each day $i$,\n\n$$Y \\mid \\lambda \\sim Pois(\\lambda)$$\n\nwith unique pmf\n\n$$\nf (y \\mid \\lambda) =  \\frac{\\lambda^y e^{−\\lambda}}{y!} \\text{ for } y \\in \\{0, 1, 2, . . .\\}\n$$\n\nYet in weighing the evidence of the phone call data, we won’t want to\nanalyze each *individual* day. Rather, we’ll need to process the\n*collective* or *joint* information in our *n* data points. This\ninformation is captured by the\n<a class='glossary' title='The joint probability mass function is a function that completely characterizes the distribution of a discrete random vector. When evaluated at a given point, it gives the probability that the realization of the random vector will be equal to that point. The term ‘joint probability function’ is often used as a synonym. Sometimes, the abbreviation ‘joint pmf’ is used. (statlect.com)'>joint probability mass function</a>.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-005-joint-pmf}\n: Joint probability mass function\n:::\n::::\n\n::: my-theorem-container\nLet $(Y_1, Y_2, . . . , Y_n)$ be an independent sample of random\nvariables and $\\overrightarrow{\\rm y} = (y_1, y_2, . . . , y_n)$ be the\ncorresponding vector of observed values. Further, let\n$f (y_i \\mid \\lambda)$ denote the pmf of an individual observed data\npoint $Y_i = y_i$. Then by the assumption of independence, the following\n**joint pmf** specifies the randomness in and plausibility of the\ncollective sample:\n\n$$\n\\begin{align*}\nf (\\overrightarrow{\\rm y} \\mid \\lambda) = \\prod_{i = 1}^n f (y_i \\mid \\lambda) = f (y_1 \\mid \\lambda) \\cdot f (y_2 \\mid \\lambda) \\cdot \\cdots \\cdot f (y_n \\mid \\lambda)\n\\end{align*}\n$$ {#eq-005-joint-pmf}\n\n**Connecting concepts**\n\nThis product is analogous to the joint probability of independent events\nbeing the product of the\n<a class='glossary' title='Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table.'>marginal probabilities</a>,\n$P (A \\cap B) = P (A)P (B)$.\n:::\n::::::\n\nThe joint pmf for our fraud risk call sample follows by applying the\ngeneral definition in @eq-005-joint-pmf to our Poisson pmfs. Letting the\nnumber of calls on each day $i$ be $y_i \\in \\{0, 1, 2, . . .\\}$,\n\n$$\nf (\\overrightarrow{\\rm y} \\mid \\lambda) = \\prod_{i = 1}^n f (y_i \\mid \\lambda) = \\prod_{i = 1}^n \\frac{\\lambda^{y_i} e^{−λ}}{y_i!}\n$$ {#eq-005-joint-pfm-fraud-calls}\n\nThis looks like a mess, but it can be simplified. In this\nsimplification, it’s important to recognize that we have $n$ unique data\npoints $y_i$, not $n$ copies of the same data point $y$. Thus, we need\nto pay careful attention to the $i$ subscripts. It follows that\n\n$$\n\\begin{align*}\nf (\\overrightarrow{\\rm y} \\mid \\lambda) &= \\frac{\\lambda^{y_1} e^{−λ}}{y_1!} \\cdot \\frac{\\lambda^{y_2} e^{−λ}}{y_2!} \\cdots \\frac{\\lambda^{y_n} e^{−λ}}{y_n!} \\\\\n&= \\frac{[\\lambda^{y_1} \\lambda^{y_2} \\cdots \\lambda^{y_n}][e^{-\\lambda} e^{-\\lambda} \\cdots e^{-\\lambda}]}{y_1! y_2! \\cdots y_n!} \\\\\n&= \\frac{\\lambda^{\\sum y_i }e^{-n\\lambda}}{\\prod_{i = 1}^n y_i!}\n\\end{align*}\n$$ where we’ve simplified the products in the final line by appealing to\nthe properties below.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-005-simplifying-products}\n: Simplifying products\n:::\n::::\n\n::: my-theorem-container\nLet $(x, y, a, b)$ be a set of constants. Then we can utilize the\nfollowing facts when simplifying products involving exponents:\n\n$$\n\\begin{align*}\nx^ax^b = x^{a+b} \\text{ and } x^ay^a = (xy)^a\n\\end{align*}\n$$ {#eq-005-simplfiying-products}\n:::\n::::::\n\nOnce we observe actual sample data, we can flip this joint pmf on its\nhead to define the\n<a class='glossary' title='The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI)'>likelihood function</a> of $\\lambda$.\nThe Poisson likelihood function is equivalent in formula to the joint\npmf $f (\\overrightarrow{\\rm y} \\mid \\lambda)$, yet is a function of\n$\\lambda$ which helps us assess the compatibility of different possible\n$\\lambda$ values with our observed *collection* of sample data\n$\\overrightarrow{\\rm y}$:\n\n$$\nL (\\overrightarrow{\\rm y} \\mid \\lambda) = \\frac{\\lambda^{\\sum y_i }e^{-n\\lambda}}{\\prod_{i = 1}^n y_i!} \\propto \\lambda^{\\sum y_i }e^{-n\\lambda} \\text { for } \\lambda > 0\n$$ {#eq-005-poisson-likelihood-function}\n\nIt is convenient to represent the likelihood function up to a\nproportionality constant here, especially since $\\prod y_i!$ will be\ncumbersome to calculate when $n$ is large, and what we really care about\nin the likelihood is $\\lambda$. And when we express the likelihood up to\na proportionality constant, note that the sum of the data points\n($\\sum y_i$) and the number of data points ($n$) is all the information\nthat is required from the data. We don’t need to know the value of each\nindividual data point $y_i$. Taking this for a spin with real data\npoints later in our analysis will provide some clarity.\n\n### Potential priors\n\nThe Poisson data model provides one of two key pieces for our Bayesian\nanalysis of $\\lambda$, the daily rate of fraud risk calls. The other key\npiece is a prior model for $\\lambda$. Our original guess was that this\nrate is most likely around 5 calls per day, but could also reasonably\nrange between 2 and 7 calls per day. In order to tune a prior to match\nthese ideas about $\\lambda$, we first have to identify a reasonable\nprobability model structure. Remember here that $\\lambda$ is a positive\nand continuous rate, meaning that $\\lambda$ does not have to be a whole\nnumber. Accordingly, a reasonable prior probability model will also have\ncontinuous and positive support, i.e., be defined on $\\lambda > 0$.\nThere are several named and studied probability models with this\nproperty, including the <a class='glossary' title='The F model (or F-distribution model) is a statistical model used for testing equality of variances and comparing means across multiple groups. The F-distribution arises from the ratio of two chi-squared distributions. (Brave-AI)'>F</a> ,\n<a class='glossary' title='The Weibull model is a statistical model where the response variable follows a Weibull distribution, which is appropriate for positive continuous data — particularly time-to-event data (survival times, failure times, lifetimes). (Brave-AI)'>Weibull</a>, and\n<a class='glossary' title='The Gamma model is a statistical model where the response variable follows a Gamma distribution, which is appropriate for positive continuous data — such as time-to-event, income, insurance claims, or size measurements. (Brave-AI)'>Gamma</a>. We don’t dig into all of these in\nthis book. Rather, to make the $\\lambda$ posterior model construction\nmore straightforward and convenient, we’ll focus on identifying a\n<a class='glossary' title='If the [posterior distribution] is in the same probability distribution family as the prior probability distribution the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. A conjugate prior is an algebraic convenience; otherwise, numerical integration may be necessary. (Wikipedia)'>conjugate prior</a> model.\n\n:::::: my-assessment\n:::: my-assessment-header\n::: {#cor-005-conjugate-prior}\n: Choose the correct conjugate prior\n:::\n::::\n\n::: my-assessment-container\nSuppose we have a random sample of Poisson random variables\n$(Y_1, Y_2, . . . , Y_n)$ with likelihood function\n$L (\\overrightarrow{\\rm y} \\mid \\lambda) = \\propto \\lambda^{\\sum y_i }e^{-n\\lambda} \\text { for } \\lambda > 0$\n\n-   \n\n    a.  A “Gamma” model with pdf\n        $f (\\lambda) \\propto \\lambda^{s−1}e^{−r\\lambda}$\\\n\n-   \n\n    b.  A “Weibull” model with pdf\n        $f (\\lambda) \\propto \\lambda^{s−1}e^{(−r\\lambda)^s}$\n\n-   \n\n    c.  A special case of the “F” model with pdf\n        $f (\\lambda) \\propto \\lambda^{ \\frac{s}{2}-1} (1 + \\lambda)^{−s}$\n\nWhat do you *think* would provide a convenient *conjugate* prior model\nfor $\\lambda$?: <select class='webex-select'><option value='blank'></option><option value='answer'>a</option><option value=''>b</option><option value=''>c</option></select>\n\nWhy?\n:::\n::::::\n\nThe Gamma model will provide a conjugate prior for $\\gamma$ when our\ndata has a Poisson model. You might have guessed this from the section\ntitle (clever). You might also have guessed this from the shared\nfeatures of the Poisson likelihood function\n$L (\\overrightarrow{\\rm y} \\mid \\lambda)$ in\n@eq-005-poisson-likelihood-function and the Gamma pdf $f (\\lambda)$. Both\nare proportional to\n\n$$\\lambda^∎e^{−∎\\lambda}$$\\\nwith differing ∎. In fact, we’ll prove that *combining* the prior and\nlikelihood produces a posterior pdf with this same structure. That is,\nthe posterior will be of the same Gamma model family as the prior.\nFirst, let’s learn more about the Gamma model.\n\n### Gamma prior\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-005-gamma-prior}\n: Gamma and Exponential models\n:::\n::::\n\n::: my-theorem-container\nLet $\\lambda$ be a continuous random variable which can take any\npositive value, i.e., $\\lambda > 0$. Then the variability in $\\lambda$\nmight be well modeled by a Gamma model with\n<a class='glossary' title='The shape parameter in the context of the gamma distribution is a key hyperparameter that controls the form or shape of the distribution.  It is often denoted as α (alpha). A higher shape parameter (α) results in a more peaked and right-skewed distribution; a lower shape parameter leads to a flatter, more spread-out distribution. (Brave-AI)'>shape hyperparameter</a> $s > 0$ and\n<a class='glossary' title='The rate hyperparameter in the gamma distribution, typically denoted as β (beta), controls the scale of the distribution and is the inverse of the scale parameter (i.e., rate = 1/scale). A higher rate means events occur more frequently, leading to a narrower and more concentrated distribution; a lower rate implies less frequent events, resulting in a broader distribution. (Brave-AI)'>rate hyperparameter</a> $r > 0$:\n\n$$\\lambda \\sim \\text{Gamma}(s, r)$$\\\nThe Gamma model is specified by continuous pdf\n\n$$\nf (\\lambda) =  \\frac{r^s}{\\tau(s)}  \\lambda^{s−1}e^{−r\\lambda} \\text{ for } \\lambda > 0\n$$ {#eq-005-gamma-model}\n\nFurther, the central tendency and variability in $\\lambda$ are measured\nby:\n\n$$\n\\begin{align*}\nE(\\lambda) &=  \\frac{s}{r} \\\\\nMode(\\lambda) &=  \\frac{s−1}{r} \\text{ for } s ≥ 1 \\\\\nVar(\\lambda) &=  \\frac{s}{r^2}\n\\end{align*}\n$$ {#eq-005-gamma-central-measures}\n\nThe <a class='glossary' title='An exponential model describes a process where a quantity changes at a rate proportional to its current value, leading to rapid growth or decay over time. (Bave-AI)'>Exponential model</a> is a special\ncase of the Gamma with shape $s = 1, \\text{Gamma}(1, r)$:\n\n$$\n\\lambda \\sim \\text{Exp}(r)\n$$\n:::\n::::::\n\nNotice that the Gamma model depends upon two hyperparameters, `r` and\n`s`. Assess your understanding of how these hyperparameters impact the\nGamma model properties in the following quiz.\n\n::::::::: my-assessment\n:::: my-assessment-header\n::: {#cor-005-r-and-s-effects}\n: Impact of hyperparameters `r` and `s` in Gamma models\n:::\n::::\n\n:::::: my-assessment-container\n\n::: {.cell}\n\n```{#lst-005-different-gamma-models .r .cell-code  lst-cap=\"Different Gamma models\"}\n# Create data for all parameter combinations\nparams <- tibble::tribble(\n  ~shape, ~rate,\n  1, 1,\n  2, 1,\n  4, 1,\n  1, 2,\n  2, 2,\n  4, 2\n) |>\n  dplyr::mutate(label = stringr::str_glue(\"Gamma({shape},{rate})\"))\n\n# Generate data for each combination\nplot_data <- params |>\n  dplyr::rowwise() |>\n  dplyr::reframe(\n    tibble::tibble(\n      x = base::seq(0, 8, length.out = 200),\n      y = stats::dgamma(x, shape = shape, rate = rate),\n      label = label,\n      shape = shape,\n      rate = rate\n    )\n  )\n\n# Generate segment data for mean and mode\nsegments_data <- params |>\n  dplyr::rowwise() |>\n  dplyr::reframe(\n    mean_val = shape / rate,\n    mode_val = (shape - 1) / rate,\n    tibble::tibble(\n      x = c(mean_val, mode_val),\n      xend = c(mean_val, mode_val),\n      y = 0,\n      yend = c(\n        stats::dgamma(mean_val, shape, rate),\n        stats::dgamma(mode_val, shape, rate)\n      ),\n      line_type = c(\"mean\", \"mode\"),\n      label = label,\n      rate = rate\n    )\n  )\n\n# Create faceted plot\nplot_data |>\n  dplyr::arrange(rate) |>\n  dplyr::mutate(label = forcats::fct_inorder(label)) |>\n  ggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n  ggplot2::geom_line() +\n  ggplot2::geom_segment(\n    data = segments_data |> \n      dplyr::arrange(rate) |>\n      dplyr::mutate(label = forcats::fct_inorder(label)),\n    ggplot2::aes(x = x, y = y, xend = xend, yend = yend, linetype = line_type),\n    color = \"darkblue\"\n  ) +\n  ggplot2::facet_wrap(~ label, ncol = 3) +\n  ggplot2::scale_x_continuous(breaks = base::seq(0, 8, 1), limits = c(0, 8)) +\n  ggplot2::scale_y_continuous(breaks = base::seq(0, 2, 0.5), limits = c(0, 2)) +\n  ggplot2::scale_linetype_manual(\n    values = c(mean = \"solid\", mode = \"dashed\"),\n    name = NULL\n  ) +\n  ggplot2::labs(\n    x = base::expression(lambda),\n    y = base::expression(paste(\"f(\", lambda, \")\"))\n  ) +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    panel.spacing = grid::unit(1.5, \"lines\"),\n    strip.background = ggplot2::element_rect(fill = \"gray90\", color = \"gray50\"),\n    strip.text = ggplot2::element_text(face = \"bold\", size = 11)\n  )\n```\n\n::: {.cell-output-display}\n![Gamma models with different hyperparameters. The dashed and solid vertical lines represent the modes and means, respectively.](005-conjugate-families_files/figure-html/fig-005-different-gamma-models-1.png){#fig-005-different-gamma-models width=672}\n:::\n:::\n\n\n@fig-005-different-gamma-models illustrates how different shape and rate\nhyperparameters impact the Gamma pdf in @eq-005-gamma-model. Based on\nthese plots:\n\n1.  How would you describe the typical behavior of a\n    $\\text{Gamma}(s, r)$ variable $\\lambda$ when $s > r$ (e.g.,\n    $\\text{Gamma}(2,1)$)?\n    <select class='webex-select'><option value='blank'></option><option value='answer'>Right-skewed with a mean greater than 1</option><option value=''>Right-skewed with a mean less than 1</option><option value=''>Symmetric with a mean around 1</option></select>\n2.  how would you describe the typical behavior of a\n    $\\text{Gamma}(s, r)$ variable $\\lambda$ when $s < r$ (e.g.,\n    $\\text{Gamma}(1,2)$)?\n    <select class='webex-select'><option value='blank'></option><option value=''>Right-skewed with a mean greater than 1</option><option value='answer'>Right-skewed with a mean less than 1</option><option value=''>Symmetric with a mean around 1</option></select>\n\n::::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmy_plot_gamma(20, 20)\n```\n\n::: {.cell-output-display}\n![$\\text{Gamma}(20,20)$](005-conjugate-families_files/figure-html/fig-005-gamma-20-20-1.png){#fig-005-gamma-20-20 width=672}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmy_plot_gamma(20, 100)\n```\n\n::: {.cell-output-display}\n![$\\text{Gamma}(20,100)$](005-conjugate-families_files/figure-html/fig-005-gamma-20-100-1.png){#fig-005-gamma-20-100 width=672}\n:::\n:::\n\n:::\n:::::\n\n3.  For which model is there greater variability in the plausible values\n    of $\\lambda$, $\\text{Gamma}(20,20)$ or $\\text{Gamma}(20,100)$?\n    <select class='webex-select'><option value='blank'></option><option value='answer'>Gamma(20, 20)</option><option value=''>Gamma(20, 100)</option></select>\n::::::\n:::::::::\n\n::::: {#nte-005-shape-rate-hyperparameter .callout-note}\n###### Shape and Rate Hyperparameters\n\n::: {#big-text style=\"font-size: 150%\"}\n**Shape Parameter**\n:::\n\nThe shape parameter in the context of the gamma distribution is a key\n<a class='glossary' title='In the Bayesian framework, a hyperparameter is a parameter of a prior distribution.  It defines the shape, location, or scale of the prior belief about a model parameter before observing any data. Unlike model parameters, which are estimated from the data (e.g., the mean or variance of a distribution), hyperparameters are set before the analysis begins and are not directly learned from the data. (Brave-AI)'>hyperparameter</a> that controls the form or shape of the\ndistribution. It is often denoted as $\\alpha$ (alpha).\n\n-   A **higher shape** parameter ($\\alpha$) results in a more **peaked\n    and right-skewed distribution**, indicating that events are **more\n    concentrated** around a specific value.\n-   A **lower shape** parameter leads to a **flatter, more spread-out\n    distribution**.\n\nIn practical applications like modeling waiting times or right-skewed\ndata (e.g., insurance claims, rainfall), **the shape parameter helps\ndetermine how the data is distributed**. For instance, in the gamma\nshape model, covariates influence the distribution through the shape\nparameter, allowing the variance to be directly proportional to the\nmean—making it useful for modeling data with increasing variability.\n\n::: {style=\"font-size: 150%\"}\n**Rate Parameter**\n:::\n\nThe rate hyperparameter in the gamma distribution, typically denoted as\n$\\beta$ (beta), controls the scale of the distribution and is the\n**inverse of the scale parameter** (i.e., rate = 1/scale).\n\n-   A **higher rate** means events occur more frequently, leading to a\n    **narrower** and more **concentrated** distribution.\n-   A **lower rate** implies less frequent events, resulting in a\n    **broader** distribution.\n\nIn applications like reliability analysis or queuing theory, **the rate\nparameter reflects the average occurrence rate of events per unit\ntime**. For example, if modeling time between customer arrivals, a rate\nof $beta = 2$ means, on average, two customers arrive per time unit, and\nthe expected waiting time follows a gamma distribution with this rate.\n:::::\n\nIn general, @fig-005-different-gamma-models illustrates that\n$\\lambda \\sim \\text{Gamma}(s, r)$ variables are positive and right\nskewed. Further, the general shape and rate of decrease in the skew are\ncontrolled by hyperparameters `s` and `r`. The quantitative measures of\ncentral tendency and variability in @eq-005-gamma-central-measures\nprovide some insight. For example, notice that the mean of\n$\\lambda, E(\\lambda) = s/r$, is greater than $1$ when $s > r$ and less\nthan $1$ when $s < r$. Further, as `s` increases relative to `r`, the\nskew in $\\lambda$ decreases and the variability,\n$\\text{Var}(\\lambda) = s/r^2$, increases.\n\nNow that we have some intuition for how the $\\text{Gamma}(s, r)$ model\nworks, we can tune it to reflect our prior information about the daily\nrate of fraud risk phone calls $\\lambda$. Recall our earlier assumption\nthat $\\lambda$ is about $5$, and most likely somewhere between $2$ and\n$7$. Our $\\text{Gamma}(s, r)$ prior should have similar patterns. For\nexample, we want to pick `s` and `r` for which $\\lambda$ tends to be\naround $5$,\n\n$$E(\\lambda) =  \\frac{s}{r} \\approx 5$$\n\nThis can be achieved by setting `s` to be $5$ times `r`, $s = 5r$. Next\nwe want to make sure that most values of our $\\text{Gamma}(s, r)$ prior\nare between $2$ and $7$. Through some trial and error within these\nconstraints, and plotting various Gamma models using\n`bayesrules::plot_gamma()`, we find that the $\\text{Gamma}(10,2)$\nfeatures closely match the central tendency *and* variability in our prior\nunderstanding (@fig-005-plot-gamma-10-2). Thus, a *reasonable* prior model for the daily rate of fraud risk phone calls is\n\n$$\\lambda \\sim \\text{Gamma}(10, 2)$$\n\nwith **prior pdf** $f (\\lambda)$ following from plugging $s = 10$ and\n$r = 2$ into\n\n$$\n\\begin{align*}\nf (\\lambda) &=  \\frac{r^s}{\\tau(s)}  \\lambda^{s−1}e^{−r\\lambda} \\text{ for } \\lambda > 0 \\\\\n&= \\frac{2^{10}}{\\tau(10)}  \\lambda^{10−1}e^{−2\\lambda} \\text{ for } \\lambda > 0\n\\end{align*}\n$$\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-005-plot-gamma-10-2}\n: Plot the $\\text{Gamma}(10, 2)$ prior\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_gamma(10, 2)\n```\n\n::: {.cell-output-display}\n![The pdf of a $\\text{Gamma}(10, 2)$ prior for $\\lambda$, the daily rate of fraud risk calls.](005-conjugate-families_files/figure-html/fig-005-plot-gamma-10-2-1.png){#fig-005-plot-gamma-10-2 width=672}\n:::\n:::\n\n:::\n::::::\n\n::: {.callout-note #nte-005-plot-gamma-11-2}\nAt first it seemed to me that $\\text{Gamma}(11, 2)$ would have been a slightly better choice: It has its mode at exactly $5$ and it is similar spread out with a slightly bigger chance to get more values above $10$.\n\n:::::: my-r-code\n:::: my-r-code-header\n::: {#cnj-005-plot-gamma-11-2}\n: Plot the $\\text{Gamma}(11, 2)$ prior\n:::\n::::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_gamma(11, 2)\n```\n\n::: {.cell-output-display}\n![The pdf of a $\\text{Gamma}(11, 2)$ prior for $\\lambda$, the daily rate of fraud risk calls.](005-conjugate-families_files/figure-html/fig-005-plot-gamma-11-2-1.png){#fig-005-plot-gamma-11-2 width=672}\n:::\n:::\n\n:::\n::::::\n\nBut in that case the mean would change to 5.5 ($E = s/r = 11/2 = 5.5$). It is better to take the mean as central tendency in a heavily skewed distribution because it would be a better measure to catch the typical property of the distribution than the mode.\n\n:::\n\n\n### Gamma-Poisson conjugacy\n\n#### Bayesian model\n\nAs we discussed at the start of this chapter, conjugate families can\ncome in handy. Fortunately for us, using a Gamma prior for a rate\nparameter $\\lambda$ and a Poisson model for corresponding count data $Y$\nis another example of a conjugate family. This means that, *spoiler*,\nthe posterior model for $\\lambda$ will also have a Gamma model with\n*updated* parameters. We’ll state and prove this in the general setting\nbefore applying the results to our phone call situation.\n\n:::::: my-theorem\n:::: my-theorem-header\n::: {#thm-005-gamma-poisson-model}\n: The Gamma-Poisson Bayesian model\n:::\n::::\n\n::: my-theorem-container\nLet $\\lambda > 0$ be an unknown rate parameter and\n$(Y_1, Y_2, . . . , Y_n)$ be an independent $\\text{Pois}(\\lambda)$\nsample. The Gamma-Poisson Bayesian model complements the Poisson\nstructure of data $Y$ with a Gamma prior on $\\lambda$:\n\n$$\n\\begin{align*}\nY_i \\mid &\\lambda \\overset{\\text{ind}}{\\sim} \\text{Pois}(\\lambda) \\\\\n&\\lambda \\sim \\text{Gamma}(s, r)\n\\end{align*}\n$$\n\nUpon observing data $\\overrightarrow{\\rm y} = (y_1, y_2, . . . , y_n)$,\nthe posterior model of $\\lambda$ is also a Gamma with updated\nparameters:\n\n$$\n\\lambda \\mid \\overrightarrow{\\rm y} \\sim \\text{Gamma}(s + \\sum y_i, r + n)\n$$ {#eq-005-gamma-poisson-model}\n:::\n::::::\n\n#### Prove conjugacy\n\nLet’s prove this result. In general, recall that the posterior pdf of\n$\\lambda$ is proportional to the product of the prior pdf and likelihood\nfunction defined by @eq-005-gamma-model and @eq-005-poisson-likelihood-function, respectively:\n\n$$\nf (\\lambda \\mid \\overrightarrow{\\rm y}) \\propto f (\\lambda)L(\\lambda \\mid \\overrightarrow{\\rm y}) =  \\frac{r^s}{\\Gamma(s)}\\lambda^{s−1}e^{−rλ} \\cdot  \\frac{\\lambda^{\\sum y_i} e{−n\\lambda}} {\\prod yi!} \\text{ for } \\lambda > 0\n$$ \nNext, remember that any non-$\\lambda$ multiplicative constant in the above equation can be “proportional-ed” out. Thus, boiling the prior pdf and likelihood function down to their kernels, we get\n\n$$\n\\begin{align*}\nf (\\lambda \\mid \\overrightarrow{\\rm y}) &\\propto \\lambda^{s−1}e^{−r\\lambda} \\cdot \\lambda^{\\sum y_i} e^{−n\\lambda} \\\\\n&= \\lambda^{s+\\sum y_i-1}e^{-(r+n)\\lambda}\n\\end{align*}\n$$ \n\nwhere the final line follows by combining like terms. What we’re left with here is the *kernel* of the posterior pdf. This particular kernel corresponds to the pdf of a Gamma model in @eq-005-gamma-model, with shape parameter $s + \\sum y_i$ and rate parameter $r + n$. Thus, we’ve proven that\n\n$$\n\\lambda \\mid \\overrightarrow{\\rm y} \\sim \\text{Gamma}(s + \\sum y_i, r + n)\n$$ \n\n\n#### Likelihood\n\nLet’s apply this result to our fraud risk calls. There we have a $\\text{Gamma}(10,2)$ prior for $\\lambda$, the daily rate of calls. Further, on four separate days in the second week of August, we received $\\overrightarrow{\\rm y} = (y_1, y_2, y_3, y_4) = (6, 2, 2, 1)$ such calls. Thus, we have a sample of $n = 4$ data points with a total of $11$ fraud risk calls and an *average* of $2.75$ phone calls per day:\n\n$$\n\\sum_{i=1}^{4}y_i = 6 + 2 + 2 + 1 = 11 \\text{ and } \\overline{\\rm y} = \\frac{\\sum_{i=1}^{4} y_i}{4} = 2.75\n$$\nPlugging this data into @eq-005-poisson-likelihood-function, the resulting Poisson likelihood function of $\\lambda$ is\n\n$$\nL(\\lambda \\mid \\overrightarrow{\\rm y}) =  \\frac{\\lambda^{11}e^{−4λ}}{  6! \\times 2! \\times 2! \\times× 1!} \\propto \\lambda^{11}e^{−4λ} \\text { for } \\lambda > 0\n$$\n\nWe visualize a portion of $L(\\lambda \\mid \\overrightarrow{\\rm y})$ for $\\lambda$ between $0$ and $10$ using the `bayesrules::plot_poisson_likelihood()` function. Here, `y` is the vector of data values and `lambda_upper_bound` is the maximum value of $\\lambda$ to view on the x-axis. (Why can’t we visualize the whole likelihood? Because $\\lambda \\in (0, \\infty)$ and this book would be pretty expensive if we had infinite pages.)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-005-plot-poisson-likelihood-fraud-calls}\n: Plot Poisson likelihood of fraud calls\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_poisson_likelihood(c(6, 2, 2, 1), lambda_upper_bound = 10)\n```\n\n::: {.cell-output-display}\n![The likelihood function of $\\lambda$, the daily rate of fraud risk calls, given a four-day sample of phone call data.](005-conjugate-families_files/figure-html/fig-005-plot-poisson-likelihood-fraud-calls-1.png){#fig-005-plot-poisson-likelihood-fraud-calls width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\nThe punchline is this: Underlying *rates* $\\lambda$ of between one to five fraud risk calls per day are consistent with our phone call data. And across this spectrum, rates near $2.75$ are the *most* compatible with this data. This makes sense. The Poisson data model assumes that $\\lambda$ is the underlying average daily phone call count, $E(Y_i \\mid \\lambda) = \\lambda$. As such, we’re *most* likely to observe a sample with an average daily phone call rate of $\\overline{y} = 2.75$ when the underlying rate $\\lambda$ is also $2.75$.\n\nCombining these observations with our $\\text{Gamma}(10,2)$ prior model of $\\lambda$, it follows from @eq-005-gamma-poisson-model that the posterior model of $\\lambda$ is a Gamma with an updated shape parameter of $21 (s + \\sum y_i = 10 + 11)$ and rate parameter of $6 (r + n = 2 + 4)$:\n\n$$\n\\begin{align*}\n\\lambda \\mid \\overrightarrow{\\rm y} &\\sim \\text{Gamma}(s + \\sum y_i, r + n) = \\\\\n&\\sim \\text{Gamma}(10 + 11, 2 + 4) = \\\\\n&\\sim \\text{Gamma}(21, 6)\n\\end{align*}\n$$\nWe can visualize the <a class='glossary' title='A prior represents the initial belief or knowledge about an uncertain quantity, such as a model parameter, before observing any data. It is formally defined as a probability distribution that quantifies the degree of belief in the parameter’s possible values based on previous information, expert opinion, or theoretical considerations.'>prior</a> pdf, scaled <a class='glossary' title='The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI)'>likelihood function</a>, and <a class='glossary' title='The posterior distribution represents the updated beliefs about the parameters of a model after incorporating observed data. It is derived by combining the prior distribution, which reflects initial beliefs about the parameter, with the likelihood of the observed data using Bayes’ theorem.'>posterior</a> pdf for $\\lambda$ all in a single plot with the `bayesrules::plot_gamma_poisson()` function. How magical. For this function to work, we must specify a few things: \n\n- the prior `shape` hyperparameter and\n- the `rate` hyperparameter as well as the information from our data, \n- the observed total number of phone calls `sum_y` ($\\sum y_i$) and the \n- the sample size `n`:\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-005-gamma-poisson-fraud-calls}\n: Gamma-Poisson model of fraud risk calls,\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::plot_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4)\n```\n\n::: {.cell-output-display}\n![The Gamma-Poisson model of $\\lambda$, the daily rate of fraud risk calls.](005-conjugate-families_files/figure-html/fig-005-gamma-poisson-fraud-calls-1.png){#fig-005-gamma-poisson-fraud-calls width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n#### Posterior\n\nOur posterior notion about the daily rate of fraud calls is, of course, a *compromise* between our vague prior and the observed phone call data. Since our prior notion was quite variable in comparison to the strength in our sample data, the posterior model of $\\lambda$ is more in sync with the data. Specifically, utilizing the properties of the $\\text{Gamma}(10,2)$ prior and $\\text{Gamma}(21,6)$ posterior as defined by @eq-005-gamma-central-measures, notice that our posterior understanding of the typical daily rate of phone calls dropped from $5$ to $3.5$ per day:\n\n$$\n\\text{E}(\\lambda) =  \\frac{10}{2} = 5 \\text{ and } \\text{E}(\\lambda \\mid \\overrightarrow{\\rm y}) =  \\frac{21}{6} = 3.5\n$$\nThough a compromise between the prior mean and data mean, this posterior mean is *closer* to the data mean of $\\overline{y} = 2.75$ calls per day.\n\n::: {.callout-tip #tip-005-posterior-between-prior-and-data}\n###### Posterior has to be between prior mean and data mean\n\nThe posterior mean will always be between the prior mean and the data mean. If your posterior mean falls outside that range, it indicates that you made an error and should retrace some steps.\n:::\n\nFurther, with the additional information about $\\lambda$ from the data, the variability in our understanding of $\\lambda$ drops by more than half, from a standard deviation of $1.581$ to $0.764$ calls per day:\n\n$$\n\\text{SD}(\\lambda) =  \\sqrt{\\frac{10}{2^2}} \\approx 1.581 \\text{ and } \\text{SD}(\\lambda \\mid \\overrightarrow{\\rm y}) =  \\sqrt\\frac{21}{6^2} \\approx  0.764.\n$$\nThe convenient `bayesrules::summarize_gamma_poisson()` function , which uses the same arguments as `bayesrules::plot_gamma_poisson()`, helps us contrast the prior and posterior models and confirms the results above:\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-005-summarize-gamma-poisson-fraud-calls}\n: Summarize Gamma-Poisson model for fraud risk calls\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#tbl-005-summarize-gamma-poisson-fraud-calls .cell tbl-cap='Summarize Gamma-Poisson model for fraud risk calls'}\n\n```{.r .cell-code}\nbayesrules::summarize_gamma_poisson(\n  shape = 10, \n  rate = 2, \n  sum_y = 11, \n  n = 4) |> \n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|model     | shape| rate| mean|   mode|    var|     sd|\n|:---------|-----:|----:|----:|------:|------:|------:|\n|prior     |    10|    2|  5.0| 4.5000| 2.5000| 1.5811|\n|posterior |    21|    6|  3.5| 3.3333| 0.5833| 0.7638|\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n## Normal-normal conjugate family\n\n\n## Glossary Entries {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Beta-Binomial-Model </td>\n   <td style=\"text-align:left;\"> The Beta-Binomial Bayesian Model is a fundamental framework in Bayesian statistics used to estimate a proportion π (e.g., success probability) when data are binary outcomes (success/failure) across n independent trials. The Beta-Binomial Bayesian Model combines a Beta prior with a Binomial likelihood, resulting in a Beta posterior due to conjugacy.  This makes it ideal for estimating unknown proportions (e.g., success rates) from binary data. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Conjugate Prior </td>\n   <td style=\"text-align:left;\"> If the [posterior distribution] is in the same probability distribution family as the prior probability distribution the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. A conjugate prior is an algebraic convenience; otherwise, numerical integration may be necessary. (&lt;a href=\"https://en.wikipedia.org/wiki/Conjugate_prior\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Exponential-Model </td>\n   <td style=\"text-align:left;\"> An exponential model describes a process where a quantity changes at a rate proportional to its current value, leading to rapid growth or decay over time. (Bave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> F-Distribution-Model </td>\n   <td style=\"text-align:left;\"> The F model (or F-distribution model) is a statistical model used for testing equality of variances and comparing means across multiple groups. The F-distribution arises from the ratio of two chi-squared distributions. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Gamma Model </td>\n   <td style=\"text-align:left;\"> The Gamma model is a statistical model where the response variable follows a Gamma distribution, which is appropriate for positive continuous data — such as time-to-event, income, insurance claims, or size measurements. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Gamma-Poisson Model </td>\n   <td style=\"text-align:left;\"> A Gamma-Poisson model is a hierarchical statistical model that combines two probability distributions to handle overdispersed count data. (a) **The Poisson distribution:** Models the observed counts, with a rate parameter λ (lambda). (b) **The Gamma distribution**: Models the variability in λ itself, treating it as a random variable rather than a fixed parameter. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hyperparameter </td>\n   <td style=\"text-align:left;\"> In the Bayesian framework, a hyperparameter is a parameter of a prior distribution.  It defines the shape, location, or scale of the prior belief about a model parameter before observing any data. Unlike model parameters, which are estimated from the data (e.g., the mean or variance of a distribution), hyperparameters are set before the analysis begins and are not directly learned from the data. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Joint_Probability_Mass_Function </td>\n   <td style=\"text-align:left;\"> The joint probability mass function is a function that completely characterizes the distribution of a discrete random vector. When evaluated at a given point, it gives the probability that the realization of the random vector will be equal to that point. The term 'joint probability function' is often used as a synonym. Sometimes, the abbreviation 'joint pmf' is used. (statlect.com) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Likelihood_x </td>\n   <td style=\"text-align:left;\"> The likelihood refers to the probability of observing the given data under a specific hypothesis or set of parameter values. It is denotated as P(E &amp;#124; H), where E represents the evidence (data) and H represents the hypothesis. The likelihood quantifies how well a statistical model explains the observed data for different parameter values and serves as a critical component in updating beliefs in Bayesian inference. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Likelihood-Function </td>\n   <td style=\"text-align:left;\"> The likelihood function measures the plausibility of different parameter values given the observed data, rather than the probability of the data itself. It is not a probability distribution over the parameters, as it does not integrate or sum to one, and it is not normalized. Instead, it quantifies how likely the observed data are under different parameter settings. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Marginal_Probability </td>\n   <td style=\"text-align:left;\"> Marginal probability refers to the probability of a single event occurring independently, without considering the outcomes of other related events. It is (the same concept) as an unconditional probability, denoted as P(A) or P(B), and is derived from a joint probability distribution by summing or integrating over the other variables involved. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Parameter_x </td>\n   <td style=\"text-align:left;\"> Unobserved variables are usually called Parameters. (SR2, Chap.2) A parameter is an unknown numerical characteristics of a population that must be estimated. (CDS). They are also numbers that govern statistical models ([stats.stackexchange](https://stats.stackexchange.com/a/255994/207389)). A parameter is also a number that is a defining characteristic of some population or a feature of a population. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PDF </td>\n   <td style=\"text-align:left;\"> A probability density function (PDF) describes a probability distribution for a random, continuous variable. Use a probability density function to find the chances that the value of a random variable will occur within a range of values that you specify. More specifically, a PDF is a function where its integral for an interval provides the probability of a value occurring in that interval. (&lt;a href=\"https://statisticsbyjim.com/probability/probability-density-function/\"&gt;Statistics By Jim&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Poisson-Model </td>\n   <td style=\"text-align:left;\"> The Poisson model is a fundamental statistical model used for count data — that is, data representing the number of times an event occurs within a fixed interval of time or space. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Posterior_Model </td>\n   <td style=\"text-align:left;\"> The posterior model represents the updated beliefs about the parameters of a model after incorporating observed data. It is derived by combining the prior distribution, which reflects initial beliefs about the parameter, with the likelihood of the observed data using Bayes' theorem. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Posteriorx </td>\n   <td style=\"text-align:left;\"> The posterior distribution represents the updated beliefs about the parameters of a model after incorporating observed data. It is derived by combining the prior distribution, which reflects initial beliefs about the parameter, with the likelihood of the observed data using Bayes' theorem. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Priorx </td>\n   <td style=\"text-align:left;\"> A prior represents the initial belief or knowledge about an uncertain quantity, such as a model parameter, before observing any data. It is formally defined as a probability distribution that quantifies the degree of belief in the parameter's possible values based on previous information, expert opinion, or theoretical considerations. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Rate-Parameter </td>\n   <td style=\"text-align:left;\"> The rate hyperparameter in the gamma distribution, typically denoted as β (beta), controls the scale of the distribution and is the inverse of the scale parameter (i.e., rate = 1/scale). A higher rate means events occur more frequently, leading to a narrower and more concentrated distribution; a lower rate implies less frequent events, resulting in a broader distribution. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Shape-Parameter </td>\n   <td style=\"text-align:left;\"> The shape parameter in the context of the gamma distribution is a key hyperparameter that controls the form or shape of the distribution.  It is often denoted as α (alpha). A higher shape parameter (α) results in a more peaked and right-skewed distribution; a lower shape parameter leads to a flatter, more spread-out distribution. (Brave-AI) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Weibull model </td>\n   <td style=\"text-align:left;\"> The Weibull model is a statistical model where the response variable follows a Weibull distribution, which is appropriate for positive continuous data — particularly time-to-event data (survival times, failure times, lifetimes). (Brave-AI) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Session Info {.unnumbered}\n\n::::: my-r-code\n::: my-r-code-header\nSession Info\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.5.2 (2025-10-31)\n#>  os       macOS Tahoe 26.2\n#>  system   aarch64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2026-02-13\n#>  pandoc   3.9 @ /opt/homebrew/bin/ (via rmarkdown)\n#>  quarto   1.8.27 @ /usr/local/bin/quarto\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  ! package        * version  date (UTC) lib source\n#>  P abind            1.4-8    2024-09-12 [?] RSPM\n#>  P backports        1.5.0    2024-05-23 [?] RSPM\n#>    base64enc        0.1-6    2026-02-02 [1] CRAN (R 4.5.2)\n#>  P bayesplot        1.15.0   2025-12-12 [?] RSPM\n#>    bayesrules       0.0.3    2026-01-20 [1] RSPM\n#>  P boot             1.3-32   2025-08-29 [?] CRAN (R 4.5.2)\n#>    checkmate        2.3.4    2026-02-03 [1] RSPM\n#>  P class            7.3-23   2025-01-01 [?] CRAN (R 4.5.2)\n#>  P cli              3.6.5    2025-04-23 [?] CRAN (R 4.5.0)\n#>  P codetools        0.2-20   2024-03-31 [?] CRAN (R 4.5.2)\n#>  P colourpicker     1.3.0    2023-08-21 [?] RSPM\n#>  P commonmark       2.0.0    2025-07-07 [?] RSPM\n#>  P crosstalk        1.2.2    2025-08-26 [?] RSPM\n#>  P curl             7.0.0    2025-08-19 [?] RSPM\n#>  P digest           0.6.39   2025-11-19 [?] CRAN (R 4.5.2)\n#>    distributional   0.6.0    2026-01-14 [1] RSPM\n#>  P dplyr            1.2.0    2026-02-03 [?] RSPM\n#>  P DT               0.34.0   2025-09-02 [?] RSPM\n#>  P dygraphs         1.1.1.6  2018-07-11 [?] RSPM\n#>    e1071            1.7-17   2025-12-18 [1] RSPM\n#>  P evaluate         1.0.5    2025-08-27 [?] CRAN (R 4.5.1)\n#>  P farver           2.1.2    2024-05-13 [?] RSPM\n#>  P fastmap          1.2.0    2024-05-15 [?] CRAN (R 4.5.0)\n#>  P forcats          1.0.1    2025-09-25 [?] RSPM\n#>  P generics         0.1.4    2025-05-09 [?] RSPM\n#>  P ggplot2          4.0.2    2026-02-03 [?] RSPM\n#>  P glossary       * 1.0.0    2023-05-30 [?] RSPM\n#>  P glue             1.8.0    2024-09-30 [?] CRAN (R 4.5.0)\n#>  P gridExtra        2.3      2017-09-09 [?] RSPM\n#>  P groupdata2       2.0.5    2024-12-18 [?] RSPM\n#>  P gtable           0.3.6    2024-10-25 [?] RSPM\n#>  P gtools           3.9.5    2023-11-20 [?] RSPM\n#>  P htmltools        0.5.9    2025-12-04 [?] CRAN (R 4.5.2)\n#>  P htmlwidgets      1.6.4    2023-12-06 [?] RSPM\n#>  P httpuv           1.6.16   2025-04-16 [?] RSPM\n#>  P igraph           2.2.1    2025-10-27 [?] RSPM\n#>  P inline           0.3.21   2025-01-09 [?] RSPM\n#>  P janitor          2.2.1    2024-12-22 [?] RSPM\n#>  P jsonlite         2.0.0    2025-03-27 [?] CRAN (R 4.5.0)\n#>  P kableExtra       1.4.0    2024-01-24 [?] RSPM\n#>  P knitr            1.51     2025-12-20 [?] CRAN (R 4.5.2)\n#>  P labeling         0.4.3    2023-08-29 [?] RSPM\n#>    later            1.4.5    2026-01-08 [1] RSPM\n#>    lattice          0.22-9   2026-02-09 [1] CRAN (R 4.5.2)\n#>  P lifecycle        1.0.5    2026-01-08 [?] RSPM\n#>  P litedown         0.9      2025-12-18 [?] RSPM\n#>    lme4             1.1-38   2025-12-02 [1] RSPM\n#>  P loo              2.9.0    2025-12-23 [?] RSPM\n#>    lubridate        1.9.5    2026-02-04 [1] RSPM\n#>    magrittr         2.0.4    2025-09-12 [1] RSPM\n#>  P markdown         2.0      2025-03-23 [?] RSPM\n#>  P MASS             7.3-65   2025-02-28 [?] CRAN (R 4.5.2)\n#>  P Matrix           1.7-4    2025-08-28 [?] CRAN (R 4.5.2)\n#>  P matrixStats      1.5.0    2025-01-07 [?] RSPM\n#>  P mime             0.13     2025-03-17 [?] CRAN (R 4.5.0)\n#>  P miniUI           0.1.2    2025-04-17 [?] RSPM\n#>  P minqa            1.2.8    2024-08-17 [?] RSPM\n#>  P nlme             3.1-168  2025-03-31 [?] CRAN (R 4.5.2)\n#>  P nloptr           2.2.1    2025-03-17 [?] RSPM\n#>  P otel             0.2.0    2025-08-29 [?] RSPM\n#>  P pillar           1.11.1   2025-09-17 [?] RSPM\n#>  P pkgbuild         1.4.8    2025-05-26 [?] RSPM\n#>  P pkgconfig        2.0.3    2019-09-22 [?] RSPM\n#>    plyr             1.8.9    2023-10-02 [1] RSPM\n#>  P posterior        1.6.1    2025-02-27 [?] RSPM\n#>  P promises         1.5.0    2025-11-01 [?] RSPM\n#>    proxy            0.4-29   2025-12-29 [1] CRAN (R 4.5.2)\n#>    purrr            1.2.1    2026-01-09 [1] CRAN (R 4.5.2)\n#>    QuickJSR         1.9.0    2026-01-25 [1] RSPM\n#>  P R6               2.6.1    2025-02-15 [?] CRAN (R 4.5.0)\n#>    rbibutils        2.4.1    2026-01-21 [1] CRAN (R 4.5.2)\n#>  P RColorBrewer     1.1-3    2022-04-03 [?] RSPM\n#>    Rcpp             1.1.1    2026-01-10 [1] RSPM\n#>  P RcppParallel     5.1.11-1 2025-08-27 [?] RSPM\n#>    Rdpack           2.6.6    2026-02-08 [1] CRAN (R 4.5.2)\n#>    reformulas       0.4.4    2026-02-02 [1] RSPM\n#>    renv             1.1.7    2026-01-27 [1] RSPM\n#>  P reshape2         1.4.5    2025-11-12 [?] RSPM\n#>  P rlang            1.1.7    2025-12-20 [?] Github (tidyverse/rlang@7a519a2)\n#>  P rmarkdown        2.30     2025-09-28 [?] CRAN (R 4.5.0)\n#>  P rstan            2.32.7   2025-03-10 [?] RSPM\n#>  P rstanarm         2.32.2   2025-09-30 [?] RSPM\n#>    rstantools       2.6.0    2026-01-10 [1] RSPM\n#>    rstudioapi       0.18.0   2026-01-16 [1] RSPM\n#>    S7               0.2.1    2025-11-14 [1] CRAN (R 4.5.2)\n#>  P scales           1.4.0    2025-04-24 [?] RSPM\n#>  P sessioninfo      1.2.3    2025-02-05 [?] RSPM\n#>  P shiny            1.12.1   2025-12-09 [?] RSPM\n#>    shinyjs          2.1.1    2026-01-15 [1] RSPM\n#>  P shinystan        2.7.0    2025-12-12 [?] RSPM\n#>  P shinythemes      1.2.0    2021-01-25 [?] RSPM\n#>  P snakecase        0.11.1   2023-08-27 [?] RSPM\n#>  P StanHeaders      2.32.10  2024-07-15 [?] RSPM\n#>  P stringi          1.8.7    2025-03-27 [?] RSPM\n#>  P stringr          1.6.0    2025-11-04 [?] RSPM\n#>    survival         3.8-6    2026-01-16 [1] CRAN (R 4.5.2)\n#>  P svglite          2.2.2    2025-10-21 [?] RSPM\n#>  P systemfonts      1.3.1    2025-10-01 [?] RSPM\n#>  P tensorA          0.36.2.1 2023-12-13 [?] RSPM\n#>  P textshaping      1.0.4    2025-10-10 [?] RSPM\n#>  P threejs          0.3.4    2025-04-21 [?] RSPM\n#>    tibble           3.3.1    2026-01-11 [1] RSPM\n#>  P tidyselect       1.2.1    2024-03-11 [?] RSPM\n#>    timechange       0.4.0    2026-01-29 [1] RSPM\n#>  P V8               8.0.1    2025-10-10 [?] RSPM\n#>    vctrs            0.7.1    2026-01-23 [1] RSPM\n#>    viridisLite      0.4.3    2026-02-04 [1] RSPM\n#>  P webexercises   * 1.1.0    2023-05-15 [?] RSPM\n#>  P withr            3.0.2    2024-10-28 [?] RSPM\n#>    xfun             0.56     2026-01-18 [1] RSPM\n#>    xml2             1.5.2    2026-01-17 [1] RSPM\n#>  P xtable           1.8-4    2019-04-21 [?] RSPM\n#>    xts              0.14.1   2024-10-15 [1] RSPM\n#>  P yaml             2.3.12   2025-12-10 [?] CRAN (R 4.5.2)\n#>    zoo              1.8-15   2025-12-15 [1] RSPM\n#> \n#>  [1] /Users/petzi/Documents/Meine-Repos/bayes-rules/renv/library/macos/R-4.5/aarch64-apple-darwin20\n#>  [2] /Users/petzi/Library/Caches/org.R-project.R/R/renv/sandbox/macos/R-4.5/aarch64-apple-darwin20/4cd76b74\n#> \n#>  * ── Packages attached to the search path.\n#>  P ── Loaded and on-disk path mismatch.\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n:::\n:::::\n",
    "supporting": [
      "005-conjugate-families_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}